<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 1 Matrix Algebra | Econometrics Notes</title>
  <meta name="description" content="Lecture 1 Matrix Algebra | Econometrics Notes" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 1 Matrix Algebra | Econometrics Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture 1 Matrix Algebra | Econometrics Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 1 Matrix Algebra | Econometrics Notes" />
  
  <meta name="twitter:description" content="Lecture 1 Matrix Algebra | Econometrics Notes" />
  

<meta name="author" content="Matteo Courthoud" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="appendix2.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../" target="_self">MATTEO COURTHOUD</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="appendix1.html"><a href="appendix1.html"><i class="fa fa-check"></i><b>1</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="appendix1.html"><a href="appendix1.html#basics"><i class="fa fa-check"></i><b>1.1</b> Basics</a></li>
<li class="chapter" data-level="1.2" data-path="appendix1.html"><a href="appendix1.html#spectral-decomposition"><i class="fa fa-check"></i><b>1.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="1.3" data-path="appendix1.html"><a href="appendix1.html#quadratic-forms-and-definite-matrices"><i class="fa fa-check"></i><b>1.3</b> Quadratic Forms and Definite Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="appendix1.html"><a href="appendix1.html#matrix-calculus"><i class="fa fa-check"></i><b>1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.5" data-path="appendix1.html"><a href="appendix1.html#references-1"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="appendix2.html"><a href="appendix2.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix2.html"><a href="appendix2.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a></li>
<li class="chapter" data-level="2.2" data-path="appendix2.html"><a href="appendix2.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="appendix2.html"><a href="appendix2.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a></li>
<li class="chapter" data-level="2.4" data-path="appendix2.html"><a href="appendix2.html#inequalities"><i class="fa fa-check"></i><b>2.4</b> Inequalities</a></li>
<li class="chapter" data-level="2.5" data-path="appendix2.html"><a href="appendix2.html#theorems"><i class="fa fa-check"></i><b>2.5</b> Theorems</a></li>
<li class="chapter" data-level="2.6" data-path="appendix2.html"><a href="appendix2.html#statistical-models"><i class="fa fa-check"></i><b>2.6</b> Statistical Models</a></li>
<li class="chapter" data-level="2.7" data-path="appendix2.html"><a href="appendix2.html#references-2"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix3.html"><a href="appendix3.html"><i class="fa fa-check"></i><b>3</b> Asymptotic Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix3.html"><a href="appendix3.html#convergence"><i class="fa fa-check"></i><b>3.1</b> Convergence</a></li>
<li class="chapter" data-level="3.2" data-path="appendix3.html"><a href="appendix3.html#theorems-1"><i class="fa fa-check"></i><b>3.2</b> Theorems</a></li>
<li class="chapter" data-level="3.3" data-path="appendix3.html"><a href="appendix3.html#ergodic-theory"><i class="fa fa-check"></i><b>3.3</b> Ergodic Theory</a></li>
<li class="chapter" data-level="3.4" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-estimators"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Properties of Estimators</a></li>
<li class="chapter" data-level="3.5" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-test-statistics"><i class="fa fa-check"></i><b>3.5</b> Asymptotic Properties of Test Statistics</a></li>
<li class="chapter" data-level="3.6" data-path="appendix3.html"><a href="appendix3.html#references-3"><i class="fa fa-check"></i><b>3.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>4</b> OLS Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="lecture1.html"><a href="lecture1.html#the-gauss-markov-model"><i class="fa fa-check"></i><b>4.1</b> The Gauss Markov Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lecture1.html"><a href="lecture1.html#matlab"><i class="fa fa-check"></i><b>4.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture1.html"><a href="lecture1.html#the-ols-estimator"><i class="fa fa-check"></i><b>4.2</b> The OLS estimator</a><ul>
<li class="chapter" data-level="4.2.1" data-path="lecture1.html"><a href="lecture1.html#matlab-1"><i class="fa fa-check"></i><b>4.2.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture1.html"><a href="lecture1.html#ols-residual-properties"><i class="fa fa-check"></i><b>4.3</b> OLS residual properties</a><ul>
<li class="chapter" data-level="4.3.1" data-path="lecture1.html"><a href="lecture1.html#matlab-2"><i class="fa fa-check"></i><b>4.3.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture1.html"><a href="lecture1.html#finite-sample-properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>4.4</b> Finite Sample Properties of the OLS estimator</a><ul>
<li class="chapter" data-level="4.4.1" data-path="lecture1.html"><a href="lecture1.html#matlab-3"><i class="fa fa-check"></i><b>4.4.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lecture1.html"><a href="lecture1.html#references-4"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>5</b> OLS Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="lecture2.html"><a href="lecture2.html#asymptotic-theory-of-the-ols-estimator"><i class="fa fa-check"></i><b>5.1</b> Asymptotic Theory of the OLS Estimator</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lecture2.html"><a href="lecture2.html#gaussian-error-term"><i class="fa fa-check"></i><b>5.1.1</b> Gaussian Error Term</a></li>
<li class="chapter" data-level="5.1.2" data-path="lecture2.html"><a href="lecture2.html#homoskedastic-error-term"><i class="fa fa-check"></i><b>5.1.2</b> Homoskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.3" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-error-term"><i class="fa fa-check"></i><b>5.1.3</b> Heteroskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.4" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-and-autocorrelated-error-term"><i class="fa fa-check"></i><b>5.1.4</b> Heteroskedastic and Autocorrelated Error Term</a></li>
<li class="chapter" data-level="5.1.5" data-path="lecture2.html"><a href="lecture2.html#fixed-b-asymptotics"><i class="fa fa-check"></i><b>5.1.5</b> Fixed b asymptotics</a></li>
<li class="chapter" data-level="5.1.6" data-path="lecture2.html"><a href="lecture2.html#fixed-g-asymptotics"><i class="fa fa-check"></i><b>5.1.6</b> Fixed G asymptotics</a></li>
<li class="chapter" data-level="5.1.7" data-path="lecture2.html"><a href="lecture2.html#matlab-4"><i class="fa fa-check"></i><b>5.1.7</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lecture2.html"><a href="lecture2.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lecture2.html"><a href="lecture2.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2.2" data-path="lecture2.html"><a href="lecture2.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="lecture2.html"><a href="lecture2.html#matlab-5"><i class="fa fa-check"></i><b>5.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="lecture2.html"><a href="lecture2.html#references-5"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>6</b> Endogeneity</a><ul>
<li class="chapter" data-level="6.1" data-path="lecture3.html"><a href="lecture3.html#instrumental-variables"><i class="fa fa-check"></i><b>6.1</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lecture3.html"><a href="lecture3.html#matlab-6"><i class="fa fa-check"></i><b>6.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lecture3.html"><a href="lecture3.html#gmm"><i class="fa fa-check"></i><b>6.2</b> GMM</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lecture3.html"><a href="lecture3.html#step-gmm"><i class="fa fa-check"></i><b>6.2.1</b> 1-step GMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="lecture3.html"><a href="lecture3.html#step-gmm-1"><i class="fa fa-check"></i><b>6.2.2</b> 2-step GMM</a></li>
<li class="chapter" data-level="6.2.3" data-path="lecture3.html"><a href="lecture3.html#matlab-7"><i class="fa fa-check"></i><b>6.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture3.html"><a href="lecture3.html#testing-overidentifying-restrictions"><i class="fa fa-check"></i><b>6.3</b> Testing Overidentifying Restrictions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lecture3.html"><a href="lecture3.html#naive-test"><i class="fa fa-check"></i><b>6.3.1</b> Naive Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="lecture3.html"><a href="lecture3.html#hansens-test"><i class="fa fa-check"></i><b>6.3.2</b> Hansen’s Test</a></li>
<li class="chapter" data-level="6.3.3" data-path="lecture3.html"><a href="lecture3.html#special-case-conditional-homoskedasticity"><i class="fa fa-check"></i><b>6.3.3</b> Special Case: Conditional Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lecture3.html"><a href="lecture3.html#small-sample-properties-of-2sls"><i class="fa fa-check"></i><b>6.4</b> Small-Sample Properties of 2SLS</a></li>
<li class="chapter" data-level="6.5" data-path="lecture3.html"><a href="lecture3.html#many-instrument-robust-estimation"><i class="fa fa-check"></i><b>6.5</b> Many Instrument Robust Estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="lecture3.html"><a href="lecture3.html#liml"><i class="fa fa-check"></i><b>6.5.1</b> LIML</a></li>
<li class="chapter" data-level="6.5.2" data-path="lecture3.html"><a href="lecture3.html#jive"><i class="fa fa-check"></i><b>6.5.2</b> JIVE</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lecture3.html"><a href="lecture3.html#hausman-test"><i class="fa fa-check"></i><b>6.6</b> Hausman Test</a></li>
<li class="chapter" data-level="6.7" data-path="lecture3.html"><a href="lecture3.html#references-6"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>7</b> Non-Parametric Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="lecture4.html"><a href="lecture4.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="lecture4.html"><a href="lecture4.html#discrete-x---cell-estimator"><i class="fa fa-check"></i><b>7.2</b> Discrete x - Cell Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="lecture4.html"><a href="lecture4.html#local-non-parametric-estimation---kernels"><i class="fa fa-check"></i><b>7.3</b> Local Non-Parametric Estimation - Kernels</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lecture4.html"><a href="lecture4.html#estimator-examples"><i class="fa fa-check"></i><b>7.3.1</b> Estimator examples:</a></li>
<li class="chapter" data-level="7.3.2" data-path="lecture4.html"><a href="lecture4.html#kernel-examples"><i class="fa fa-check"></i><b>7.3.2</b> Kernel examples:</a></li>
<li class="chapter" data-level="7.3.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-bandwidth"><i class="fa fa-check"></i><b>7.3.3</b> Choice of the optimal bandwidth</a></li>
<li class="chapter" data-level="7.3.4" data-path="lecture4.html"><a href="lecture4.html#inference-1"><i class="fa fa-check"></i><b>7.3.4</b> Inference</a></li>
<li class="chapter" data-level="7.3.5" data-path="lecture4.html"><a href="lecture4.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.3.5</b> Bias-variance trade-off</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lecture4.html"><a href="lecture4.html#global-non-parametric-estimation---series"><i class="fa fa-check"></i><b>7.4</b> Global Non-Parametric Estimation - Series</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lecture4.html"><a href="lecture4.html#examples"><i class="fa fa-check"></i><b>7.4.1</b> Examples</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture4.html"><a href="lecture4.html#estimation"><i class="fa fa-check"></i><b>7.4.2</b> Estimation</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-k"><i class="fa fa-check"></i><b>7.4.3</b> Choice of the optimal <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture4.html"><a href="lecture4.html#inference-2"><i class="fa fa-check"></i><b>7.4.4</b> Inference</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture4.html"><a href="lecture4.html#kernel-vs-series"><i class="fa fa-check"></i><b>7.4.5</b> Kernel vs Series</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lecture4.html"><a href="lecture4.html#references-7"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>8</b> Variable Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="lecture5.html"><a href="lecture5.html#lasso"><i class="fa fa-check"></i><b>8.1</b> Lasso</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lecture5.html"><a href="lecture5.html#choosing-the-optimal-lambda"><i class="fa fa-check"></i><b>8.1.1</b> Choosing the optimal lambda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lecture5.html"><a href="lecture5.html#pre-testing"><i class="fa fa-check"></i><b>8.2</b> Pre-Testing</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lecture5.html"><a href="lecture5.html#omitted-variable-bias"><i class="fa fa-check"></i><b>8.2.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.2.2" data-path="lecture5.html"><a href="lecture5.html#pre-test-bias"><i class="fa fa-check"></i><b>8.2.2</b> Pre-test bias</a></li>
<li class="chapter" data-level="8.2.3" data-path="lecture5.html"><a href="lecture5.html#partioned-regression"><i class="fa fa-check"></i><b>8.2.3</b> Partioned Regression</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lecture5.html"><a href="lecture5.html#post-double-selection"><i class="fa fa-check"></i><b>8.3</b> Post Double Selection</a></li>
<li class="chapter" data-level="8.4" data-path="lecture5.html"><a href="lecture5.html#references-8"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matlabcode.html"><a href="matlabcode.html"><i class="fa fa-check"></i><b>9</b> Matlab Code</a><ul>
<li class="chapter" data-level="9.1" data-path="matlabcode.html"><a href="matlabcode.html#lecture-1"><i class="fa fa-check"></i><b>9.1</b> Lecture 1</a></li>
<li class="chapter" data-level="9.2" data-path="matlabcode.html"><a href="matlabcode.html#lecture-2"><i class="fa fa-check"></i><b>9.2</b> Lecture 2</a></li>
<li class="chapter" data-level="9.3" data-path="matlabcode.html"><a href="matlabcode.html#lecture-3"><i class="fa fa-check"></i><b>9.3</b> Lecture 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix1" class="section level1">
<h1><span class="header-section-number">Lecture 1</span> Matrix Algebra</h1>
<div id="basics" class="section level2">
<h2><span class="header-section-number">1.1</span> Basics</h2>
<p>A real <span class="math inline">\(n \times m\)</span> matrix <span class="math inline">\(A\)</span> is an array</p>
<p><span class="math display">\[
  A=
  \begin{bmatrix}
  a_{11} &amp; a_{12} &amp; a_{13} &amp; \dots  &amp; a_{1m} \\
  a_{21} &amp; a_{22} &amp; a_{23} &amp; \dots  &amp; a_{2m} \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \dots  &amp; a_{nm}
  \end{bmatrix}
\]</span></p>
<p>We write <span class="math inline">\([A]_ {ij} = a_ {ij}\)</span> to indicate the <span class="math inline">\((i,j)\)</span>-element of <span class="math inline">\(A\)</span>.</p>
<blockquote>
<p>We will usually take the convention that a real vector <span class="math inline">\(x \in \mathbb R^n\)</span> is identified with an <span class="math inline">\(n \times 1\)</span> matrix.</p>
</blockquote>
<p>The <span class="math inline">\(n \times n\)</span> <strong>identity matrix</strong> <span class="math inline">\(I_n\)</span> is given by<br />
<span class="math display">\[
  [I_n] _ {ij} = \begin{cases} 1 \ \ \ \text{if} \ i=j \\
  0 \ \ \ \text{if} \ i \neq j \end{cases}
\]</span></p>
<p>Fundamental operations on matrices:</p>
<ol style="list-style-type: decimal">
<li>Two <span class="math inline">\(n \times m\)</span> matrices, <span class="math inline">\(A,B\)</span>, are added element-wise so that <span class="math inline">\([A+B]_ {ij} = [A]_ {ij} + [B]_ {ij}\)</span>.</li>
<li>A matrix <span class="math inline">\(A\)</span> can be multiplied by a scalar <span class="math inline">\(c\in \mathbb{R}\)</span> in which case we set <span class="math inline">\([cA]_{ij} = c[A]_ {ij}\)</span>.<br />
</li>
<li>An <span class="math inline">\(n \times m\)</span> matrix <span class="math inline">\(A\)</span> can be multiplied with an <span class="math inline">\(m \times p\)</span> matrix <span class="math inline">\(B\)</span>.<br />
</li>
<li>The product <span class="math inline">\(AB\)</span> is defined according to the rule <span class="math inline">\([AB]_{ij} = \sum_{k=1}^m [A]_{ik}[B]_{kj}\)</span>.</li>
<li>An <span class="math inline">\(n \times n\)</span> matrix is invertible if there exists a matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(AB=I\)</span>. In this case, we use the notational convention of writing <span class="math inline">\(B = A^{-1}\)</span>.</li>
<li>Matrix transposition is defined by <span class="math inline">\([A&#39;] _ {ij} = [A] _ {ji}\)</span>.</li>
</ol>
<p>The <strong>trace</strong> of a square matrix <span class="math inline">\(A\)</span> with dimension <span class="math inline">\(n \times n\)</span> is <span class="math inline">\(\text{tr}(A) = \sum _ {i=1}^n a _ {ii}\)</span>.</p>
<p>The <strong>determinant</strong> of a square <span class="math inline">\(n \times n\)</span> matrix A is defined according to one of the following three (equivalent) definitions.</p>
<ol style="list-style-type: decimal">
<li>Recursively as <span class="math inline">\(det(A) = \sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]_{-i,-j})\)</span> where <span class="math inline">\([A]_{-i,-j}\)</span> is the matrix obtained by deleting the <span class="math inline">\(i\)</span>th row and the <span class="math inline">\(j\)</span>th column.</li>
<li><span class="math inline">\(A \mapsto det(A)\)</span> under the unique alternating multilinear map on <span class="math inline">\(n \times n\)</span> matrices such that <span class="math inline">\(I \mapsto 1\)</span>.</li>
</ol>
<p>Vectors <span class="math inline">\(x_1,...,x_k\)</span> are <strong>linearly independent</strong> if the only solution to the equation <span class="math inline">\(b_1x_1 + ... + b_k x_k=0, \ b_j \in \mathbb R\)</span>, is <span class="math inline">\(b_1=b_2=...=b_k=0\)</span>.</p>
<p>Useful matrix identities:</p>
<ul>
<li><span class="math inline">\((A+B)&#39; =A&#39;+B&#39;\)</span></li>
<li><span class="math inline">\((AB)C = A(BC)\)</span></li>
<li><span class="math inline">\(A(B+C) = AB+AC\)</span></li>
<li><span class="math inline">\((AB&#39;) = B&#39;A&#39;\)</span></li>
<li><span class="math inline">\((A^{-1})&#39; = (A&#39;)^{-1}\)</span></li>
<li><span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span></li>
<li><span class="math inline">\(\text{tr}(cA) = c\text{tr}(A)\)</span></li>
<li><span class="math inline">\(\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)\)</span></li>
<li><span class="math inline">\(\text{tr}(AB) =\text{tr}(BA)\)</span></li>
<li><span class="math inline">\(det(I)=1\)</span></li>
<li><span class="math inline">\(det(cA) = c^ndet(A)\)</span> if <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times n\)</span> and <span class="math inline">\(c \in \mathbb R\)</span></li>
<li><span class="math inline">\(det(A) = det(A&#39;)\)</span></li>
<li><span class="math inline">\(det(AB) = det(A)det(B)\)</span></li>
<li><span class="math inline">\(det(A^{-1}) = (det(A))^{-1}\)</span></li>
<li><span class="math inline">\(A^{-1}\)</span> exists iff <span class="math inline">\(det(A) \neq 0\)</span></li>
<li><span class="math inline">\(rank(A) = rank(A&#39;) = rank(A&#39;A) = rank(AA&#39;)\)</span></li>
<li><span class="math inline">\(A^{-1}\)</span> exists iff <span class="math inline">\(rank(A)=n\)</span> for <span class="math inline">\(A\)</span> <span class="math inline">\(n \times n\)</span></li>
<li><span class="math inline">\(rank(AB) \leq \min \{ rank(A), rank(B) \}\)</span></li>
</ul>
<p>The <strong>rank</strong> of a matrix, <span class="math inline">\(rank(A)\)</span> is equal to the maximal number of linearly independent rows for <span class="math inline">\(A\)</span>.</p>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix. The <span class="math inline">\(n \times 1\)</span> vector <span class="math inline">\(x \neq 0\)</span> is an <strong>eigenvector</strong> of <span class="math inline">\(A\)</span> with corresponding <strong>eigenvalue</strong> <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(Ax = \lambda x\)</span>.</p>
<p>The following types of matrices are defined:</p>
<ol style="list-style-type: decimal">
<li>A matrix <span class="math inline">\(A\)</span> is diagonal if <span class="math inline">\([A]_ {ij} \neq 0\)</span> only if <span class="math inline">\(i=j\)</span>.</li>
<li>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonal if <span class="math inline">\(A&#39;A = I\)</span></li>
<li>A matrix <span class="math inline">\(A\)</span> is symmetric if <span class="math inline">\([A]_ {ij} = [A]_ {ji}\)</span>.</li>
<li>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is idempotent if <span class="math inline">\(A^2=A\)</span>.</li>
<li>The matrix of zeros (<span class="math inline">\([A]_ {ij} =0\)</span> for each <span class="math inline">\(i,j\)</span>) is simply denoted 0.</li>
<li>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is nilpotent if <span class="math inline">\(A^k=0\)</span> for some integer <span class="math inline">\(k&gt;0\)</span>.</li>
</ol>
</div>
<div id="spectral-decomposition" class="section level2">
<h2><span class="header-section-number">1.2</span> Spectral Decomposition</h2>
<p><strong>Theorem</strong>:
Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> symmetric matrix. Then <span class="math inline">\(A\)</span> can be factored as <span class="math inline">\(A = C \Lambda C&#39;\)</span> where <span class="math inline">\(C\)</span> is orthogonal and <span class="math inline">\(\Lambda\)</span> is diagonal.</p>
<p>If we postmultiply <span class="math inline">\(A\)</span> by <span class="math inline">\(C\)</span>, we get</p>
<ul>
<li><span class="math inline">\(AC = C \Lambda C&#39;C\)</span> and</li>
<li><span class="math inline">\(AC = C \Lambda\)</span>.</li>
</ul>
<blockquote>
<p>This is a matrix equation which can be split into columns. The <span class="math inline">\(i\)</span>th column of the equation reads <span class="math inline">\(A c_i = \lambda_i c_i\)</span> which corresponds to the definition of eigenvalues and eigenvectors. So if the decomposition exists, then <span class="math inline">\(C\)</span> is the eigenvector matrix and <span class="math inline">\(\Lambda\)</span> contains the eigenvalues.</p>
</blockquote>
<p><strong>Theorem</strong>:
The trace of a symmetric matrix equals the sum of its eigenvalues. The determinant of a symmetric matrix equals the product of its eigenvalues.</p>
<p><strong>Theorem</strong>:
The rank of a symmetric matrix equals the number of non zero eigenvalues.</p>
<p><strong>Proof</strong>:
<span class="math inline">\(rank(A) = rank(C\Lambda C&#39;) = rank(\Lambda) = | \{i: \lambda_i \neq 0 \}|\)</span>.
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p><strong>Theorem</strong>:
The nonzero eigenvalues of <span class="math inline">\(AA&#39;\)</span> and <span class="math inline">\(A&#39;A\)</span> are identical.</p>
<p><strong>Theorem</strong>:
The trace of a symmetric matrix equals the sum of its eignevalues.</p>
<p><strong>Proof</strong>:
<span class="math inline">\(tr(A) = tr(C \Lambda C&#39;) = tr((C \Lambda)C&#39;) = tr(C&#39;C \Lambda) = tr(\Lambda) = \sum_ {i=1}^n \lambda_i.\)</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p><strong>Theorem</strong>:
The determinant of a symmetric matrix equals the product of its eignevalues.</p>
<p><strong>Proof</strong>:
<span class="math inline">\(det(A) = det(C \Lambda C&#39;) = det(C)det(\Lambda)det(C&#39;) = det(C)det(C&#39;)det(\Lambda) = det(CC&#39;) det(\Lambda) = det(I)det(\Lambda) = det(\Lambda) = \prod_ {i=1}^n \lambda_i.\)</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p><strong>Theorem</strong>:
For any symmetric matrix <span class="math inline">\(A\)</span>, the eigenvalues of <span class="math inline">\(A^2\)</span> are the square of the eignevalues of <span class="math inline">\(A\)</span>, and the eigenvectors are the same.</p>
<p><strong>Proof</strong>:
<span class="math inline">\(A = C \Lambda C&#39; \implies A^2 = C \Lambda C&#39; C \Lambda C&#39; = C \Lambda I \Lambda C&#39; = C \Lambda^2 C&#39;\)</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p><strong>Theorem</strong>: For any symmetric matrix <span class="math inline">\(A\)</span>, and any integer <span class="math inline">\(k&gt;0\)</span>, the eigenvalues of <span class="math inline">\(A^k\)</span> are the <span class="math inline">\(k\)</span>th power of the eignevalues of <span class="math inline">\(A\)</span>, and the eigenvectors are the same.</p>
<p><strong>Theorem</strong>:
Any square symmetric matrix <span class="math inline">\(A\)</span> with positive eigenvalues can be written as the product of a lower triangular matrix <span class="math inline">\(L\)</span> and its (upper triangular) transpose <span class="math inline">\(L&#39; = U\)</span>. That is <span class="math inline">\(A = LU = LL&#39;\)</span></p>
<blockquote>
<p>Note that
<span class="math display">\[
A = LL&#39; = LU = U&#39;U  = (L&#39;)^{-1}L^{-1} = U^{-1}(U&#39;)^{-1}
\]</span>
where <span class="math inline">\(L^{-1}\)</span> is lower triangular and <span class="math inline">\(U^{ -1}\)</span> is upper trianguar. You can check this for the <span class="math inline">\(2 \times 2\)</span> case. Also note that the validity of the theorem can be extended to symmetric matrices with non- negative eigenvalues by a limiting argument. However, then the proof is not constructive anymore.</p>
</blockquote>
</div>
<div id="quadratic-forms-and-definite-matrices" class="section level2">
<h2><span class="header-section-number">1.3</span> Quadratic Forms and Definite Matrices</h2>
<p>A <strong>quadratic form</strong> in the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(n \times 1\)</span> vector <span class="math inline">\(x\)</span> is defined by the scalar <span class="math inline">\(x&#39;Ax\)</span>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A\)</span> is negative definite (ND) if for each <span class="math inline">\(x \neq 0\)</span>, <span class="math inline">\(x&#39;Ax &lt; 0\)</span></li>
<li><span class="math inline">\(A\)</span> is negative semidefinite (NSD) if for each <span class="math inline">\(x \neq 0\)</span>, <span class="math inline">\(x&#39;Ax \leq 0\)</span></li>
<li><span class="math inline">\(A\)</span> is positive definite (PD) if for each <span class="math inline">\(x \neq 0\)</span>, <span class="math inline">\(x&#39;Ax &gt; 0\)</span></li>
<li><span class="math inline">\(A\)</span> is positive semidefinite (PSD) if for each <span class="math inline">\(x \neq 0\)</span>, <span class="math inline">\(x&#39;Ax \geq 0\)</span></li>
</ol>
<p><strong>Theorem</strong>:
Let <span class="math inline">\(A\)</span> be a symmetric matrix. Then <span class="math inline">\(A\)</span> is PD(ND) <span class="math inline">\(\iff\)</span> all of its eigenvalues are positive (negative).</p>
<p>Some more results:</p>
<ol style="list-style-type: decimal">
<li>If a symmetric matrix <span class="math inline">\(A\)</span> is PD (PSD, ND, NSD), then <span class="math inline">\(\text{det}(A) &gt;(\geq,&lt;,\leq) 0\)</span>.</li>
<li>If symmetric matrix <span class="math inline">\(A\)</span> is PD (ND) then <span class="math inline">\(A^{-1}\)</span> is symmetric PD (ND).</li>
<li>The identity matrix is PD (since all eigenvalues are equal to 1).</li>
<li>Every symmetric idempotent matrix is PSD (since the eigenvalues are only 0 or 1).</li>
</ol>
<p><strong>Theorem</strong>:
If <span class="math inline">\(A\)</span> is <span class="math inline">\(n\times k\)</span> with <span class="math inline">\(n&gt;k\)</span> and <span class="math inline">\(rank(A)=k\)</span>, then <span class="math inline">\(A&#39;A\)</span> is PD and <span class="math inline">\(AA&#39;\)</span> is PSD.</p>
<p>The <strong>semidefinite partial order</strong> is defined by <span class="math inline">\(A \geq B\)</span> iff <span class="math inline">\(A-B\)</span> is PSD.</p>
<p><strong>Theorem</strong>:
Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> be symmetric,square , PD, conformable. Then <span class="math inline">\(A-B\)</span> is PD iff <span class="math inline">\(A^{-1}-B^{-1}\)</span> is PD.</p>
</div>
<div id="matrix-calculus" class="section level2">
<h2><span class="header-section-number">1.4</span> Matrix Calculus</h2>
<p>We first define matrices blockwise when they are conformable. In particular, we assume that if <span class="math inline">\(A_1, A_2, A_3, A_4\)</span> are matrices with appropriate dimensions then the matrix
<span class="math display">\[
    A = \begin{bmatrix} A_1 &amp; A_1 \\
    A_3 &amp; A_4 \end{bmatrix}
\]</span>
is defined in the obvious way.</p>
<p>Let <span class="math inline">\(F: \mathbb R^m \times \mathbb R^n \rightarrow \mathbb R^p \times \mathbb R^q\)</span> be a matrix valued function. More precisely, given a real <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(X\)</span>, <span class="math inline">\(F(X)\)</span> returns the <span class="math inline">\(p \times q\)</span> matrix<br />
<span class="math display">\[
    \begin{bmatrix}
    f_ {11}(X) &amp; ... &amp; f_ {1q}(X) \\ \vdots &amp; \ddots &amp; \vdots \\
    f_ {p1}(X)&amp; ... &amp; f_ {pq}(X)
    \end{bmatrix}
\]</span></p>
<p>The derivative of <span class="math inline">\(F\)</span> with respect to the matrix <span class="math inline">\(X\)</span> is the <span class="math inline">\(mp \times nq\)</span> matrix
<span class="math display">\[
    \frac{\partial F(X)}{\partial X} = \begin{bmatrix}
    \frac{\partial F(X)}{\partial x_ {11}} &amp; ... &amp; \frac{\partial F(X)}{\partial x_ {1n}} \\ \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial F(X)}{\partial x_ {m1}} &amp; ... &amp; \frac{\partial F(X)}{\partial x_ {mn}}
    \end{bmatrix}
\]</span>
where each <span class="math inline">\(\frac{\partial F(X)}{\partial x_ {ij}}\)</span> is a <span class="math inline">\(p\times q\)</span> matrix given by<br />
<span class="math display">\[
    \frac{\partial F(X)}{\partial x_ {ij}} = \begin{bmatrix}
    \frac{\partial f_ {11}(X)}{\partial x_ {ij}} &amp; ... &amp; \frac{\partial f_ {1q}(X)}{\partial x_ {ij}} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial f_ {p1}(X)}{\partial x_ {ij}} &amp; ... &amp; \frac{\partial f_ {pq}(X)}{\partial x_ {ij}}
    \end{bmatrix}
\]</span>
The most important case is when <span class="math inline">\(F: \mathbb R^n \rightarrow \mathbb R\)</span> since this simplifies the derivation of the least squares estimator. Also, the trickiest thing is to make sure that dimensions are correct.</p>
<p>Useful results in matrix calculus:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\frac{\partial b&#39;x}{\partial x}= b\)</span> for <span class="math inline">\(dim(b) = dim(x)\)</span></li>
<li><span class="math inline">\(\frac{\partial B&#39;x}{\partial x}= B\)</span> for arbitrary, conformable <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(\frac{\partial B&#39;x}{\partial x&#39;}= B&#39;\)</span> for arbitrary, conformable <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(\frac{\partial x&#39;Ax}{\partial x} = (A + A&#39;)x\)</span></li>
<li><span class="math inline">\(\frac{\partial x&#39;Ax}{\partial A} = xx&#39;\)</span></li>
<li><span class="math inline">\(\frac{\partial x&#39;Ax}{\partial x} = det(A) (A^{-1})&#39;\)</span></li>
<li><span class="math inline">\(\frac{\partial \ln det(A)}{\partial A} = (A^{-1})&#39;\)</span></li>
</ol>
</div>
<div id="references-1" class="section level2">
<h2><span class="header-section-number">1.5</span> References</h2>
<ul>
<li>Kozbur (2019). PhD Econometrics - Lecture Notes.</li>
<li>Greene (2006). “<em>Econometric Analysis</em>”. Appendix A: Matrix Algebra.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["notes.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
