<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 7 Non-Parametric Estimation | Econometrics Notes</title>
  <meta name="description" content="Lecture 7 Non-Parametric Estimation | Econometrics Notes" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 7 Non-Parametric Estimation | Econometrics Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture 7 Non-Parametric Estimation | Econometrics Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 7 Non-Parametric Estimation | Econometrics Notes" />
  
  <meta name="twitter:description" content="Lecture 7 Non-Parametric Estimation | Econometrics Notes" />
  

<meta name="author" content="Matteo Courthoud" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture3.html">
<link rel="next" href="lecture5.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../" target="_self">MATTEO COURTHOUD</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="appendix1.html"><a href="appendix1.html"><i class="fa fa-check"></i><b>1</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="appendix1.html"><a href="appendix1.html#basics"><i class="fa fa-check"></i><b>1.1</b> Basics</a></li>
<li class="chapter" data-level="1.2" data-path="appendix1.html"><a href="appendix1.html#spectral-decomposition"><i class="fa fa-check"></i><b>1.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="1.3" data-path="appendix1.html"><a href="appendix1.html#quadratic-forms-and-definite-matrices"><i class="fa fa-check"></i><b>1.3</b> Quadratic Forms and Definite Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="appendix1.html"><a href="appendix1.html#matrix-calculus"><i class="fa fa-check"></i><b>1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.5" data-path="appendix1.html"><a href="appendix1.html#references-1"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="appendix2.html"><a href="appendix2.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix2.html"><a href="appendix2.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a></li>
<li class="chapter" data-level="2.2" data-path="appendix2.html"><a href="appendix2.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="appendix2.html"><a href="appendix2.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a></li>
<li class="chapter" data-level="2.4" data-path="appendix2.html"><a href="appendix2.html#inequalities"><i class="fa fa-check"></i><b>2.4</b> Inequalities</a></li>
<li class="chapter" data-level="2.5" data-path="appendix2.html"><a href="appendix2.html#theorems"><i class="fa fa-check"></i><b>2.5</b> Theorems</a></li>
<li class="chapter" data-level="2.6" data-path="appendix2.html"><a href="appendix2.html#statistical-models"><i class="fa fa-check"></i><b>2.6</b> Statistical Models</a></li>
<li class="chapter" data-level="2.7" data-path="appendix2.html"><a href="appendix2.html#references-2"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix3.html"><a href="appendix3.html"><i class="fa fa-check"></i><b>3</b> Asymptotic Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix3.html"><a href="appendix3.html#convergence"><i class="fa fa-check"></i><b>3.1</b> Convergence</a></li>
<li class="chapter" data-level="3.2" data-path="appendix3.html"><a href="appendix3.html#theorems-1"><i class="fa fa-check"></i><b>3.2</b> Theorems</a></li>
<li class="chapter" data-level="3.3" data-path="appendix3.html"><a href="appendix3.html#ergodic-theory"><i class="fa fa-check"></i><b>3.3</b> Ergodic Theory</a></li>
<li class="chapter" data-level="3.4" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-estimators"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Properties of Estimators</a></li>
<li class="chapter" data-level="3.5" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-test-statistics"><i class="fa fa-check"></i><b>3.5</b> Asymptotic Properties of Test Statistics</a></li>
<li class="chapter" data-level="3.6" data-path="appendix3.html"><a href="appendix3.html#references-3"><i class="fa fa-check"></i><b>3.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>4</b> OLS Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="lecture1.html"><a href="lecture1.html#the-gauss-markov-model"><i class="fa fa-check"></i><b>4.1</b> The Gauss Markov Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lecture1.html"><a href="lecture1.html#matlab"><i class="fa fa-check"></i><b>4.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture1.html"><a href="lecture1.html#the-ols-estimator"><i class="fa fa-check"></i><b>4.2</b> The OLS estimator</a><ul>
<li class="chapter" data-level="4.2.1" data-path="lecture1.html"><a href="lecture1.html#matlab-1"><i class="fa fa-check"></i><b>4.2.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture1.html"><a href="lecture1.html#ols-residual-properties"><i class="fa fa-check"></i><b>4.3</b> OLS residual properties</a><ul>
<li class="chapter" data-level="4.3.1" data-path="lecture1.html"><a href="lecture1.html#matlab-2"><i class="fa fa-check"></i><b>4.3.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture1.html"><a href="lecture1.html#finite-sample-properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>4.4</b> Finite Sample Properties of the OLS estimator</a><ul>
<li class="chapter" data-level="4.4.1" data-path="lecture1.html"><a href="lecture1.html#matlab-3"><i class="fa fa-check"></i><b>4.4.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lecture1.html"><a href="lecture1.html#references-4"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>5</b> OLS Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="lecture2.html"><a href="lecture2.html#asymptotic-theory-of-the-ols-estimator"><i class="fa fa-check"></i><b>5.1</b> Asymptotic Theory of the OLS Estimator</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lecture2.html"><a href="lecture2.html#gaussian-error-term"><i class="fa fa-check"></i><b>5.1.1</b> Gaussian Error Term</a></li>
<li class="chapter" data-level="5.1.2" data-path="lecture2.html"><a href="lecture2.html#homoskedastic-error-term"><i class="fa fa-check"></i><b>5.1.2</b> Homoskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.3" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-error-term"><i class="fa fa-check"></i><b>5.1.3</b> Heteroskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.4" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-and-autocorrelated-error-term"><i class="fa fa-check"></i><b>5.1.4</b> Heteroskedastic and Autocorrelated Error Term</a></li>
<li class="chapter" data-level="5.1.5" data-path="lecture2.html"><a href="lecture2.html#fixed-b-asymptotics"><i class="fa fa-check"></i><b>5.1.5</b> Fixed b asymptotics</a></li>
<li class="chapter" data-level="5.1.6" data-path="lecture2.html"><a href="lecture2.html#fixed-g-asymptotics"><i class="fa fa-check"></i><b>5.1.6</b> Fixed G asymptotics</a></li>
<li class="chapter" data-level="5.1.7" data-path="lecture2.html"><a href="lecture2.html#matlab-4"><i class="fa fa-check"></i><b>5.1.7</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lecture2.html"><a href="lecture2.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lecture2.html"><a href="lecture2.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2.2" data-path="lecture2.html"><a href="lecture2.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="lecture2.html"><a href="lecture2.html#matlab-5"><i class="fa fa-check"></i><b>5.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="lecture2.html"><a href="lecture2.html#references-5"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>6</b> Endogeneity</a><ul>
<li class="chapter" data-level="6.1" data-path="lecture3.html"><a href="lecture3.html#instrumental-variables"><i class="fa fa-check"></i><b>6.1</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lecture3.html"><a href="lecture3.html#matlab-6"><i class="fa fa-check"></i><b>6.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lecture3.html"><a href="lecture3.html#gmm"><i class="fa fa-check"></i><b>6.2</b> GMM</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lecture3.html"><a href="lecture3.html#step-gmm"><i class="fa fa-check"></i><b>6.2.1</b> 1-step GMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="lecture3.html"><a href="lecture3.html#step-gmm-1"><i class="fa fa-check"></i><b>6.2.2</b> 2-step GMM</a></li>
<li class="chapter" data-level="6.2.3" data-path="lecture3.html"><a href="lecture3.html#matlab-7"><i class="fa fa-check"></i><b>6.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture3.html"><a href="lecture3.html#testing-overidentifying-restrictions"><i class="fa fa-check"></i><b>6.3</b> Testing Overidentifying Restrictions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lecture3.html"><a href="lecture3.html#naive-test"><i class="fa fa-check"></i><b>6.3.1</b> Naive Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="lecture3.html"><a href="lecture3.html#hansens-test"><i class="fa fa-check"></i><b>6.3.2</b> Hansen’s Test</a></li>
<li class="chapter" data-level="6.3.3" data-path="lecture3.html"><a href="lecture3.html#special-case-conditional-homoskedasticity"><i class="fa fa-check"></i><b>6.3.3</b> Special Case: Conditional Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lecture3.html"><a href="lecture3.html#small-sample-properties-of-2sls"><i class="fa fa-check"></i><b>6.4</b> Small-Sample Properties of 2SLS</a></li>
<li class="chapter" data-level="6.5" data-path="lecture3.html"><a href="lecture3.html#many-instrument-robust-estimation"><i class="fa fa-check"></i><b>6.5</b> Many Instrument Robust Estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="lecture3.html"><a href="lecture3.html#liml"><i class="fa fa-check"></i><b>6.5.1</b> LIML</a></li>
<li class="chapter" data-level="6.5.2" data-path="lecture3.html"><a href="lecture3.html#jive"><i class="fa fa-check"></i><b>6.5.2</b> JIVE</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lecture3.html"><a href="lecture3.html#hausman-test"><i class="fa fa-check"></i><b>6.6</b> Hausman Test</a></li>
<li class="chapter" data-level="6.7" data-path="lecture3.html"><a href="lecture3.html#references-6"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>7</b> Non-Parametric Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="lecture4.html"><a href="lecture4.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="lecture4.html"><a href="lecture4.html#discrete-x---cell-estimator"><i class="fa fa-check"></i><b>7.2</b> Discrete x - Cell Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="lecture4.html"><a href="lecture4.html#local-non-parametric-estimation---kernels"><i class="fa fa-check"></i><b>7.3</b> Local Non-Parametric Estimation - Kernels</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lecture4.html"><a href="lecture4.html#estimator-examples"><i class="fa fa-check"></i><b>7.3.1</b> Estimator examples:</a></li>
<li class="chapter" data-level="7.3.2" data-path="lecture4.html"><a href="lecture4.html#kernel-examples"><i class="fa fa-check"></i><b>7.3.2</b> Kernel examples:</a></li>
<li class="chapter" data-level="7.3.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-bandwidth"><i class="fa fa-check"></i><b>7.3.3</b> Choice of the optimal bandwidth</a></li>
<li class="chapter" data-level="7.3.4" data-path="lecture4.html"><a href="lecture4.html#inference-1"><i class="fa fa-check"></i><b>7.3.4</b> Inference</a></li>
<li class="chapter" data-level="7.3.5" data-path="lecture4.html"><a href="lecture4.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.3.5</b> Bias-variance trade-off</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lecture4.html"><a href="lecture4.html#global-non-parametric-estimation---series"><i class="fa fa-check"></i><b>7.4</b> Global Non-Parametric Estimation - Series</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lecture4.html"><a href="lecture4.html#examples"><i class="fa fa-check"></i><b>7.4.1</b> Examples</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture4.html"><a href="lecture4.html#estimation"><i class="fa fa-check"></i><b>7.4.2</b> Estimation</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-k"><i class="fa fa-check"></i><b>7.4.3</b> Choice of the optimal <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture4.html"><a href="lecture4.html#inference-2"><i class="fa fa-check"></i><b>7.4.4</b> Inference</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture4.html"><a href="lecture4.html#kernel-vs-series"><i class="fa fa-check"></i><b>7.4.5</b> Kernel vs Series</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lecture4.html"><a href="lecture4.html#references-7"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>8</b> Variable Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="lecture5.html"><a href="lecture5.html#lasso"><i class="fa fa-check"></i><b>8.1</b> Lasso</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lecture5.html"><a href="lecture5.html#choosing-the-optimal-lambda"><i class="fa fa-check"></i><b>8.1.1</b> Choosing the optimal lambda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lecture5.html"><a href="lecture5.html#pre-testing"><i class="fa fa-check"></i><b>8.2</b> Pre-Testing</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lecture5.html"><a href="lecture5.html#omitted-variable-bias"><i class="fa fa-check"></i><b>8.2.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.2.2" data-path="lecture5.html"><a href="lecture5.html#pre-test-bias"><i class="fa fa-check"></i><b>8.2.2</b> Pre-test bias</a></li>
<li class="chapter" data-level="8.2.3" data-path="lecture5.html"><a href="lecture5.html#partioned-regression"><i class="fa fa-check"></i><b>8.2.3</b> Partioned Regression</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lecture5.html"><a href="lecture5.html#post-double-selection"><i class="fa fa-check"></i><b>8.3</b> Post Double Selection</a></li>
<li class="chapter" data-level="8.4" data-path="lecture5.html"><a href="lecture5.html#references-8"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matlabcode.html"><a href="matlabcode.html"><i class="fa fa-check"></i><b>9</b> Matlab Code</a><ul>
<li class="chapter" data-level="9.1" data-path="matlabcode.html"><a href="matlabcode.html#lecture-1"><i class="fa fa-check"></i><b>9.1</b> Lecture 1</a></li>
<li class="chapter" data-level="9.2" data-path="matlabcode.html"><a href="matlabcode.html#lecture-2"><i class="fa fa-check"></i><b>9.2</b> Lecture 2</a></li>
<li class="chapter" data-level="9.3" data-path="matlabcode.html"><a href="matlabcode.html#lecture-3"><i class="fa fa-check"></i><b>9.3</b> Lecture 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture4" class="section level1">
<h1><span class="header-section-number">Lecture 7</span> Non-Parametric Estimation</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<p>Non-parametric regression is a flexible estimation procedure for (i) regression functions <span class="math inline">\(\mathbb E [y|x ] = g (x)\)</span> and (ii) density functions <span class="math inline">\(f(x)\)</span>. You want to let your data to tell you how flexible you can afford to be in terms of estimation procedures. Non-parametric regression is naturally introduced in terms of fitting a curve.</p>
<p>Consider the problem of estimating the Conditional Expectation Function, defined as <span class="math inline">\(\mathbb E [y_i |x_i ] = g(x_i)\)</span> given data <span class="math inline">\(D = (x_i, y_i)_{i=1}^n\)</span> under minimal assumption of <span class="math inline">\(g(\cdot)\)</span>, e.g. smoothness. There are two main methods:</p>
<ol style="list-style-type: decimal">
<li>Local methods: Kernel-based estimation</li>
<li>Global methods: Series-based estimation</li>
</ol>
<p>Another way of looking at non-parametrics is to do estimation/inference without specifying functional forms. With no assumptions, informative inference is impossible. Non parametrics tries to work with functional restrictions—continuity, differentiability, etc.—rather than pre-specifying functional form.</p>
</div>
<div id="discrete-x---cell-estimator" class="section level2">
<h2><span class="header-section-number">7.2</span> Discrete x - Cell Estimator</h2>
<p>Suppose that <span class="math inline">\(x\)</span> can take <span class="math inline">\(R\)</span> distinct values, e.g. gender <span class="math inline">\(R=2\)</span>, years of schooling <span class="math inline">\(R=20\)</span>, gender<span class="math inline">\(\times\)</span>years of schooling <span class="math inline">\(R = 2\times20\)</span>.</p>
<p>A simple way for estimating <span class="math inline">\(\mathbb E \left[ y |x \right] = g(x)\)</span> is to split the sample to include observations with <span class="math inline">\(x_i = x\)</span> and calculate the sample mean of <span class="math inline">\(\bar{y}\)</span> for these observations. Note that this requires no assumptions about how <span class="math inline">\(\mathbb E [y_i |x_i]\)</span> varies with <span class="math inline">\(x\)</span> since we fit a different value for each value <span class="math inline">\(x\)</span>.
<span class="math display">\[
    \hat{g}(x) = \frac{1}{\# \{ i: x_i = x \}} \sum_{i : x_i = x} y_i 
\]</span></p>
<p>Issues:</p>
<ul>
<li><strong>Curse of dimensionality</strong>: if <span class="math inline">\(R\)</span> is big compared to <span class="math inline">\(n\)</span>, there will be only a small number of observations per <span class="math inline">\(x\)</span> values. If <span class="math inline">\(x_i\)</span> is continuous, <span class="math inline">\(R=n\)</span> with probability 1. Solution: we can borrow information about <span class="math inline">\(g_0(x)\)</span> using neighboring observations of <span class="math inline">\(x\)</span>.</li>
<li>Averaging for each separate <span class="math inline">\(x_r\)</span> value is only feasible in cases where <span class="math inline">\(x_i\)</span> is coarsely discrete.</li>
</ul>
</div>
<div id="local-non-parametric-estimation---kernels" class="section level2">
<h2><span class="header-section-number">7.3</span> Local Non-Parametric Estimation - Kernels</h2>
<p>Suppose we believe that <span class="math inline">\(\mathbb E [y_i |x_i]\)</span> is a smooth function of <span class="math inline">\(x_i\)</span> – e.g. continuous, differentiable, etc. Then it should not change too much across values of <span class="math inline">\(x\)</span> that are close to each other: we can estimate the conditional expectation at <span class="math inline">\(x = \bar{x}\)</span> by averaging <span class="math inline">\(y\)</span>’s over the values of <span class="math inline">\(x\)</span> that are “close”" to <span class="math inline">\(\bar{x}\)</span>. This procedure relies on two (three) arbitrary choices:</p>
<ul>
<li>Choice of the <strong>kernel function</strong> <span class="math inline">\(K (\cdot)\)</span>; it is used to weight “far out”" observations, such that
<ul>
<li><span class="math inline">\(K: \mathbb R \to \mathbb R\)</span></li>
<li><span class="math inline">\(K\)</span> is symmetric: <span class="math inline">\(K(\bar{x} + x_i) = K(\bar{x} - x_i)\)</span></li>
<li><span class="math inline">\(\lim_{x_i \to \infty}K(x_i - \bar{x}) = 0\)</span></li>
</ul></li>
<li>Choice of the **bandwidth} <span class="math inline">\(h\)</span>: it measures the size of a ``small’’ window around <span class="math inline">\(\bar{x}\)</span>, e.g. <span class="math inline">\((\bar{x} - h, \bar{x} + h)\)</span>.</li>
<li>Choice of the local estimation procedure. Examples are locally constant, a.k.a. Nadaraya-Watson (<strong>NW</strong>), and locally linear (<strong>LL</strong>).</li>
</ul>
<blockquote>
<p>Generally, the choice of <span class="math inline">\(h\)</span> is more important than <span class="math inline">\(K(\cdot)\)</span> in low dimensional settings.</p>
</blockquote>
<p>We need to define what is an “optimal” <span class="math inline">\(h\)</span>, depending on the smoothness level of <span class="math inline">\(g_0\)</span>, typically unknown. The choice of <span class="math inline">\(h\)</span> relates to the bias-variance trade-off:</p>
<ul>
<li>large <span class="math inline">\(h\)</span>: small variance, higher bias;</li>
<li>small <span class="math inline">\(h\)</span>: high variance, smaller bias.</li>
</ul>
<blockquote>
<p>Note that <span class="math inline">\(K_h (\cdot) = K (\cdot / h)\)</span>.</p>
</blockquote>
<div id="estimator-examples" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Estimator examples:</h3>
<ul>
<li><strong>Nadaraya-Watson</strong> estimator, or locally constant estimator. It assumes the CEF locally takes the form <span class="math inline">\(g(x) = \beta_0(x)\)</span>. The local parameter is estimated as:
<span class="math display">\[
 \hat{\beta}_0 (\bar{x}) = \arg\min_{\beta_0}  \quad  \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 \big)^2 \Big]
\]</span></li>
</ul>
<div class="figure">
<img src="figures/Fig_521.png" alt="" />
<p class="caption">NW regression</p>
</div>
<p>The Nadaraya-Watson estimate of the CEF takes the form:
<span class="math display">\[
    \mathbb E_n \left[ y | x = \bar{x}\right] = \hat{g}(\bar{x}) = \frac{\sum_{i=1}^n y_i K_h (x_i - \bar{x})}{\sum_{i=1}^n K_h (x_i - \bar{x})} \\
\]</span></p>
<ul>
<li><strong>Local Linear</strong> estimator. It assumes the CEF locally takes the form <span class="math inline">\(g(x) = \beta_0(x) + \beta_1(x) x\)</span>. The local parameters are estimated as:
<span class="math display">\[
\left( \hat{\beta}_0 (\bar{x}), \hat{\beta}_1 (\bar{x}) \right) = \arg\min_{\beta_0, \beta_1}  \quad   \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 \big)^2 \Big]
\]</span></li>
</ul>
<div class="figure">
<img src="figures/Fig_522.png" alt="" />
<p class="caption">LL regression</p>
</div>
<p>In this case, we do LS estimate with <span class="math inline">\(i\)</span>’s contribution of residual weighted by the kernel <span class="math inline">\(K_h (x_i - \bar{x})\)</span>. The final estimate at <span class="math inline">\(\bar{x}\)</span> is given by:
<span class="math display">\[
  \hat{g} (\bar{x}) = \hat{\beta}_0 (\bar{x}) + (\bar{x} - \bar{x}) \hat{\beta}_1 (\bar{x}) = \hat{\beta}_0 (\bar{x})
\]</span>
since we have centered the <span class="math inline">\(x_s\)</span> at <span class="math inline">\(\bar{x}\)</span> in the kernel.
- It is possible to add linearly higher order polynomials, e.g. do locally quadratic least squares using loss function:</p>
<p><span class="math display">\[
  \mathbb E_n \left[ K_h (x_i - \bar{x}) \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 - (x_i - \bar{x})^2 \beta_2 \big)^2 \right] 
\]</span></p>
</div>
<div id="kernel-examples" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Kernel examples:</h3>
<ul>
<li><strong>Uniform kernel</strong>. LS restricted to sample <span class="math inline">\(i\)</span> such that <span class="math inline">\(x_i\)</span> within <span class="math inline">\(h\)</span> of <span class="math inline">\(\bar{x}\)</span>.
<span class="math display">\[
\begin{aligned}
&amp; K (\cdot) = \mathbb I\{ \cdot \in [-1, 1] \}  \\
&amp; K_h (\cdot) = \mathbb I\{ \cdot/h \in [-1, 1] \} = \mathbb I\{ \cdot \in [-h, h] \}  \\
&amp; K_h (x_i - \bar{x}) = \mathbb I\{ x_i - \bar{x} \in [-h, h] \}  = \mathbb I\{ x_i \in [\bar{x}-h, \bar{x} + h] \} 
\end{aligned}
\]</span>
Employed together with the locally linear estimator, the estimation procedure reduces to **local least squares}. The loss function is:
<span class="math display">\[
\mathbb E_n \Big[ K_n (x_i - \bar{x}) \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2 \Big] = \frac{1}{n} \sum_{i: x_i \in [\bar{x}-h, \bar{x} +h ]}  \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2
\]</span></li>
</ul>
<p>The more local is the estimation, the more appropriate the linear regression: if <span class="math inline">\(g_0\)</span> is smooth, <span class="math inline">\(g_0(\bar{x}) + g_0&#39;(\bar{x}) (x_i - \bar{x})\)</span> is a better approximation for <span class="math inline">\(g_0 (x_i)\)</span>.</p>
<p>However, the uniform density is not a good kernel choice as it produces discontinuous CEF estimates. The following are two popular alternative choices that produce continuous CEF estimates.</p>
<ul>
<li><p><strong>Epanechnikov kernel</strong>
<span class="math display">\[
  K_h(x_i - \bar{x}) = \frac { 3 } { 4 } \left( 1 - (x_i - \bar{x}) ^ { 2 } \right)  \mathbb I\{ x_i \in [\bar{x}-h, \bar{x} + h] \} 
\]</span></p></li>
<li><p><strong>Normal or Gaussian kernel</strong>
<span class="math display">\[
  K_\phi (x_i - \bar{x})  = \frac { 1 } { \sqrt { 2 \pi } } \exp \left( - \frac { (x_i - \bar{x}) ^ { 2 } } { 2 } \right)
\]</span></p></li>
</ul>
<div class="figure">
<img src="figures/Fig_523.png" alt="" />
<p class="caption">Kernelsh</p>
</div>
<ul>
<li><strong>K-Nearest Neighbors (KNN)</strong>: choose bandwidth so that there is a fixed number of observations in each kernel. This kernel is different from the others since it takes a nonparamentric form.</li>
</ul>
</div>
<div id="choice-of-the-optimal-bandwidth" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Choice of the optimal bandwidth</h3>
<p>Practical methods:</p>
<ul>
<li><p><strong>Eyeball Method.</strong> (i) Choose a bandwidth (ii) Estimate the regression function (iii) Look at the result: if it looks more wiggly than you would like, increase the bandwidth: if it looks more smooth than you would like, decrease the bandwidth. Con: It only works for <span class="math inline">\(\dim(x_i) = 1\)</span> or <span class="math inline">\(2\)</span>.</p></li>
<li><p><strong>Rule of Thumb.</strong> For example, Silverman’s rule of thumb: <span class="math inline">\(h = \left( \frac{4 \hat{\sigma}^5}{3n} \right)^{\frac{1}{5}}\)</span>.
Con: It requires too much knowledge about <span class="math inline">\(g_0\)</span> (i.e. normality) which you don’t have.</p></li>
<li><p><strong>Cross Validation.</strong> Under some assumptions, CV will approximately gives the MSE optimal bandwidth. The basic idea is to evaluate quality of the bandwidth by looking at how well the resulting estimator forecasts in the given sample.</p></li>
</ul>
<p>Leave-one-out CV. For each <span class="math inline">\(h &gt; 0\)</span> and each <span class="math inline">\(i\)</span>, <span class="math inline">\(\hat{g}_{-i} (x_i)\)</span> is the estimate of the conditional expectation at <span class="math inline">\(x_i\)</span> using bandwidth <span class="math inline">\(h\)</span> and all observations expect observation <span class="math inline">\(i\)</span>. The CV bandwidth is defined as
<span class="math display">\[
    \hat{h} = \arg \min_h CV(h) = \arg \min_h \sum_{i=1}^n  \Big( y_i -  \hat{g}_{-i} (x_i) \Big)^2
\]</span></p>
<blockquote>
<p>How to choose the optimal bandwidth:</p>
<ul>
<li>Select a value for <span class="math inline">\(h\)</span>.</li>
<li>For each observation <span class="math inline">\(i\)</span>, calculate
<span class="math display">\[
\hat{g}_{-i} (x_i) = \frac{\sum_{j \ne i} y_j K_h (x_j - x_i) }{\sum_{i=1}^n K_h (x_j - x_i)}, \qquad e_{i,h}^2 = \left(y_i - \hat{g}_{-i} (x_i) \right)^2
\]</span></li>
<li>Calculate <span class="math inline">\(\text{CV}(h) = \sum_{i=1}^n e^2_{i,h}\)</span>.</li>
<li>Repeat for each <span class="math inline">\(h\)</span> and choose the one that minimizes <span class="math inline">\(\text{CV}(h)\)</span>.</li>
</ul>
</blockquote>
<div class="figure">
<img src="figures/Fig_524.png" alt="" />
<p class="caption">Optimal Bandwidth</p>
</div>
</div>
<div id="inference-1" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Inference</h3>
<p><strong>Theorem</strong>:
Consider data <span class="math inline">\(\{ y_i, x_i \}_{i=1}^n\)</span>, iid and suppose that <span class="math inline">\(y_i = g(x_i) + \varepsilon_i\)</span> where <span class="math inline">\(\mathbb E[\varepsilon_i|x_i] = 0\)</span>. Assume that <span class="math inline">\(x_i \in Interior(X)\)</span> where <span class="math inline">\(X \subseteq \mathbb R\)</span>, <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(f(x)\)</span> are three times continuously differentiable, and <span class="math inline">\(f(x) &gt; 0\)</span> on <span class="math inline">\(X\)</span>. <span class="math inline">\(f(x)\)</span> is the probability density of <span class="math inline">\(x \in X\)</span> , and <span class="math inline">\(g(x)\)</span> is the function of interest. Suppose that <span class="math inline">\(K(\cdot)\)</span> is a kernel function. Suppose <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(h\to0\)</span> , <span class="math inline">\(nh\to\infty\)</span>, and <span class="math inline">\(nh^7\to0\)</span>. Then for any fixed <span class="math inline">\(x\in X\)</span>,
<span class="math display">\[
  AMSE = \sqrt{nh} \Big( \hat{g}(x) - g(x) - h^2 B(x)\Big) \overset{d}{\to} N \left( 0, \frac{\kappa \sigma^2(x)}{f(x)}\right)
\]</span>
for <span class="math inline">\(\sigma^2(x) = Var(y_i|x_i = x)\)</span>, <span class="math inline">\(\kappa = \int K^2(v)dv\)</span>, and <span class="math inline">\(B(x) = \frac{\kappa_2}{2} \frac{f&#39;(x)g&#39;(x) + f(x) g&#39;&#39;(x)}{f(x)}\)</span> where <span class="math inline">\(\kappa_2 = \int v^2 K(v)dv\)</span>.</p>
<blockquote>
<p>Remark on the theorem:</p>
<ul>
<li>If the function is smooth enough and the bandwidth small enough, you can ignore the bias relative to sampling variation. To make this plausible, use a smaller bandwidth than would be the “optimal”.</li>
<li>All kernel regression estimators can be written as a weighted average
<span class="math display">\[
\hat{g}(x) = \frac{1}{n} \sum_{i=1}^n w_i (x) y_i, \quad \text{ with } \quad w_i (x) = \frac{n K_h (x_i - x)}{\sum_{i=1}^n K_h (x_i - x)} 
\]</span>
Do inference as if you were estimating a mean <span class="math inline">\(\mathbb E[z_i]\)</span> with sample mean <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n z_i\)</span> using <span class="math inline">\(z_i = w_i (x) y_i\)</span>.</li>
<li>If you are doing inference at more than one value of <span class="math inline">\(x\)</span>, do inference as in the previous point, treating each value of <span class="math inline">\(x\)</span> as a different sample mean and note that even with independent data, these means will be correlated in general because there will generally be some common observations in to each of the averages. If you have a time series, make sure you account for correlation between the observations going in the different averages even if they don’t overlap.</li>
</ul>
</blockquote>
<p>Issue when doing inference: the estimation of the bandwidth from the data is generally not accounted for in the distributional approximation (when doing inference). In large-samples, this is unlikely to lead to large changes, but uncertainty is understated in small samples.</p>
</div>
<div id="bias-variance-trade-off" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Bias-variance trade-off</h3>
<p><strong>Theorem</strong>:
For any estimator mean-square error MSE is decomposable into variance and bias-squared:
<span class="math display">\[
\text{MSE} (\bar{x}, \hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right] = \mathbb E \Big[\underbrace{ \hat{g}(\bar{x}) - g_0 (\bar{x}) }_{\text{Bias}} \Big]^2 +  Var (\hat{g} (\bar{x})). 
\]</span></p>
<p>The theorem follows from the following corollary.</p>
<p><strong>Corollary</strong>:
Let <span class="math inline">\(A\)</span> be a random variable and <span class="math inline">\(\theta_0\)</span> a fixed parameter. Then,
<span class="math display">\[
    \mathbb E [ (A - \theta_0)^2] = Var (A) + \mathbb E [A-\theta_0]^2
\]</span></p>
<p><strong>Proof</strong>:
<span class="math display">\[
  \begin{aligned}
    \mathbb E [ (A - \theta_0)^2] &amp; = \mathbb E[A^2] - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \\
    &amp;  = \mathbb E[A^2] \underbrace{-  \mathbb E[A]^2 + E[A]^2}_{\text{add and subtract}} - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \\
    &amp;  = Var(A) + \mathbb E [A]^2 - 2 \theta_0 \mathbb E [A ] + \mathbb E [\theta_0] \\
    &amp; = Var(A) + \mathbb E [A - \theta_0]^2
    \end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbb E [ (A - \theta_0)^2] = \mathbb E [A - \theta_0]^2\)</span>.
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p><strong>Criteria to solve the bias-variance trade-off</strong>:</p>
<ul>
<li><p><strong>Mean squared error (MSE)}:
<span class="math display">\[
  \text{MSE} (\bar{x}) (\hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right] 
\]</span>
</strong>NB!** This is the criterium we are going to use.</p></li>
<li><p><strong>Integrated mean squared error (IMSE)</strong>:
<span class="math display">\[
  \text{IMSE} ( \hat{g} ) = \mathbb E \left[ \int | \hat{g} (x) - g_0 (x) |^2 \mathrm{d} F(x)  \right] 
\]</span></p></li>
<li><p>Type I - Type II error.</p></li>
</ul>
<p>Hansen (2019): the theorem above implies that we can asymptotically approximate the MSE as
<span class="math display">\[
\text{AMSE} = \Big( h^2 \sigma_k^2 B(x) \Big)^2 + \frac{\kappa \sigma^2(x)}{nh f(x)} \approx \text{const} \cdot \left( h^4 + \frac{1}{n h} \right) 
\]</span></p>
<p>Where</p>
<ul>
<li><span class="math inline">\(Var \propto \frac{1}{h n}\)</span>, where you can think of <span class="math inline">\(n h\)</span> as the <strong>effective sample size</strong>.</li>
<li>Bias <span class="math inline">\(\propto h^2\)</span>, derived if <span class="math inline">\(g_0\)</span> is twice continuously differentiable using Taylor expansion.</li>
</ul>
<p>Trade-off: the asymptotic MSE is dominated by the larger of <span class="math inline">\(h^4\)</span> and <span class="math inline">\(\frac{1}{h n}\)</span>. Notice that the bias is increasing in <span class="math inline">\(h\)</span> and the variance is decreasing in <span class="math inline">\(h\)</span> (more smoothing means more observations are used for local estimation: this increases the bias but decreases estimation variance). To select <span class="math inline">\(h\)</span> to minimize the asymptotic MSE, these two components should balance each other:
<span class="math display">\[
\frac{1}{h n} \propto h^4 \quad \Rightarrow \quad  h \propto n^{-1/5} 
\]</span></p>
<p>This result means that the bandwidth should take the form <span class="math inline">\(h = c \cdot n^{-1/5}\)</span>. The optimal constant <span class="math inline">\(c\)</span> depends on the kernel <span class="math inline">\(k\)</span> the bias function <span class="math inline">\(B(x)\)</span> and the marginal density <span class="math inline">\(f_x(x)\)</span>. A common misinterpretation is to set <span class="math inline">\(h = n^{-1/5}\)</span> which is equivalent to setting <span class="math inline">\(c = 1\)</span> and is completely arbitrary. Instead, an empirical bandwidth selection rule such as cross-validation should be used in practice.</p>
</div>
</div>
<div id="global-non-parametric-estimation---series" class="section level2">
<h2><span class="header-section-number">7.4</span> Global Non-Parametric Estimation - Series</h2>
<p>The goal is to try to globally approximate the CEF with a function <span class="math inline">\(g(x)\)</span>. Series methods are based on the **Stone-Weierstrass theorem}: a real-valued continuous function <span class="math inline">\(g(x)\)</span> defined in a compact set can be approximated with polynomials for any degree of accuracy
<span class="math display">\[
    g_0 (x) = p_1 (x) \beta _1 + \dots + p_K (x) \beta_K + r(x)
\]</span>
where <span class="math inline">\(p_1(x), \dots, p_K(x)\)</span> are called ``a dictionary of approximating series’’ and <span class="math inline">\(r(x)\)</span> is a remainder function. If <span class="math inline">\(p_1(x), \dots, p_K(x)\)</span> are sufficiently rich, <span class="math inline">\(r(x)\)</span> will be small. If <span class="math inline">\(K \to \infty\)</span>, then <span class="math inline">\(r \to 0\)</span>.</p>
<blockquote>
<p>Example - Taylor series: if <span class="math inline">\(g(x)\)</span> is infinitely differentiable, then
<span class="math display">\[
g(x) = \sum_{k=0}^{\infty } a_k x^k
\]</span>
where <span class="math inline">\(a_k = \frac{1}{k!} \frac{\partial^k g_0}{\partial x^k}\)</span>.</p>
</blockquote>
<p>The basic idea is to approximate the infinite sum by chopping it off after <span class="math inline">\(K\)</span> terms and then estimate the coefficients by OLS.</p>
<p><strong>Series estimation</strong>:</p>
<ul>
<li>Choose <span class="math inline">\(K\)</span>, i.e. the number of series terms, and an approximating dictionary <span class="math inline">\(p_1(x), \dots, p_K(x)\)</span></li>
<li>Expand data to <span class="math inline">\(D = \left( y_i, p_1(x_i), \dots, p_K(x_i) \right)_{i=1}^n\)</span></li>
<li>Estimate OLS to get <span class="math inline">\(\hat{\beta}_1, \dots, \hat{\beta}_K\)</span></li>
<li>Set <span class="math inline">\(\hat{g}(x) = p_1 (x)\hat{\beta}_1 + \dots + p_K(x) \hat{\beta}_K\)</span></li>
</ul>
<div id="examples" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Examples</h3>
<ul>
<li><p><strong>Monomials</strong>: <span class="math inline">\(p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \dots\)</span></p></li>
<li><p><strong>Hermite Polynomials</strong>: <span class="math inline">\(p_1(x) = 1\)</span>, <span class="math inline">\(p_2(x) = x\)</span>, <span class="math inline">\(p_3(x)=x^2 -1\)</span>, <span class="math inline">\(p_4(x)= x^3 - 3x, \dots\)</span>. Con: <strong>edge effects</strong>. The estimated function is particularly volatile at the edges of the sample space (Gibbs effect)</p></li>
</ul>
<div class="figure">
<img src="figures/Fig_531.png" alt="" />
<p class="caption">Hermite Polynomials</p>
</div>
<ul>
<li><p><strong>Trig Polynomials</strong>: <span class="math inline">\(p_1(x) = 1\)</span>, <span class="math inline">\(p_2(x) = \cos 2 \pi x\)</span>, <span class="math inline">\(p_3(x)= \sin 2 \pi x\)</span>, <span class="math inline">\(p_4(x) = \cos 2 \pi x \cdot 2 x \dots\)</span>. Pro: cyclical therefore good for series. Con: edge effects</p></li>
<li><p><strong>B-splines</strong>: recursively constructed using knot points
<span class="math display">\[
B_{i, 0} = \begin{cases}
1 &amp; \text{if } t_i \leq x &lt; t_{i+1} \\ 0 &amp; \text{otherwise}
\end{cases} \qquad B_{i_k} (x) = \frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) +  \frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x)
\]</span>
where <span class="math inline">\(t_0, \dots, t_i, \dots\)</span> are knot points and <span class="math inline">\(k\)</span> is the order of the spline.
Pro: faster rate of convergence and lower asymptotic bias.</p></li>
</ul>
</div>
<div id="estimation" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Estimation</h3>
<p>Given <span class="math inline">\(K\)</span>, inference proceeds exactly as if one had run an OLS of <span class="math inline">\(y\)</span> on <span class="math inline">\((p_k)_{k=1}^K\)</span>. The idea is that you ignore that you are doing non-parametric regression as long as you believe you have put enough terms (high <span class="math inline">\(K\)</span>). Then the function is smooth enough so that the bias of the approximation is small relative to the variance (see Newey, 1997). Note that his approximation does not account for data-dependent estimation of the bandwidth.</p>
<p>Newey (1997): results about consistency of <span class="math inline">\(\hat{g}\)</span> and asymptotic normality of <span class="math inline">\(\hat{g}\)</span>.</p>
<ul>
<li>OLS: <span class="math inline">\(\hat{\beta} \overset{p}{\to} \beta_0\)</span></li>
<li>Non-parametric: you have a sequence <span class="math inline">\(\{\beta_k\}_{k=1}^K\)</span> with <span class="math inline">\(\hat{\beta}_k \overset{p}{\to} \beta_k\)</span> as <span class="math inline">\(n \to \infty\)</span> (as <span class="math inline">\(k \to \infty\)</span>). However, this does not make sense because <span class="math inline">\(\{\beta_k\}\)</span> is not constant. Moreover, <span class="math inline">\(\beta_k\)</span> is not the quantity of interest. We want to make inference on <span class="math inline">\(\hat{g}(x)\)</span>.</li>
</ul>
<p><strong>Theorem</strong>:
Under regularity conditions, including <span class="math inline">\(| | \hat{\beta} - \beta_0 | | \overset{p}{\to} 0\)</span>,</p>
<ul>
<li>Uniform Consistency: <span class="math inline">\(\sup_x | \hat{g}(x) - g_0(x)| \overset{p}{\to} 0\)</span></li>
<li>Mean-square Consistency: <span class="math inline">\(\int | \hat{g}(x) - g_0(x)|^2 \mathrm{d} F(x) \overset{p}{\to} 0\)</span></li>
</ul>
<p><strong>Theorem</strong>:
Under the following assumptions:</p>
<ul>
<li><span class="math inline">\((x_i, y_i)\)</span> are iid and <span class="math inline">\(Var(y_i|x_i)\)</span> is bounded;</li>
<li>For all <span class="math inline">\(K\)</span>, there exists a non-singular matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(A = \left[ (B p(x)) (B p(x))&#39; \right]\)</span> where <span class="math inline">\(p(x) = \left( p_1(x), \dots, p_K (x) \right)\)</span> has the properties that <span class="math inline">\(\lambda_{\min} (A)^{-1} = O(1)\)</span>. In addition, <span class="math inline">\(\sup_x | | B p(x) | | = o(\sqrt{K/n})\)</span>.</li>
<li>There exists <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta_K\)</span> for all <span class="math inline">\(K\)</span> such that
<span class="math display">\[
  \sup_x | g_0 (x) - p(x) \beta_K | = O_p(K^{-\alpha})
\]</span></li>
</ul>
<p>Then, it holds that</p>
<p><span class="math display">\[
  \text{IMSE = }\int \left( g_0 (x) - \hat{g} (x) \right)^2 \mathrm{d} F(x) = O_p \left( \frac{K}{n} + K^{-2\alpha}\right)
\]</span></p>
</div>
<div id="choice-of-the-optimal-k" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Choice of the optimal <span class="math inline">\(K\)</span></h3>
<p>The bias-variance trade-off for series comes in through the choice of <span class="math inline">\(K\)</span>:</p>
<ul>
<li>Higher <span class="math inline">\(K\)</span>: smaller bias, since we are leaving out less terms form the infinite sum.</li>
<li>Smaller <span class="math inline">\(K\)</span>: smaller variance, since we are estimating less regression coefficients from the same amount of data.</li>
</ul>
<p><strong>Cross-validation for series</strong>:
For each <span class="math inline">\(K \geq 0\)</span> and for each <span class="math inline">\(i=1, \dots, n\)</span>, consider</p>
<p><span class="math display">\[
  D_{-i} = \{ (x_1, y_1), \dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \dots (x_n, y_n) \}
\]</span>
and calculate <span class="math inline">\(\hat{g}^{(K)}_{-i} (x)\)</span> using series estimate with <span class="math inline">\(p_1(x), \dots, p_K (x)\)</span> in order to get <span class="math inline">\(e^{(K)}_i = y_i - \hat{g}^{(K)}_{-i} (x_i)\)</span>. Choose <span class="math inline">\(\hat{K}\)</span> such that</p>
<p><span class="math display">\[
  \hat{K} = \arg \min_K \mathbb E_n \left[ {e^{(K)}_i}^2 \right]
\]</span></p>
</div>
<div id="inference-2" class="section level3">
<h3><span class="header-section-number">7.4.4</span> Inference</h3>
<p>Consider the data <span class="math inline">\(D = \{ (x_i, y_i) \}_{i=1}^n\)</span> such that <span class="math inline">\(y_i = g_0 (x_i) + \varepsilon_i\)</span>. You may want to form confidence intervals for quantities that depends on <span class="math inline">\(g_0\)</span>.</p>
<blockquote>
<p>Example: <span class="math inline">\(\theta_0\)</span> functional forms of interests:</p>
<ul>
<li>Point estimate: <span class="math inline">\(\theta_0 = g_0 (\bar{x} )\)</span> for fixed <span class="math inline">\(\bar{x}\)</span></li>
<li>Interval estimate: <span class="math inline">\(\theta_0 = g_0 (\bar{x}_2) - g_0 (\bar{x}_1)\)</span></li>
<li>Point derivative estimate: <span class="math inline">\(\theta_0 = g_0 &#39; (\bar{x})\)</span> at <span class="math inline">\(\bar{x}\)</span></li>
<li>Average derivative <span class="math inline">\(\theta_0 = \mathbb E [g_0 &#39; (x) ]\)</span></li>
<li>Consumer surplus: <span class="math inline">\(\theta_0 = \int_a^b g_0(x)dx \quad\)</span> when <span class="math inline">\(g_0\)</span> is a demand function.</li>
</ul>
</blockquote>
<p>Those estimates are functionals: maps from a function to a real number. We are doing inference on a function now, not on a point estimate.</p>
<p>In order to form a confidence interval for <span class="math inline">\(\theta_0\)</span>, with series you can</p>
<ul>
<li><strong>Undersmooth</strong>: in order to apply a , you need deviations around the function to be approximately gaussian. Undersmoothing makes the function oscillate much more than the curve you are estimating in order to obtain such guassian deviations.</li>
</ul>
<div class="figure">
<img src="figures/Fig_541.png" alt="" />
<p class="caption">Smoothing</p>
</div>
<blockquote>
<p>Example: if on the contrary you oversmooth (e.g. <span class="math inline">\(g_0\)</span> linear), errors are going to constantly be on either one or the other side of the curve <span class="math inline">\(\to\)</span> not gaussian!</p>
</blockquote>
<ul>
<li>Use the <strong>delta method</strong>. It would usually require more series terms than a criterion like cross-validation would suggest.</li>
</ul>
<p><strong>Theorem</strong>:
Under the assumptions of the consistency theorem
<span class="math display">\[
    \frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 + B(r_K) \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
\]</span></p>
<p><strong>Theorem</strong>:
Under the assumptions of the consistency theorem and <span class="math inline">\(\sqrt{n} K^{-\alpha} = o(1)\)</span> (or equivalently <span class="math inline">\(n K^{-2\alpha} = O(1)\)</span> in Hansen),
<span class="math display">\[
  \frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
\]</span></p>
<blockquote>
<p>Remark on convergence rate:</p>
<ul>
<li>The rate of convergence of splines is faster than for power series (Newey 1997).</li>
<li>We have **undersmoothing} if <span class="math inline">\(\sqrt{n} K^{\alpha} = o(1)\)</span> (see comment below)</li>
<li>Usually, in order to prove asymptotic normality, we first prove unbiasedness. However here we have a <strong>biased</strong> estimator but we make the bias converge to zero faster than the variance.</li>
</ul>
</blockquote>
<p>Hansen (2019): The critical condition is the assumption that <span class="math inline">\(\sqrt{n} K^{\alpha} = o(1)\)</span> This requires that <span class="math inline">\(K \to \infty\)</span> at a rate faster than <span class="math inline">\(n^{\frac{1}{2\alpha}}\)</span> This is a troubling condition. The optimal rate for estimation of <span class="math inline">\(g(x)\)</span> is <span class="math inline">\(K = O(n^{\frac{1}{1+ 2\alpha}})\)</span>. If we set <span class="math inline">\(K = n^{\frac{1}{1+ 2\alpha}}\)</span> by this rule then <span class="math inline">\(n K^{-2\alpha} = n^{\frac{1}{1+ 2\alpha}} \to \infty\)</span> not zero. Thus this assumption is equivalent to assuming that <span class="math inline">\(K\)</span> is much larger than optimal. The reason why this trick works (that is, why the bias is negligible) is that by increasing <span class="math inline">\(K\)</span> the asymptotic bias decreases and the asymptotic variance increases and thus the variance dominates. Because <span class="math inline">\(K\)</span> is larger than optimal, we typically say that <span class="math inline">\(\hat{g}(x)\)</span> is <strong>undersmoothed</strong> relative to the optimal series estimator.</p>
<blockquote>
<p>Many authors like to focus their asymptotic theory on the assumptions in the theorem, as the distribution of <span class="math inline">\(\theta\)</span> appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption <span class="math inline">\(\sqrt{n} K^{-\alpha} = o(1)\)</span> and the approximation of the theorem.</p>
<ul>
<li>First, it says that if we intentionally pick <span class="math inline">\(K\)</span> to be larger than optimal, we can increase the estimation variance relative to the bias so the variance will dominate the bias. But why would we want to intentionally use an estimator which is sub-optimal?</li>
<li>Second, the assumption <span class="math inline">\(\sqrt{n} K^{-\alpha} = o(1)\)</span> does not eliminate the asymptotic bias, it only makes it of lower order than the variance. So the approximation of the theorem is technically valid, but the missing asymptotic bias term is just slightly smaller in asymptotic order, and thus still relevant in finite samples.</li>
<li>Third, the condition <span class="math inline">\(\sqrt{n} K^{\alpha} = o(1)\)</span> is just an assumption, it has nothing to do with actual empirical practice. Thus the difference between the two theorems is in the assumptions, not in the actual reality or in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) through an assumption is a trick, not a substantive use of theory. My strong view is that the result (1) is more informative than (2). It shows that the asymptotic distribution is normal but has a non-trivial finite sample bias.</li>
</ul>
</blockquote>
</div>
<div id="kernel-vs-series" class="section level3">
<h3><span class="header-section-number">7.4.5</span> Kernel vs Series</h3>
<p>Hansen (2019): in this and the previous chapter we have presented two distinct methods of nonparametric regression based on kernel methods and series methods. Which should be used in practice? Both methods have advantages and disadvantages and there is no clear overall winner.</p>
<p>First, while the asymptotic theory of the two estimators appear quite different, they are actually rather closely related. When the regression function <span class="math inline">\(g(x)\)</span> is twice differentiable <span class="math inline">\((s = 2)\)</span> then the rate of convergence of both the MSE of the kernel regression estimator with optimal bandwidth <span class="math inline">\(h\)</span> and the series estimator with optimal <span class="math inline">\(K\)</span> is <span class="math inline">\(n^{-\frac{2}{k+4}}\)</span> (where <span class="math inline">\(k = \dim(x)\)</span>). There is no difference. If the regression function is smoother than twice differentiable (<span class="math inline">\(s &gt; 2\)</span>) then the rate of the convergence of the series estimator improves. This may appear to be an advantage for series methods, but kernel regression can also take advantage of the higher smoothness by using so-called higher-order kernels or local polynomial regression, so perhaps this advantage is not too large.</p>
<p>Both estimators are asymptotically normal and have straightforward asymptotic standard error formulae. The series estimators are a bit more convenient for this purpose, as classic parametric standard error formula work without amendment.</p>
<p>An advantage of kernel methods is that their distributional theory is easier to derive. The theory is all based on local averages which is relatively straightforward. In contrast, series theory is more challenging, dealing with increasing parameter spaces. An important difference in the theory is that for kernel estimators we have explicit representations for the bias while we only have rates for series methods. This means that plug-in methods can be used for bandwidth selection in kernel regression. However, typically we rely on cross-validation, which is equally applicable in both kernel and series regression.</p>
<p>Kernel methods are also relatively easy to implement when the dimension of <span class="math inline">\(x\)</span>, <span class="math inline">\(k\)</span>, is large. There is not a major change in the methodology as <span class="math inline">\(k\)</span> increases. In contrast, series methods become quite cumbersome as <span class="math inline">\(k\)</span> increases as the number of cross-terms increases exponentially. E.g (<span class="math inline">\(K=2\)</span>) with <span class="math inline">\(k=1\)</span> you have only <span class="math inline">\(\{x_1, x_1^2\}\)</span>; with <span class="math inline">\(k=2\)</span> you have to add <span class="math inline">\(\{x_2, x_2^2, x_1 x_2 \}\)</span>; with <span class="math inline">\(k=3\)</span> you have to add <span class="math inline">\(\{x_3, x_3^2, x_1 x_3, x_2 x_3\}\)</span>, etc..</p>
<p>A major advantage of series methods is that it has inherently a high degree of flexibility, and the user is able to implement shape restrictions quite easily. For example, in series estimation it is relatively simple to implement a partial linear CEF, an additively separable CEF, monotonicity, concavity or convexity. These restrictions are harder to implement in kernel regression.</p>
</div>
</div>
<div id="references-7" class="section level2">
<h2><span class="header-section-number">7.5</span> References</h2>
<ul>
<li>Newey, W. K. (1997). <em>Convergence rates and asymptotic normality for series estimators</em>. Journal of Econometrics, 79(1), 147–168.</li>
<li>Hansen (2019). “<em>Econometrics</em>”. Chapters 19, 20 and 21.</li>
<li>Kozbur (2019). PhD Econometrics - Lecture Notes.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["notes.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
