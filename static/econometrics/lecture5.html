<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 8 Variable Selection | Econometrics Notes</title>
  <meta name="description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 8 Variable Selection | Econometrics Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 8 Variable Selection | Econometrics Notes" />
  
  <meta name="twitter:description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  

<meta name="author" content="Matteo Courthoud" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture4.html">
<link rel="next" href="matlabcode.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../" target="_self">MATTEO COURTHOUD</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="appendix1.html"><a href="appendix1.html"><i class="fa fa-check"></i><b>1</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="appendix1.html"><a href="appendix1.html#basics"><i class="fa fa-check"></i><b>1.1</b> Basics</a></li>
<li class="chapter" data-level="1.2" data-path="appendix1.html"><a href="appendix1.html#spectral-decomposition"><i class="fa fa-check"></i><b>1.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="1.3" data-path="appendix1.html"><a href="appendix1.html#quadratic-forms-and-definite-matrices"><i class="fa fa-check"></i><b>1.3</b> Quadratic Forms and Definite Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="appendix1.html"><a href="appendix1.html#matrix-calculus"><i class="fa fa-check"></i><b>1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.5" data-path="appendix1.html"><a href="appendix1.html#references-1"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="appendix2.html"><a href="appendix2.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix2.html"><a href="appendix2.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a></li>
<li class="chapter" data-level="2.2" data-path="appendix2.html"><a href="appendix2.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="appendix2.html"><a href="appendix2.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a></li>
<li class="chapter" data-level="2.4" data-path="appendix2.html"><a href="appendix2.html#inequalities"><i class="fa fa-check"></i><b>2.4</b> Inequalities</a></li>
<li class="chapter" data-level="2.5" data-path="appendix2.html"><a href="appendix2.html#theorems"><i class="fa fa-check"></i><b>2.5</b> Theorems</a></li>
<li class="chapter" data-level="2.6" data-path="appendix2.html"><a href="appendix2.html#statistical-models"><i class="fa fa-check"></i><b>2.6</b> Statistical Models</a></li>
<li class="chapter" data-level="2.7" data-path="appendix2.html"><a href="appendix2.html#references-2"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix3.html"><a href="appendix3.html"><i class="fa fa-check"></i><b>3</b> Asymptotic Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix3.html"><a href="appendix3.html#convergence"><i class="fa fa-check"></i><b>3.1</b> Convergence</a></li>
<li class="chapter" data-level="3.2" data-path="appendix3.html"><a href="appendix3.html#theorems-1"><i class="fa fa-check"></i><b>3.2</b> Theorems</a></li>
<li class="chapter" data-level="3.3" data-path="appendix3.html"><a href="appendix3.html#ergodic-theory"><i class="fa fa-check"></i><b>3.3</b> Ergodic Theory</a></li>
<li class="chapter" data-level="3.4" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-estimators"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Properties of Estimators</a></li>
<li class="chapter" data-level="3.5" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-test-statistics"><i class="fa fa-check"></i><b>3.5</b> Asymptotic Properties of Test Statistics</a></li>
<li class="chapter" data-level="3.6" data-path="appendix3.html"><a href="appendix3.html#references-3"><i class="fa fa-check"></i><b>3.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>4</b> OLS Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="lecture1.html"><a href="lecture1.html#the-gauss-markov-model"><i class="fa fa-check"></i><b>4.1</b> The Gauss Markov Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lecture1.html"><a href="lecture1.html#matlab"><i class="fa fa-check"></i><b>4.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture1.html"><a href="lecture1.html#the-ols-estimator"><i class="fa fa-check"></i><b>4.2</b> The OLS estimator</a><ul>
<li class="chapter" data-level="4.2.1" data-path="lecture1.html"><a href="lecture1.html#matlab-1"><i class="fa fa-check"></i><b>4.2.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture1.html"><a href="lecture1.html#ols-residual-properties"><i class="fa fa-check"></i><b>4.3</b> OLS residual properties</a><ul>
<li class="chapter" data-level="4.3.1" data-path="lecture1.html"><a href="lecture1.html#matlab-2"><i class="fa fa-check"></i><b>4.3.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture1.html"><a href="lecture1.html#finite-sample-properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>4.4</b> Finite Sample Properties of the OLS estimator</a><ul>
<li class="chapter" data-level="4.4.1" data-path="lecture1.html"><a href="lecture1.html#matlab-3"><i class="fa fa-check"></i><b>4.4.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lecture1.html"><a href="lecture1.html#references-4"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>5</b> OLS Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="lecture2.html"><a href="lecture2.html#asymptotic-theory-of-the-ols-estimator"><i class="fa fa-check"></i><b>5.1</b> Asymptotic Theory of the OLS Estimator</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lecture2.html"><a href="lecture2.html#gaussian-error-term"><i class="fa fa-check"></i><b>5.1.1</b> Gaussian Error Term</a></li>
<li class="chapter" data-level="5.1.2" data-path="lecture2.html"><a href="lecture2.html#homoskedastic-error-term"><i class="fa fa-check"></i><b>5.1.2</b> Homoskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.3" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-error-term"><i class="fa fa-check"></i><b>5.1.3</b> Heteroskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.4" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-and-autocorrelated-error-term"><i class="fa fa-check"></i><b>5.1.4</b> Heteroskedastic and Autocorrelated Error Term</a></li>
<li class="chapter" data-level="5.1.5" data-path="lecture2.html"><a href="lecture2.html#fixed-b-asymptotics"><i class="fa fa-check"></i><b>5.1.5</b> Fixed b asymptotics</a></li>
<li class="chapter" data-level="5.1.6" data-path="lecture2.html"><a href="lecture2.html#fixed-g-asymptotics"><i class="fa fa-check"></i><b>5.1.6</b> Fixed G asymptotics</a></li>
<li class="chapter" data-level="5.1.7" data-path="lecture2.html"><a href="lecture2.html#matlab-4"><i class="fa fa-check"></i><b>5.1.7</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lecture2.html"><a href="lecture2.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lecture2.html"><a href="lecture2.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2.2" data-path="lecture2.html"><a href="lecture2.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="lecture2.html"><a href="lecture2.html#matlab-5"><i class="fa fa-check"></i><b>5.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="lecture2.html"><a href="lecture2.html#references-5"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>6</b> Endogeneity</a><ul>
<li class="chapter" data-level="6.1" data-path="lecture3.html"><a href="lecture3.html#instrumental-variables"><i class="fa fa-check"></i><b>6.1</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lecture3.html"><a href="lecture3.html#matlab-6"><i class="fa fa-check"></i><b>6.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lecture3.html"><a href="lecture3.html#gmm"><i class="fa fa-check"></i><b>6.2</b> GMM</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lecture3.html"><a href="lecture3.html#step-gmm"><i class="fa fa-check"></i><b>6.2.1</b> 1-step GMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="lecture3.html"><a href="lecture3.html#step-gmm-1"><i class="fa fa-check"></i><b>6.2.2</b> 2-step GMM</a></li>
<li class="chapter" data-level="6.2.3" data-path="lecture3.html"><a href="lecture3.html#matlab-7"><i class="fa fa-check"></i><b>6.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture3.html"><a href="lecture3.html#testing-overidentifying-restrictions"><i class="fa fa-check"></i><b>6.3</b> Testing Overidentifying Restrictions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lecture3.html"><a href="lecture3.html#naive-test"><i class="fa fa-check"></i><b>6.3.1</b> Naive Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="lecture3.html"><a href="lecture3.html#hansens-test"><i class="fa fa-check"></i><b>6.3.2</b> Hansen’s Test</a></li>
<li class="chapter" data-level="6.3.3" data-path="lecture3.html"><a href="lecture3.html#special-case-conditional-homoskedasticity"><i class="fa fa-check"></i><b>6.3.3</b> Special Case: Conditional Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lecture3.html"><a href="lecture3.html#small-sample-properties-of-2sls"><i class="fa fa-check"></i><b>6.4</b> Small-Sample Properties of 2SLS</a></li>
<li class="chapter" data-level="6.5" data-path="lecture3.html"><a href="lecture3.html#many-instrument-robust-estimation"><i class="fa fa-check"></i><b>6.5</b> Many Instrument Robust Estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="lecture3.html"><a href="lecture3.html#liml"><i class="fa fa-check"></i><b>6.5.1</b> LIML</a></li>
<li class="chapter" data-level="6.5.2" data-path="lecture3.html"><a href="lecture3.html#jive"><i class="fa fa-check"></i><b>6.5.2</b> JIVE</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lecture3.html"><a href="lecture3.html#hausman-test"><i class="fa fa-check"></i><b>6.6</b> Hausman Test</a></li>
<li class="chapter" data-level="6.7" data-path="lecture3.html"><a href="lecture3.html#references-6"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>7</b> Non-Parametric Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="lecture4.html"><a href="lecture4.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="lecture4.html"><a href="lecture4.html#discrete-x---cell-estimator"><i class="fa fa-check"></i><b>7.2</b> Discrete x - Cell Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="lecture4.html"><a href="lecture4.html#local-non-parametric-estimation---kernels"><i class="fa fa-check"></i><b>7.3</b> Local Non-Parametric Estimation - Kernels</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lecture4.html"><a href="lecture4.html#estimator-examples"><i class="fa fa-check"></i><b>7.3.1</b> Estimator examples:</a></li>
<li class="chapter" data-level="7.3.2" data-path="lecture4.html"><a href="lecture4.html#kernel-examples"><i class="fa fa-check"></i><b>7.3.2</b> Kernel examples:</a></li>
<li class="chapter" data-level="7.3.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-bandwidth"><i class="fa fa-check"></i><b>7.3.3</b> Choice of the optimal bandwidth</a></li>
<li class="chapter" data-level="7.3.4" data-path="lecture4.html"><a href="lecture4.html#inference-1"><i class="fa fa-check"></i><b>7.3.4</b> Inference</a></li>
<li class="chapter" data-level="7.3.5" data-path="lecture4.html"><a href="lecture4.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.3.5</b> Bias-variance trade-off</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lecture4.html"><a href="lecture4.html#global-non-parametric-estimation---series"><i class="fa fa-check"></i><b>7.4</b> Global Non-Parametric Estimation - Series</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lecture4.html"><a href="lecture4.html#examples"><i class="fa fa-check"></i><b>7.4.1</b> Examples</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture4.html"><a href="lecture4.html#estimation"><i class="fa fa-check"></i><b>7.4.2</b> Estimation</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-k"><i class="fa fa-check"></i><b>7.4.3</b> Choice of the optimal <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture4.html"><a href="lecture4.html#inference-2"><i class="fa fa-check"></i><b>7.4.4</b> Inference</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture4.html"><a href="lecture4.html#kernel-vs-series"><i class="fa fa-check"></i><b>7.4.5</b> Kernel vs Series</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lecture4.html"><a href="lecture4.html#references-7"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>8</b> Variable Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="lecture5.html"><a href="lecture5.html#lasso"><i class="fa fa-check"></i><b>8.1</b> Lasso</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lecture5.html"><a href="lecture5.html#choosing-the-optimal-lambda"><i class="fa fa-check"></i><b>8.1.1</b> Choosing the optimal lambda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lecture5.html"><a href="lecture5.html#pre-testing"><i class="fa fa-check"></i><b>8.2</b> Pre-Testing</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lecture5.html"><a href="lecture5.html#omitted-variable-bias"><i class="fa fa-check"></i><b>8.2.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.2.2" data-path="lecture5.html"><a href="lecture5.html#pre-test-bias"><i class="fa fa-check"></i><b>8.2.2</b> Pre-test bias</a></li>
<li class="chapter" data-level="8.2.3" data-path="lecture5.html"><a href="lecture5.html#partioned-regression"><i class="fa fa-check"></i><b>8.2.3</b> Partioned Regression</a></li>
<li class="chapter" data-level="8.2.4" data-path="lecture5.html"><a href="lecture5.html#matlab-8"><i class="fa fa-check"></i><b>8.2.4</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lecture5.html"><a href="lecture5.html#post-double-selection"><i class="fa fa-check"></i><b>8.3</b> Post Double Selection</a></li>
<li class="chapter" data-level="8.4" data-path="lecture5.html"><a href="lecture5.html#references-8"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matlabcode.html"><a href="matlabcode.html"><i class="fa fa-check"></i><b>9</b> Matlab Code</a><ul>
<li class="chapter" data-level="9.1" data-path="matlabcode.html"><a href="matlabcode.html#lecture-1"><i class="fa fa-check"></i><b>9.1</b> Lecture 1</a></li>
<li class="chapter" data-level="9.2" data-path="matlabcode.html"><a href="matlabcode.html#lecture-2"><i class="fa fa-check"></i><b>9.2</b> Lecture 2</a></li>
<li class="chapter" data-level="9.3" data-path="matlabcode.html"><a href="matlabcode.html#lecture-3"><i class="fa fa-check"></i><b>9.3</b> Lecture 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture5" class="section level1">
<h1><span class="header-section-number">Lecture 8</span> Variable Selection</h1>
<div id="lasso" class="section level2">
<h2><span class="header-section-number">8.1</span> Lasso</h2>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) is a popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called <em>penalized estimators</em>. It allows the number of regressors, <span class="math inline">\(p\)</span>, to be larger than the sample size, <span class="math inline">\(n\)</span>.</p>
<p>Consider data <span class="math inline">\(D = \{ x_i, y_i \}_{i=1}^n\)</span> with <span class="math inline">\(\dim (x_i) = p\)</span>. Assume that <span class="math inline">\(p\)</span> is large relative to <span class="math inline">\(n\)</span>. Two possible reasons:</p>
<ul>
<li>we have an intrinsic problem of high dimensionality</li>
<li><span class="math inline">\(p\)</span> indicates the number of expansion terms of small number of underlying important variables (e.g. series estimation)</li>
</ul>
<p><strong>Assumption</strong>:
<span class="math inline">\(y_i = x_i&#39; \beta_0 + r_i + \varepsilon_i\)</span> where <span class="math inline">\(\beta_0\)</span> depends on <span class="math inline">\(p\)</span>, <span class="math inline">\(r_i\)</span> is a remainder term.</p>
<p>Note that in classic non-parametrics, we have <span class="math inline">\(x_i&#39;\beta_0\)</span> as <span class="math inline">\(p_1(x_i) \beta_{1,K} + \dots + p_K(x_i) \beta_{K,K}\)</span>. For simplicity, we assume <span class="math inline">\(r_i = 0\)</span>, as if we had extreme undersmoothing. Hence the model becomes:
<span class="math display">\[
    y_i = x_i&#39; \beta_0 + \varepsilon_i, \qquad p \geq n
\]</span>
We cannot run OLS because <span class="math inline">\(p \geq n\)</span>, thus the rank condition is violated.</p>
<p>We define the <strong>Lasso estimator</strong> as
<span class="math display">\[
    \hat{\beta}_L = \arg \min \quad \underbrace{\mathbb E_n \Big[ (y_i - x_i&#39; \beta)^2 \Big]}_{\text{SSR term}} + \underbrace{\frac{\lambda}{n} \sum_{j=1}^{P} | \beta_j |}_{\text{Penalty term}}
\]</span>
where <span class="math inline">\(\lambda\)</span> is called <strong>penalty parameter</strong>.</p>
<p>The <strong>penalty term</strong> discourages large values of <span class="math inline">\(| \beta_j |\)</span>.
The choice of <span class="math inline">\(\lambda\)</span> is analogous to the choice of <span class="math inline">\(K\)</span> in series estimation and <span class="math inline">\(h\)</span> in kernel estimation.</p>
<blockquote>
<p>The shrinkage to zero of the coefficients directly follows from the <span class="math inline">\(|| \cdot ||_1\)</span> norm. On the contrary, another famous penalized estimator, <em>ridge regression</em>, uses the <span class="math inline">\(|| \cdot ||_2\)</span> norm and does not have this property.</p>
</blockquote>
<div class="figure">
<img src="figures/Fig_551.png" alt="" />
<p class="caption">Constraints</p>
</div>
<blockquote>
<p>Minimizing SSR + penalty is equivalent to minimize SSR <span class="math inline">\(s.t.\)</span> pen <span class="math inline">\(\leq c\)</span> (clear from the picture).</p>
</blockquote>
<p>Let <span class="math inline">\(S_0 = \{ j: \beta_{0,j} \ne 0 \}\)</span>, we define <span class="math inline">\(s_0 = \# S_0\)</span> as the <strong>sparsity</strong> of <span class="math inline">\(\beta_0\)</span>. If <span class="math inline">\(s_0/n \to 0\)</span>, we are dealing with a <strong>sparse regression</strong> (analogous of smooth regression).</p>
<blockquote>
<p>Remark on sparsity:</p>
<ul>
<li>In words, sparsity means that even if we have a lot of variables, only a small number of them (relative to <span class="math inline">\(n\)</span>) have an effect on the dependent variable.</li>
<li><em>Approximate sparsity imposes a restriction that only <span class="math inline">\(s_0\)</span> variables among all of <span class="math inline">\(x_{ij}\)</span>, where <span class="math inline">\(s_0\)</span> is much smaller than <span class="math inline">\(n\)</span>, have associated coefficients <span class="math inline">\(\beta_{0j}\)</span> that are different from zero, while permitting a nonzero approximation error. Thus, estimators for this kind of model attempt to learn the identities of the variables with large nonzero coefficients, while simultaneously estimating these coefficients.</em> (Belloni et al., 2004)</li>
<li>Sparsity is an assumption. <span class="math inline">\(\beta_0\)</span> is said to be <span class="math inline">\(s_0\)</span>-sparse with <span class="math inline">\(s_0 &lt; n\)</span> if
<span class="math display">\[
| \{ j: \beta_{0j} \neq 0 \} | \leq s_0
\]</span></li>
</ul>
</blockquote>
<p><strong>Theorem</strong>:
Suppose that for data <span class="math inline">\(D_n = (y_i, x_i)_{i=1}^N\)</span> with <span class="math inline">\(y_i = x_i&#39; \beta + \varepsilon_i\)</span>. Let <span class="math inline">\(\hat{\beta}_L\)</span> be the Lasso estimator. Let <span class="math inline">\(\mathcal{S} = 2 \max_j | \mathbb E[ x_{ij} \varepsilon_i] |\)</span>. Suppose <span class="math inline">\(|support(\beta_0) \leq s_0\)</span> (sparsity assumption). Let <span class="math inline">\(c_0 = (\mathcal{S} + \lambda/n )/(-\mathcal{S} + \lambda/n )\)</span>. Let
<span class="math display">\[
    \kappa_{c_0, s_0} = \min_{  d \in \mathbb R^p, A \subseteq \{ 1, ... , p \} : |A| \leq s_0 ,  || d_{A^c}|| \leq c_0 || d_A ||_1  }  \sqrt{  \frac{ s_0 d&#39; \mathbb E_n [x_i x_i&#39;] d }{|| d_A ||_1^2}  }
\]</span>
Then</p>
<p><span class="math display">\[
    \mathbb I_{ \left\{ \frac{\lambda}{n} &gt; \mathcal{S}  \right\}} \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \leq 2 \frac{\lambda}{n} \frac{\sqrt{s_0}}{\kappa_{c_0, s_0}} \\
\]</span></p>
<p>Intuition: for a sufficiently high lambda the root mean squared error of Lasso is approximately zero.</p>
<p><span class="math display">\[
    \text{ RMSE }:  \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \simeq 0  \quad \Leftrightarrow \quad \frac{\lambda}{n} &gt; \mathcal{S}
\]</span></p>
<blockquote>
<p>Remarks on the theorem:</p>
<ul>
<li>The minimization region is the set of “essentially sparse” vectors <span class="math inline">\(d \in \mathbb R^p\)</span>, where “essentially sparse” is defined by <span class="math inline">\(\mathcal{C}, \mathcal{S}\)</span>. In particular the condition <span class="math inline">\(k_{\mathcal{C}, \mathcal{S}}&gt;0\)</span> means that no essentially sparse vector <span class="math inline">\(d\)</span> has <span class="math inline">\(\mathbb E[x_i x_i&#39;]d = 0\)</span>, i.e. regressors were not added multiple times.</li>
<li>Need to dominate the score with the penalty term <span class="math inline">\(\lambda\)</span>.</li>
<li>Need no collinearity on a small (<span class="math inline">\(\leq s_0\)</span>) subset of regressors (<span class="math inline">\(\to k_{c_0, s_0}&gt;0\)</span>).</li>
</ul>
</blockquote>
<p><strong>When Lasso?</strong> For prediction problems in high dimensional environments. <strong>NB!</strong> Lasso is not good for inference, only for prediction.</p>
<p>In particular, in econometrics it’s used for selecting either</p>
<ul>
<li>instruments (predicting <span class="math inline">\(\hat{x}\)</span> in the first stage)</li>
<li>control variables (next section: double prediction problem, in the first stage and in the reduced form)</li>
</ul>
<div id="choosing-the-optimal-lambda" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Choosing the optimal lambda</h3>
<p>The choice of <span class="math inline">\(\lambda\)</span> determines the bias-variance tradeoff:</p>
<ul>
<li>if <span class="math inline">\(\lambda\)</span> is too big: <span class="math inline">\(\lambda \approx \infty \mathbb \Rightarrow \hat{\beta} \approx 0\)</span>;</li>
<li>if <span class="math inline">\(\lambda\)</span> is too small: <span class="math inline">\(\lambda \approx 0 \mathbb \Rightarrow\)</span> overfitting.</li>
</ul>
<p>Possible solutions: Bonferroni correction, bootstrapping or <span class="math inline">\(\frac{\lambda}{n} \asymp \sqrt{\frac{\log(p)}{n}}\)</span> (asymptotically equal to), <span class="math inline">\(\mathcal{S}\)</span> behaves like the maximum of gaussians.</p>
<p><strong>The Lasso Path</strong>: how the estimated <span class="math inline">\(\hat{\beta}\)</span> depends on the penalty parameter <span class="math inline">\(\lambda\)</span>?</p>
<div class="figure">
<img src="figures/Fig_642.png" alt="" />
<p class="caption">Lasso Path</p>
</div>
<p><strong>Post Lasso</strong>: fit OLS without the penalty with all the nonzero coeficients selected by Lasso in the first step.</p>
<blockquote>
<p>Remarks on Lasso:</p>
<ul>
<li>Do not do inference with post-Lasso because standard errors are not uniformely valid.</li>
<li>As <span class="math inline">\(n \to \infty\)</span> the CV and the <strong>score domination</strong> bounds converge to a unique bound.</li>
<li>What is the problem of cross-validation? In high dimensional settings you can overfit in so many ways that CV doesn’t work and still overfits.</li>
<li>Using <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\frac{\lambda}{n} &gt; \mathcal{S}\)</span> small coefficients get shrunk to zero with high probability. In this case with small we mean <span class="math inline">\(\propto \frac{1}{\sqrt{n}}\)</span> or <span class="math inline">\(2 \max_j | \mathbb E_n[\varepsilon_i x_{ij}] |\)</span>.</li>
<li>If <span class="math inline">\(| \beta_{0j}| \leq \frac{c}{\sqrt{n}}\)</span> for a sufficiently small constant <span class="math inline">\(c\)</span>, then <span class="math inline">\(\hat{\beta}_{LASSO} \overset{p}{\to} 0\)</span>.</li>
<li>In standard t-tests <span class="math inline">\(c = 1.96\)</span>.</li>
<li><span class="math inline">\(\sqrt{n}\)</span> factor is important since it is the demarcation line for reliable statistical detection.</li>
</ul>
</blockquote>
<p>What is the criterium that should guide the selection of <span class="math inline">\(\lambda\)</span>?
<span class="math display">\[
  \frac{\lambda}{n} \geq 2 \mathbb E_n[x_{ij} \varepsilon_i] \qquad \forall j \quad \text{ if } Var(x_{ij} \varepsilon_i) = 1
\]</span></p>
<p>How to choose the optimal <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>Decide the coverage of the confidence intervals (<span class="math inline">\(1-\alpha\)</span>):
<span class="math display">\[
\Pr \left( \sqrt{n} \Big| \mathbb E_n [x_{ij} \varepsilon_i] \Big| &gt; t \right) = 1- \alpha
\]</span></li>
<li>Solve for <span class="math inline">\(t\)</span></li>
<li>Get <span class="math inline">\(\lambda\)</span> such that all scores are dominated by <span class="math inline">\(\frac{\lambda}{n}\)</span> with <span class="math inline">\(\alpha\%\)</span> probability.</li>
</ul>
<blockquote>
<p>It turns out that the optimal <span class="math inline">\(t \propto \sqrt{\log(p)}\)</span></p>
</blockquote>
</div>
</div>
<div id="pre-testing" class="section level2">
<h2><span class="header-section-number">8.2</span> Pre-Testing</h2>
<p>Main reference for this section: Belloni, Chernozhukov and Hansen <em>Inference for Treatment Effects with High Dimensional Controls</em> in the <em>Review of Economic Studies</em> (2014).</p>
<div id="omitted-variable-bias" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Omitted Variable Bias</h3>
<p>Consider two separate statistical models. Assume the following <strong>long regression</strong> of interest:</p>
<p><span class="math display">\[
  y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
\]</span></p>
<p>Define the corresponding <strong>short regression</strong> as</p>
<p><span class="math display">\[
  y_i = x_i&#39; \alpha_0 + v_i \quad \text{ with } v_i = z_i&#39; \beta_0 + \varepsilon_i
\]</span></p>
<p><strong>Theorem</strong>:
Suppose that the DGP for the long regression corresponds to <span class="math inline">\(\alpha_0\)</span>, <span class="math inline">\(\beta_0\)</span>. Suppose further that <span class="math inline">\(\mathbb E[x_i] = 0\)</span>, <span class="math inline">\(\mathbb E[z_i] = 0\)</span>, <span class="math inline">\(\mathbb E[\varepsilon_i |x_i,z_i] = 0\)</span>. Then, unless <span class="math inline">\(\beta_0 = 0\)</span> or <span class="math inline">\(z_i\)</span> is orthogonal to <span class="math inline">\(x_i\)</span>, the (sole) stochastic regressor <span class="math inline">\(x_i\)</span> is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for <span class="math inline">\(\alpha_0\)</span> due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of <span class="math inline">\(\hat{\alpha}_{SHORT}\)</span> from the short regression is
<span class="math display">\[
    \hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
\]</span></p>
</div>
<div id="pre-test-bias" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Pre-test bias</h3>
<p>Consider data <span class="math inline">\(D= (y_i, x_i, z_i)_{i=1}^n\)</span>, where the true model is:
<span class="math display">\[
\begin{aligned}
&amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \\
&amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(x_i\)</span> is the variable of interest (we want to make inference on <span class="math inline">\(\alpha_0\)</span>) and <span class="math inline">\(z_i\)</span> is a high dimensional set of control variables.</p>
<p>From now on, we will work under the following assumptions:</p>
<ul>
<li><span class="math inline">\(\dim(x_i)=1\)</span> for all <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> uniformely bounded in <span class="math inline">\(n\)</span></li>
<li>Strict exogeneity: <span class="math inline">\(\mathbb E[\varepsilon_i | x_i, z_i] = 0\)</span> and <span class="math inline">\(\mathbb E[u_i | z_i] = 0\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\gamma_0\)</span> have dimension (and hence value) that depend on <span class="math inline">\(n\)</span></li>
</ul>
<p>Pre-Testing procedure:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span></li>
<li>For each <span class="math inline">\(j = 1, ..., p = \dim(z_i)\)</span> calculate a test statistic <span class="math inline">\(t_j\)</span></li>
<li>Let <span class="math inline">\(\hat{T} = \{ j: |t_j| &gt; C &gt; 0 \}\)</span> for some constant <span class="math inline">\(C\)</span> (set of statistically significant coefficients).</li>
<li>Re-run the new “model” using <span class="math inline">\((x_i, z_{\hat{T},i})\)</span> (i.e. using the selected covariates with statistically significant coefficients).</li>
<li>Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.</li>
</ol>
<div class="figure">
<img src="figures/Fig_621.png" alt="" />
<p class="caption">Omitted Variable Bias and Pre-Test Distortion</p>
</div>
<p>As we can see from the figure above (code below), running the short regression instead of the long one introduces Omitted Variable Bias (second column). Instead, the Pre-Testing estimator is consistent but not normally distributed (third column).</p>
<p>Pre-testing is problematic because the post-selection estimator is not asymptotically normal. Moreover, for particular data generating processes, it even fails to be consistent at the rate of <span class="math inline">\(\sqrt{n}\)</span> (Belloni et al., 2014).</p>
<blockquote>
<p>Intuition: when performing pre-testing, we might have an Omitted Variable Bias problem when <span class="math inline">\(\beta_0&gt;0\)</span> but we fail to reject the null hypothesis <span class="math inline">\(H_0 : \beta_0 = 0\)</span> because of lack of statistical power, i.e. <span class="math inline">\(|\beta_0|\)</span> is small with respect to the sample size. In particular, we fail to reject the null hypothesis for $ _0 = O (  ) $. Note however, that the problem vanishes asymptotically, as the resulting estimator is consistent. In fact, if <span class="math inline">\(\beta_0(n) = O\left( \frac{1}{n}\right)\)</span>, then <span class="math inline">\(\alpha_0 - \hat \alpha_{PRETEST} \overset{p}{\to} \beta_0 \gamma_0 = 0\)</span>. We now clarify what it means to have a coefficient depending on the sample size, <span class="math inline">\(\beta_0(n)\)</span>.</p>
</blockquote>
<p>Concept of <strong>uniformity</strong>: the DGP varies with <span class="math inline">\(n\)</span>. Instead of having a fixed “true” parameter <span class="math inline">\(\beta_0\)</span>, you have a sequence <span class="math inline">\(\beta_0(n)\)</span>.
Having a cofficient that depends on the sample size <span class="math inline">\(n\)</span> is useful to preserve the concept of “small with respect to the sample size” in asymptotic theory.</p>
<p>In the context of Pre-Testing, all problems vanish asymptotically since we are able to always reject the null hypothesis <span class="math inline">\(H_0 : \beta_0 = 0\)</span> when <span class="math inline">\(\beta_0 \neq 0\)</span>. In the figure above, I plot simulation results for <span class="math inline">\(\hat \alpha_{PRETESTING}\)</span> for a fixed coefficient <span class="math inline">\(\beta_0\)</span> (first row) and variable coefficient <span class="math inline">\(\beta_0(n)\)</span> that depends on the sample size (second row), for different sample sizes (columns). We see that if <span class="math inline">\(\beta_0\)</span> is independent from the sample size (first row), the distribution of <span class="math inline">\(\hat \alpha_{PRETEST}\)</span> is not normal in small samples and it displays the bimodality that characterizes pre-testing. However, it becomes normal in large samples. On the other hand, when <span class="math inline">\(\beta_0(n)\)</span> depends on the sample size (second row), the distribution of <span class="math inline">\(\hat \alpha_{PRETEST}\)</span> stays bimodal even when the sample size increases.</p>
<blockquote>
<p>Note that the estimator is always consistent!</p>
</blockquote>
<div class="figure">
<img src="figures/Fig_624.png" alt="" />
<p class="caption">Distrubution of <span class="math inline">\(\hat \alpha_{PRETEST}\)</span></p>
</div>
<p>One could show that problems arise exactly when <span class="math inline">\(\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)\)</span>. If we were to draw a map of where the gaussianity assumption of <span class="math inline">\(\beta_0(n)\)</span> holds well, it would look like the following figure.</p>
<div class="figure">
<img src="figures/Fig_623.png" alt="" />
<p class="caption">Uniformty</p>
</div>
<p>The intuition for the three different regions (from bottom to top) is the following.</p>
<ol style="list-style-type: decimal">
<li>When <span class="math inline">\(\beta_0 = o \left( \frac{1}{\sqrt{n}} \right)\)</span>, <span class="math inline">\(z_i\)</span> is excluded with probability <span class="math inline">\(p \to 1\)</span>. But, given that <span class="math inline">\(\beta_0\)</span> is small enough, failing to control for <span class="math inline">\(z_i\)</span> does not introduce large omitted variables bias (Belloni et al., 2014).</li>
<li>If however the coefficient on the control is “moderately close to zero”, <span class="math inline">\(\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)\)</span>, the t-test set-up above cannot distinguish this coefficient from <span class="math inline">\(0\)</span>, and the control <span class="math inline">\(z_i\)</span> is dropped with probability <span class="math inline">\(p \to 1\)</span>. However, in this case the omitted variable bias generated by excluding <span class="math inline">\(z_i\)</span> scaled by <span class="math inline">\(\sqrt{n}\)</span> does not converge to zero. That is, the standard post-selection estimator is not asymptotically normal and even fails to be consistent at the rate of <span class="math inline">\(\sqrt{n}\)</span> (Belloni et al., 2014).</li>
<li>Lastly, when <span class="math inline">\(\beta_0\)</span> is large enough, the null pre-testing hypothesis <span class="math inline">\(H_0 : \beta_0 = 0\)</span> will be rejected sufficiently often so that the bias is negligible.</li>
</ol>
<p>The post-double-selection estimator, <span class="math inline">\(\hat{\alpha}_{PDS}\)</span> solves this problem by doing variable selection via standard t-tests or Lasso-type selectors with the two “true model” equations (<strong>first stage</strong> and <strong>reduced form</strong>) that contain the information from the model and then estimating <span class="math inline">\(\alpha_0\)</span> by regressing <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and the union of the selected controls. By doing so, <span class="math inline">\(z_i\)</span> is omitted only if its coefficient in both equations is small which greatly limits the potential for omitted variables bias (Belloni et al., 2014).</p>
</div>
<div id="partioned-regression" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Partioned Regression</h3>
<p><strong>Frisch-Waugh Theorem</strong>:
Consider the data <span class="math inline">\(D = \{ x_i, y_i, z_i \}_{i=1}^\infty\)</span> with DGP: <span class="math inline">\(Y = X \alpha + Z \beta + \varepsilon\)</span>. The following estimators of <span class="math inline">\(\alpha\)</span> are numerically equivalent (if <span class="math inline">\([X, Z]\)</span> has full rank):</p>
<ul>
<li><span class="math inline">\(\hat{\alpha}\)</span> from regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X, Z\)</span></li>
<li><span class="math inline">\(\tilde{\alpha}\)</span> from regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(\tilde{X}\)</span></li>
<li><span class="math inline">\(\bar{\alpha}\)</span> from regressing <span class="math inline">\(\tilde{Y}\)</span> on <span class="math inline">\(\tilde{X}\)</span></li>
</ul>
<p>where the operation of passing to <span class="math inline">\(Y, X\)</span> to <span class="math inline">\(\tilde{Y}, \tilde{X}\)</span> is called <em>projection out <span class="math inline">\(Z\)</span></em>, e.g.<span class="math inline">\(\tilde{X}\)</span> are the residuals from regressing <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>.</p>
<p><strong>Proof</strong>:
We want to show that <span class="math inline">\(\hat{\alpha} = \tilde{\alpha}\)</span>.</p>
<p>Claim: <span class="math inline">\(\hat{\alpha } = \tilde{\alpha} \Leftrightarrow \tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0\)</span>.</p>
<p>Proof of the claim: if <span class="math inline">\(\hat{\alpha} = \tilde{\alpha}\)</span>, we can write <span class="math inline">\(Y\)</span> as
<span class="math display">\[
    Y =  X \hat{\alpha} + Z \hat{\beta} + \hat{\varepsilon}  = \tilde{X} \hat{\alpha} + \underbrace{(X - \tilde{X}) \hat{\alpha } + Z \hat{\beta} + \hat{\varepsilon}}_\text{residual of $Y$ on $\tilde{X} $} = \tilde{X} \tilde{\alpha} + \nu_i
\]</span></p>
<p>Therefore, by the orthogonality property of the OLS residual, it must be that <span class="math inline">\(\tilde{X}&#39;\nu_i= 0\)</span>.
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p>Having established the claim, we want to show that the normal equation <span class="math inline">\(\tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0\)</span> is satisfied. We follow 3 steps:</p>
<ol style="list-style-type: decimal">
<li><p>First we have that <span class="math inline">\(\tilde{X}&#39; (X - \tilde{X})\hat{\alpha} = 0\)</span>. This follows from the fact that <span class="math inline">\(\tilde{X}&#39; = X&#39; M_Z\)</span> and hence:
<span class="math display">\[
  \begin{aligned}
  \tilde{X}&#39; (X - \tilde{X})  &amp;  = X&#39; M_Z (X - M_Z) = X&#39; M_Z X - X&#39; \overbrace{M_Z M_Z}^{M_Z} X \\ &amp; = X&#39;M_Z X - X&#39; M_Z X = 0
  \end{aligned}
\]</span></p></li>
<li><p><span class="math inline">\(\tilde{X}&#39; Z \hat{\beta} = 0\)</span> since <span class="math inline">\(\tilde{X}\)</span> is the residual from the regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>, by normal equation it holds that <span class="math inline">\(\tilde{X}&#39; Z = 0\)</span>.</p></li>
<li><p><span class="math inline">\(\tilde{X}&#39; \hat{\varepsilon} = 0\)</span>. This follows from (i) <span class="math inline">\(M_Z &#39; M_{X, Z} = M_{X,Z}\)</span> and (ii) <span class="math inline">\(X&#39; M_{X, Z} = 0\)</span>:
<span class="math display">\[
  \tilde{X}&#39; \hat{\varepsilon} = (M_Z X)&#39; (M_{X, Z} \varepsilon)  = X&#39;M_Z&#39; M_{X, Z} \varepsilon = \underbrace{X&#39; M_{X, Z}}_0 \varepsilon = 0.
\]</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p></li>
</ol>
<p>The coefficient <span class="math inline">\(\hat{\alpha}\)</span> is a <em>partial regression</em> coefficient identified from the variation in <span class="math inline">\(X\)</span> that is orthogonal to <span class="math inline">\(Z\)</span>. This is often known as <strong>residual variation</strong>.</p>
</div>
<div id="matlab-8" class="section level3">
<h3><span class="header-section-number">8.2.4</span> <code>Matlab</code></h3>
<p>First, we consider the following model: $ y_i = x_i’ _0 + z_i’ _0 + _i $ and
$ x_i = z_i’ _0 + u_i $ where <span class="math inline">\(\alpha_0 = 1\)</span>, <span class="math inline">\(\beta_0 = 0.1\)</span> and <span class="math inline">\(\gamma = -4\)</span>, and <span class="math inline">\(\varepsilon_i, u_i \sim N(0,1)\)</span>. We simulate three different estimators over 10000 samples: <span class="math inline">\(\hat \alpha_{LONG}\)</span> from the long regression, <span class="math inline">\(\hat \alpha_{SHORT}\)</span> from the short regression and <span class="math inline">\(\hat \alpha_{PRE-TESTING}\)</span> that is either <span class="math inline">\(\hat \alpha_{LONG}\)</span> or <span class="math inline">\(\hat \alpha_{SHORT}\)</span>, depending on whether <span class="math inline">\(\hat \beta\)</span> is significant at the 5% level or not in the long regression.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb10-1"><a href="lecture5.html#cb10-1"></a><span class="co">% Set seed</span></span>
<span id="cb10-2"><a href="lecture5.html#cb10-2"></a>rng(<span class="fl">123</span>)</span>
<span id="cb10-3"><a href="lecture5.html#cb10-3"></a></span>
<span id="cb10-4"><a href="lecture5.html#cb10-4"></a><span class="co">% Define Model Parameters</span></span>
<span id="cb10-5"><a href="lecture5.html#cb10-5"></a>n = <span class="fl">1000</span>;</span>
<span id="cb10-6"><a href="lecture5.html#cb10-6"></a>alpha = <span class="fl">1</span>;</span>
<span id="cb10-7"><a href="lecture5.html#cb10-7"></a>beta = <span class="fl">.1</span>;</span>
<span id="cb10-8"><a href="lecture5.html#cb10-8"></a>gamma = -<span class="fl">4</span>;</span>
<span id="cb10-9"><a href="lecture5.html#cb10-9"></a></span>
<span id="cb10-10"><a href="lecture5.html#cb10-10"></a><span class="co">% Set number of simulations</span></span>
<span id="cb10-11"><a href="lecture5.html#cb10-11"></a>n_simulations = <span class="fl">10000</span>;</span>
<span id="cb10-12"><a href="lecture5.html#cb10-12"></a></span>
<span id="cb10-13"><a href="lecture5.html#cb10-13"></a><span class="co">% Preallocate simulation results</span></span>
<span id="cb10-14"><a href="lecture5.html#cb10-14"></a>alpha_short = zeros(n_simulations,<span class="fl">1</span>);</span>
<span id="cb10-15"><a href="lecture5.html#cb10-15"></a>alpha_long = zeros(n_simulations,<span class="fl">1</span>);</span>
<span id="cb10-16"><a href="lecture5.html#cb10-16"></a>alpha_pretest = zeros(n_simulations,<span class="fl">1</span>);</span>
<span id="cb10-17"><a href="lecture5.html#cb10-17"></a></span>
<span id="cb10-18"><a href="lecture5.html#cb10-18"></a><span class="co">% Loop over simulations</span></span>
<span id="cb10-19"><a href="lecture5.html#cb10-19"></a>for sim = <span class="fl">1</span>:n_simulations</span>
<span id="cb10-20"><a href="lecture5.html#cb10-20"></a></span>
<span id="cb10-21"><a href="lecture5.html#cb10-21"></a>    <span class="co">% Generate Data</span></span>
<span id="cb10-22"><a href="lecture5.html#cb10-22"></a>    Z = randn(n,<span class="fl">1</span>);</span>
<span id="cb10-23"><a href="lecture5.html#cb10-23"></a>    X = gamma*Z + randn(n,<span class="fl">1</span>);</span>
<span id="cb10-24"><a href="lecture5.html#cb10-24"></a>    e = randn(n,<span class="fl">1</span>);</span>
<span id="cb10-25"><a href="lecture5.html#cb10-25"></a>    Y = alpha*X+ beta*Z + e;</span>
<span id="cb10-26"><a href="lecture5.html#cb10-26"></a>    </span>
<span id="cb10-27"><a href="lecture5.html#cb10-27"></a>    <span class="co">% Alpha estimate from long regression</span></span>
<span id="cb10-28"><a href="lecture5.html#cb10-28"></a>    alpha_short(sim) = inv(X&#39;*X)*X&#39;*Y;</span>
<span id="cb10-29"><a href="lecture5.html#cb10-29"></a>    </span>
<span id="cb10-30"><a href="lecture5.html#cb10-30"></a>    <span class="co">% Alpha estimate from long regression</span></span>
<span id="cb10-31"><a href="lecture5.html#cb10-31"></a>    estimates_temp = inv([X,Z]&#39;*[X,Z])*[X,Z]&#39;*Y;</span>
<span id="cb10-32"><a href="lecture5.html#cb10-32"></a>    alpha_long(sim) = estimates_temp(<span class="fl">1</span>);</span>
<span id="cb10-33"><a href="lecture5.html#cb10-33"></a></span>
<span id="cb10-34"><a href="lecture5.html#cb10-34"></a>    <span class="co">% Compute test statistic</span></span>
<span id="cb10-35"><a href="lecture5.html#cb10-35"></a>    e = Y - [X,Z]*estimates_temp;</span>
<span id="cb10-36"><a href="lecture5.html#cb10-36"></a>    t = estimates_temp(<span class="fl">2</span>)/sqrt(var(e)*([<span class="fl">0</span>,<span class="fl">1</span>]*inv([X,Z]&#39;*[X,Z])*[<span class="fl">0</span>;<span class="fl">1</span>]));</span>
<span id="cb10-37"><a href="lecture5.html#cb10-37"></a>    </span>
<span id="cb10-38"><a href="lecture5.html#cb10-38"></a>    <span class="co">% Pick estimate depending on test statistic</span></span>
<span id="cb10-39"><a href="lecture5.html#cb10-39"></a>    if abs(t)&gt;<span class="fl">1.96</span></span>
<span id="cb10-40"><a href="lecture5.html#cb10-40"></a>        alpha_pretest(sim) = alpha_long(sim);</span>
<span id="cb10-41"><a href="lecture5.html#cb10-41"></a>    else</span>
<span id="cb10-42"><a href="lecture5.html#cb10-42"></a>        alpha_pretest(sim) = alpha_short(sim);</span>
<span id="cb10-43"><a href="lecture5.html#cb10-43"></a>    end</span>
<span id="cb10-44"><a href="lecture5.html#cb10-44"></a>end</span>
<span id="cb10-45"><a href="lecture5.html#cb10-45"></a></span>
<span id="cb10-46"><a href="lecture5.html#cb10-46"></a><span class="co">% Plot results</span></span>
<span id="cb10-47"><a href="lecture5.html#cb10-47"></a>figure</span>
<span id="cb10-48"><a href="lecture5.html#cb10-48"></a>set(gcf,<span class="st">&#39;position&#39;</span>,[<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1000</span>,<span class="fl">500</span>])</span>
<span id="cb10-49"><a href="lecture5.html#cb10-49"></a>subplot(<span class="fl">1</span>,<span class="fl">3</span>,<span class="fl">1</span>)</span>
<span id="cb10-50"><a href="lecture5.html#cb10-50"></a>hist(alpha_short, <span class="fl">20</span>), title([<span class="st">&#39;alpha short&#39;</span>],<span class="st">&#39;fontsize&#39;</span>,<span class="fl">12</span>)</span>
<span id="cb10-51"><a href="lecture5.html#cb10-51"></a>subplot(<span class="fl">1</span>,<span class="fl">3</span>,<span class="fl">2</span>)</span>
<span id="cb10-52"><a href="lecture5.html#cb10-52"></a>hist(alpha_long, <span class="fl">20</span>), title([<span class="st">&#39;alpha long&#39;</span>],<span class="st">&#39;fontsize&#39;</span>,<span class="fl">12</span>)</span>
<span id="cb10-53"><a href="lecture5.html#cb10-53"></a>xlabel([<span class="st">&#39;alpha=&#39;</span>,num2str(alpha),<span class="st">&#39;, beta=&#39;</span>,num2str(beta),<span class="st">&#39;, gamma=&#39;</span>,num2str(gamma),<span class="st">&#39;, n=&#39;</span>,num2str(n)], <span class="st">&#39;fontsize&#39;</span>,<span class="fl">10</span>)</span>
<span id="cb10-54"><a href="lecture5.html#cb10-54"></a>subplot(<span class="fl">1</span>,<span class="fl">3</span>,<span class="fl">3</span>)</span>
<span id="cb10-55"><a href="lecture5.html#cb10-55"></a>hist(alpha_pretest, <span class="fl">20</span>), title([<span class="st">&#39;alpha pre-test&#39;</span>],<span class="st">&#39;fontsize&#39;</span>,<span class="fl">12</span>)</span></code></pre></div>
<p>Now we repeat the same exercise but for two different specifications of <span class="math inline">\(\beta_0\)</span>: one that depends on the sample size <span class="math inline">\(n\)</span> and one that does not. We check what happens for different sample sizes.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb11-1"><a href="lecture5.html#cb11-1"></a></span>
<span id="cb11-2"><a href="lecture5.html#cb11-2"></a><span class="co">% Sequence of sample sizes</span></span>
<span id="cb11-3"><a href="lecture5.html#cb11-3"></a>n_sequence = [<span class="fl">100</span>,<span class="fl">1000</span>,<span class="fl">10000</span>,<span class="fl">50000</span>,<span class="fl">100000</span>];</span>
<span id="cb11-4"><a href="lecture5.html#cb11-4"></a></span>
<span id="cb11-5"><a href="lecture5.html#cb11-5"></a><span class="co">% Initialize figure</span></span>
<span id="cb11-6"><a href="lecture5.html#cb11-6"></a>figure</span>
<span id="cb11-7"><a href="lecture5.html#cb11-7"></a>set(gcf,<span class="st">&#39;position&#39;</span>,[<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2000</span>,<span class="fl">1000</span>])</span>
<span id="cb11-8"><a href="lecture5.html#cb11-8"></a>n_fig = <span class="fl">1</span>;</span>
<span id="cb11-9"><a href="lecture5.html#cb11-9"></a></span>
<span id="cb11-10"><a href="lecture5.html#cb11-10"></a><span class="co">% Loop over data generating processes</span></span>
<span id="cb11-11"><a href="lecture5.html#cb11-11"></a>for dgp=<span class="fl">0</span>:<span class="fl">1</span></span>
<span id="cb11-12"><a href="lecture5.html#cb11-12"></a>    for n=n_sequence</span>
<span id="cb11-13"><a href="lecture5.html#cb11-13"></a>        for sim=<span class="fl">1</span>:n_simulations</span>
<span id="cb11-14"><a href="lecture5.html#cb11-14"></a></span>
<span id="cb11-15"><a href="lecture5.html#cb11-15"></a>            <span class="co">% Select beta according to dgp</span></span>
<span id="cb11-16"><a href="lecture5.html#cb11-16"></a>            if dgp==<span class="fl">0</span></span>
<span id="cb11-17"><a href="lecture5.html#cb11-17"></a>                beta_n = beta;</span>
<span id="cb11-18"><a href="lecture5.html#cb11-18"></a>            else</span>
<span id="cb11-19"><a href="lecture5.html#cb11-19"></a>                beta_n = beta/sqrt(n)*sqrt(<span class="fl">100</span>);</span>
<span id="cb11-20"><a href="lecture5.html#cb11-20"></a>            end</span>
<span id="cb11-21"><a href="lecture5.html#cb11-21"></a></span>
<span id="cb11-22"><a href="lecture5.html#cb11-22"></a>            <span class="co">% Generate Data</span></span>
<span id="cb11-23"><a href="lecture5.html#cb11-23"></a>            Z = randn(n,<span class="fl">1</span>);</span>
<span id="cb11-24"><a href="lecture5.html#cb11-24"></a>            X = gamma*Z + randn(n,<span class="fl">1</span>);</span>
<span id="cb11-25"><a href="lecture5.html#cb11-25"></a>            e = randn(n,<span class="fl">1</span>);</span>
<span id="cb11-26"><a href="lecture5.html#cb11-26"></a>            Y = alpha*X+ beta_n*Z + e;</span>
<span id="cb11-27"><a href="lecture5.html#cb11-27"></a></span>
<span id="cb11-28"><a href="lecture5.html#cb11-28"></a>            <span class="co">% Alpha estimate from long regression</span></span>
<span id="cb11-29"><a href="lecture5.html#cb11-29"></a>            alpha_short(sim) = inv(X&#39;*X)*X&#39;*Y;</span>
<span id="cb11-30"><a href="lecture5.html#cb11-30"></a></span>
<span id="cb11-31"><a href="lecture5.html#cb11-31"></a>            <span class="co">% Alpha estimate from long regression</span></span>
<span id="cb11-32"><a href="lecture5.html#cb11-32"></a>            estimates_temp = inv([X,Z]&#39;*[X,Z])*[X,Z]&#39;*Y;</span>
<span id="cb11-33"><a href="lecture5.html#cb11-33"></a>            alpha_long(sim) = estimates_temp(<span class="fl">1</span>);</span>
<span id="cb11-34"><a href="lecture5.html#cb11-34"></a></span>
<span id="cb11-35"><a href="lecture5.html#cb11-35"></a>            <span class="co">% Compute test statistic</span></span>
<span id="cb11-36"><a href="lecture5.html#cb11-36"></a>            e = Y - [X,Z]*estimates_temp;</span>
<span id="cb11-37"><a href="lecture5.html#cb11-37"></a>            t = estimates_temp(<span class="fl">2</span>)/sqrt(var(e)*([<span class="fl">0</span>,<span class="fl">1</span>]*inv([X,Z]&#39;*[X,Z])*[<span class="fl">0</span>;<span class="fl">1</span>]));</span>
<span id="cb11-38"><a href="lecture5.html#cb11-38"></a></span>
<span id="cb11-39"><a href="lecture5.html#cb11-39"></a>            <span class="co">% Pick estimate depending on test statistic</span></span>
<span id="cb11-40"><a href="lecture5.html#cb11-40"></a>            if abs(t)&gt;<span class="fl">1.96</span></span>
<span id="cb11-41"><a href="lecture5.html#cb11-41"></a>                alpha_pretest(sim) = alpha_long(sim);</span>
<span id="cb11-42"><a href="lecture5.html#cb11-42"></a>            else</span>
<span id="cb11-43"><a href="lecture5.html#cb11-43"></a>                alpha_pretest(sim) = alpha_short(sim);</span>
<span id="cb11-44"><a href="lecture5.html#cb11-44"></a>            end</span>
<span id="cb11-45"><a href="lecture5.html#cb11-45"></a>        end</span>
<span id="cb11-46"><a href="lecture5.html#cb11-46"></a></span>
<span id="cb11-47"><a href="lecture5.html#cb11-47"></a>        <span class="co">% Add subplot</span></span>
<span id="cb11-48"><a href="lecture5.html#cb11-48"></a>        subplot(<span class="fl">2</span>,length(n_sequence),n_fig)</span>
<span id="cb11-49"><a href="lecture5.html#cb11-49"></a>        hist((alpha_pretest-alpha)*sqrt(n/<span class="fl">100</span>), <span class="fl">12</span>)</span>
<span id="cb11-50"><a href="lecture5.html#cb11-50"></a>        title([<span class="st">&#39;beta=&#39;</span>,num2str(beta_n)],<span class="st">&#39;fontsize&#39;</span>,<span class="fl">10</span>)</span>
<span id="cb11-51"><a href="lecture5.html#cb11-51"></a>        xlabel([<span class="st">&#39;n=&#39;</span>,num2str(n)], <span class="st">&#39;fontsize&#39;</span>,<span class="fl">10</span>)</span>
<span id="cb11-52"><a href="lecture5.html#cb11-52"></a>        n_fig = n_fig+<span class="fl">1</span>;</span>
<span id="cb11-53"><a href="lecture5.html#cb11-53"></a></span>
<span id="cb11-54"><a href="lecture5.html#cb11-54"></a>    end</span>
<span id="cb11-55"><a href="lecture5.html#cb11-55"></a>end</span></code></pre></div>
</div>
</div>
<div id="post-double-selection" class="section level2">
<h2><span class="header-section-number">8.3</span> Post Double Selection</h2>
<p>Consider again data <span class="math inline">\(D= (y_i, x_i, z_i)_{i=1}^n\)</span>, where the true model is:
<span class="math display">\[
\begin{aligned}
&amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \\
&amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
\]</span></p>
<p>We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.</p>
<p>Consider a regression <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span>. <span class="math inline">\(x_i\)</span> is the 1-dimensional variable of interest, <span class="math inline">\(z_i\)</span> is a high-dimensional set of control variables. We have the following procedure:</p>
<ol style="list-style-type: decimal">
<li><strong>First Stage</strong> selection: lasso <span class="math inline">\(x_i\)</span> on <span class="math inline">\(z_i\)</span>. Let the selected variables be collected in the set <span class="math inline">\(S_{FS} \subseteq z_i\)</span></li>
<li><strong>Reduced Form</strong> selection: lasso <span class="math inline">\(y_i\)</span> on <span class="math inline">\(z_i\)</span>. Let the selected variables be collected in the set <span class="math inline">\(S_{RF} \subseteq z_i\)</span></li>
<li>Regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(S_{FS} \cup S_{RF}\)</span></li>
</ol>
<p><strong>Theorem</strong>:
Let <span class="math inline">\(\{P^n\}\)</span> be a sequence of data-generating processes for <span class="math inline">\(D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n\)</span> where <span class="math inline">\(p\)</span> depends on <span class="math inline">\(n\)</span>. For each <span class="math inline">\(n\)</span>, the data are iid with <span class="math inline">\(yi = x_i&#39;\alpha_0^{(n)} + z_i&#39; \beta_0^{(n)} + \varepsilon_i\)</span> and <span class="math inline">\(x_i = z_i&#39; \gamma_0^{(n)} + u_i\)</span> where <span class="math inline">\(\mathbb E[\varepsilon_i | x_i,z_i] = 0\)</span> and <span class="math inline">\(\mathbb E[u_i|z_i] = 0\)</span>. The sparsity of the vectors <span class="math inline">\(\beta_0^{(n)}\)</span>, <span class="math inline">\(\gamma_0^{(n)}\)</span> is controlled by <span class="math inline">\(|| \beta_0^{(n)} ||_0 \leq s\)</span> with <span class="math inline">\(s^2 (\log p)^2/n \to 0\)</span>. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables <span class="math inline">\(y_i\)</span> , <span class="math inline">\(x_i\)</span> , <span class="math inline">\(z_i\)</span> as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level <span class="math inline">\(\xi \in (0, 1)\)</span>
<span class="math display">\[
        \Pr(\alpha_0 \in CI) \to 1- \xi
\]</span></p>
<p>In order to have valid confidence intervals you want their bias to be negligibly. Since
<span class="math display">\[
  CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
\]</span></p>
<p>If the bias is <span class="math inline">\(o \left( \frac{1}{\sqrt{n}} \right)\)</span> then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is <span class="math inline">\(O \left( \frac{1}{\sqrt{n}} \right)\)</span> then it has the same magnitude of the confidence interval and it does not asymptotically vanish.</p>
<p>The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:</p>
<ol style="list-style-type: decimal">
<li>Its partial correlation with the outcome, and</li>
<li>Its partial correlation with the variable of interest.</li>
</ol>
<p>If both those partial correlations are <span class="math inline">\(O( \sqrt{\log p/n})\)</span>, then the omitted variables bias is <span class="math inline">\((s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)\)</span>, provided <span class="math inline">\(s^2 (\log p)^2/n \to 0\)</span>. Relative to the <span class="math inline">\(\frac{1}{\sqrt{n}\)</span> convergence rate, the omitted variables bias is negligible.</p>
<p>In our omitted variable bias case, we want <span class="math inline">\(| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)\)</span>. Post-double selection guarantees that</p>
<ul>
<li><em>Reduced form</em> selection (pre-testing): any “missing” variable has <span class="math inline">\(|\beta_{0j}| \leq \frac{c}{\sqrt{n}}\)</span></li>
<li><em>First stage</em> selection (additional): any “missing” variable has <span class="math inline">\(|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}\)</span></li>
</ul>
<p>As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
<span class="math display">\[
    OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
\]</span></p>
<div class="figure">
<img src="figures/Fig_641.png" alt="" />
<p class="caption">Pre-test Bias</p>
</div>
<p>Under homoskedasticity, the above estimator achieves the semiparametric efficiency bound.</p>
</div>
<div id="references-8" class="section level2">
<h2><span class="header-section-number">8.4</span> References</h2>
<ul>
<li>Belloni, A., Chernozhukov, V., &amp; Hansen, C. (2014). <em>Inference on Treatment Effects after Selection among High-Dimensional Controls</em>. The Review of Economic Studies, 81(2), 608–650.</li>
<li>Hastie, Tibshirani, Friedman (2001). “<em>The Elements of Statistical Learning</em>”.</li>
<li>Hansen (2019). “<em>Econometrics</em>”. Chapter 24.</li>
<li>Kozbur (2019). PhD Econometrics - Lecture Notes.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matlabcode.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["notes.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
