<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 8 Variable Selection | Econometrics Notes</title>
  <meta name="description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 8 Variable Selection | Econometrics Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 8 Variable Selection | Econometrics Notes" />
  
  <meta name="twitter:description" content="Lecture 8 Variable Selection | Econometrics Notes" />
  

<meta name="author" content="Matteo Courthoud" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture4.html">
<link rel="next" href="matlabcode.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../" target="_self">MATTEO COURTHOUD</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="appendix1.html"><a href="appendix1.html"><i class="fa fa-check"></i><b>1</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="appendix1.html"><a href="appendix1.html#basics"><i class="fa fa-check"></i><b>1.1</b> Basics</a></li>
<li class="chapter" data-level="1.2" data-path="appendix1.html"><a href="appendix1.html#spectral-decomposition"><i class="fa fa-check"></i><b>1.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="1.3" data-path="appendix1.html"><a href="appendix1.html#quadratic-forms-and-definite-matrices"><i class="fa fa-check"></i><b>1.3</b> Quadratic Forms and Definite Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="appendix1.html"><a href="appendix1.html#matrix-calculus"><i class="fa fa-check"></i><b>1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.5" data-path="appendix1.html"><a href="appendix1.html#references-1"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="appendix2.html"><a href="appendix2.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix2.html"><a href="appendix2.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a></li>
<li class="chapter" data-level="2.2" data-path="appendix2.html"><a href="appendix2.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="appendix2.html"><a href="appendix2.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a></li>
<li class="chapter" data-level="2.4" data-path="appendix2.html"><a href="appendix2.html#inequalities"><i class="fa fa-check"></i><b>2.4</b> Inequalities</a></li>
<li class="chapter" data-level="2.5" data-path="appendix2.html"><a href="appendix2.html#theorems"><i class="fa fa-check"></i><b>2.5</b> Theorems</a></li>
<li class="chapter" data-level="2.6" data-path="appendix2.html"><a href="appendix2.html#statistical-models"><i class="fa fa-check"></i><b>2.6</b> Statistical Models</a></li>
<li class="chapter" data-level="2.7" data-path="appendix2.html"><a href="appendix2.html#references-2"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix3.html"><a href="appendix3.html"><i class="fa fa-check"></i><b>3</b> Asymptotic Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix3.html"><a href="appendix3.html#convergence"><i class="fa fa-check"></i><b>3.1</b> Convergence</a></li>
<li class="chapter" data-level="3.2" data-path="appendix3.html"><a href="appendix3.html#theorems-1"><i class="fa fa-check"></i><b>3.2</b> Theorems</a></li>
<li class="chapter" data-level="3.3" data-path="appendix3.html"><a href="appendix3.html#ergodic-theory"><i class="fa fa-check"></i><b>3.3</b> Ergodic Theory</a></li>
<li class="chapter" data-level="3.4" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-estimators"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Properties of Estimators</a></li>
<li class="chapter" data-level="3.5" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-test-statistics"><i class="fa fa-check"></i><b>3.5</b> Asymptotic Properties of Test Statistics</a></li>
<li class="chapter" data-level="3.6" data-path="appendix3.html"><a href="appendix3.html#references-3"><i class="fa fa-check"></i><b>3.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>4</b> OLS Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="lecture1.html"><a href="lecture1.html#the-gauss-markov-model"><i class="fa fa-check"></i><b>4.1</b> The Gauss Markov Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lecture1.html"><a href="lecture1.html#matlab"><i class="fa fa-check"></i><b>4.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture1.html"><a href="lecture1.html#the-ols-estimator"><i class="fa fa-check"></i><b>4.2</b> The OLS estimator</a><ul>
<li class="chapter" data-level="4.2.1" data-path="lecture1.html"><a href="lecture1.html#matlab-1"><i class="fa fa-check"></i><b>4.2.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture1.html"><a href="lecture1.html#ols-residual-properties"><i class="fa fa-check"></i><b>4.3</b> OLS residual properties</a><ul>
<li class="chapter" data-level="4.3.1" data-path="lecture1.html"><a href="lecture1.html#matlab-2"><i class="fa fa-check"></i><b>4.3.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture1.html"><a href="lecture1.html#finite-sample-properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>4.4</b> Finite Sample Properties of the OLS estimator</a><ul>
<li class="chapter" data-level="4.4.1" data-path="lecture1.html"><a href="lecture1.html#matlab-3"><i class="fa fa-check"></i><b>4.4.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lecture1.html"><a href="lecture1.html#references-4"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>5</b> OLS Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="lecture2.html"><a href="lecture2.html#asymptotic-theory-of-the-ols-estimator"><i class="fa fa-check"></i><b>5.1</b> Asymptotic Theory of the OLS Estimator</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lecture2.html"><a href="lecture2.html#gaussian-error-term"><i class="fa fa-check"></i><b>5.1.1</b> Gaussian Error Term</a></li>
<li class="chapter" data-level="5.1.2" data-path="lecture2.html"><a href="lecture2.html#homoskedastic-error-term"><i class="fa fa-check"></i><b>5.1.2</b> Homoskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.3" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-error-term"><i class="fa fa-check"></i><b>5.1.3</b> Heteroskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.4" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-and-autocorrelated-error-term"><i class="fa fa-check"></i><b>5.1.4</b> Heteroskedastic and Autocorrelated Error Term</a></li>
<li class="chapter" data-level="5.1.5" data-path="lecture2.html"><a href="lecture2.html#fixed-b-asymptotics"><i class="fa fa-check"></i><b>5.1.5</b> Fixed b asymptotics</a></li>
<li class="chapter" data-level="5.1.6" data-path="lecture2.html"><a href="lecture2.html#fixed-g-asymptotics"><i class="fa fa-check"></i><b>5.1.6</b> Fixed G asymptotics</a></li>
<li class="chapter" data-level="5.1.7" data-path="lecture2.html"><a href="lecture2.html#matlab-4"><i class="fa fa-check"></i><b>5.1.7</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lecture2.html"><a href="lecture2.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lecture2.html"><a href="lecture2.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2.2" data-path="lecture2.html"><a href="lecture2.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="lecture2.html"><a href="lecture2.html#matlab-5"><i class="fa fa-check"></i><b>5.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="lecture2.html"><a href="lecture2.html#references-5"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>6</b> Endogeneity</a><ul>
<li class="chapter" data-level="6.1" data-path="lecture3.html"><a href="lecture3.html#instrumental-variables"><i class="fa fa-check"></i><b>6.1</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lecture3.html"><a href="lecture3.html#matlab-6"><i class="fa fa-check"></i><b>6.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lecture3.html"><a href="lecture3.html#gmm"><i class="fa fa-check"></i><b>6.2</b> GMM</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lecture3.html"><a href="lecture3.html#step-gmm"><i class="fa fa-check"></i><b>6.2.1</b> 1-step GMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="lecture3.html"><a href="lecture3.html#step-gmm-1"><i class="fa fa-check"></i><b>6.2.2</b> 2-step GMM</a></li>
<li class="chapter" data-level="6.2.3" data-path="lecture3.html"><a href="lecture3.html#matlab-7"><i class="fa fa-check"></i><b>6.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture3.html"><a href="lecture3.html#testing-overidentifying-restrictions"><i class="fa fa-check"></i><b>6.3</b> Testing Overidentifying Restrictions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lecture3.html"><a href="lecture3.html#naive-test"><i class="fa fa-check"></i><b>6.3.1</b> Naive Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="lecture3.html"><a href="lecture3.html#hansens-test"><i class="fa fa-check"></i><b>6.3.2</b> Hansen’s Test</a></li>
<li class="chapter" data-level="6.3.3" data-path="lecture3.html"><a href="lecture3.html#special-case-conditional-homoskedasticity"><i class="fa fa-check"></i><b>6.3.3</b> Special Case: Conditional Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lecture3.html"><a href="lecture3.html#small-sample-properties-of-2sls"><i class="fa fa-check"></i><b>6.4</b> Small-Sample Properties of 2SLS</a></li>
<li class="chapter" data-level="6.5" data-path="lecture3.html"><a href="lecture3.html#many-instrument-robust-estimation"><i class="fa fa-check"></i><b>6.5</b> Many Instrument Robust Estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="lecture3.html"><a href="lecture3.html#liml"><i class="fa fa-check"></i><b>6.5.1</b> LIML</a></li>
<li class="chapter" data-level="6.5.2" data-path="lecture3.html"><a href="lecture3.html#jive"><i class="fa fa-check"></i><b>6.5.2</b> JIVE</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lecture3.html"><a href="lecture3.html#hausman-test"><i class="fa fa-check"></i><b>6.6</b> Hausman Test</a></li>
<li class="chapter" data-level="6.7" data-path="lecture3.html"><a href="lecture3.html#references-6"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>7</b> Non-Parametric Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="lecture4.html"><a href="lecture4.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="lecture4.html"><a href="lecture4.html#discrete-x---cell-estimator"><i class="fa fa-check"></i><b>7.2</b> Discrete x - Cell Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="lecture4.html"><a href="lecture4.html#local-non-parametric-estimation---kernels"><i class="fa fa-check"></i><b>7.3</b> Local Non-Parametric Estimation - Kernels</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lecture4.html"><a href="lecture4.html#estimator-examples"><i class="fa fa-check"></i><b>7.3.1</b> Estimator examples:</a></li>
<li class="chapter" data-level="7.3.2" data-path="lecture4.html"><a href="lecture4.html#kernel-examples"><i class="fa fa-check"></i><b>7.3.2</b> Kernel examples:</a></li>
<li class="chapter" data-level="7.3.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-bandwidth"><i class="fa fa-check"></i><b>7.3.3</b> Choice of the optimal bandwidth</a></li>
<li class="chapter" data-level="7.3.4" data-path="lecture4.html"><a href="lecture4.html#inference-1"><i class="fa fa-check"></i><b>7.3.4</b> Inference</a></li>
<li class="chapter" data-level="7.3.5" data-path="lecture4.html"><a href="lecture4.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.3.5</b> Bias-variance trade-off</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lecture4.html"><a href="lecture4.html#global-non-parametric-estimation---series"><i class="fa fa-check"></i><b>7.4</b> Global Non-Parametric Estimation - Series</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lecture4.html"><a href="lecture4.html#examples"><i class="fa fa-check"></i><b>7.4.1</b> Examples</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture4.html"><a href="lecture4.html#estimation"><i class="fa fa-check"></i><b>7.4.2</b> Estimation</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-k"><i class="fa fa-check"></i><b>7.4.3</b> Choice of the optimal <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture4.html"><a href="lecture4.html#inference-2"><i class="fa fa-check"></i><b>7.4.4</b> Inference</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture4.html"><a href="lecture4.html#kernel-vs-series"><i class="fa fa-check"></i><b>7.4.5</b> Kernel vs Series</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lecture4.html"><a href="lecture4.html#references-7"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>8</b> Variable Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="lecture5.html"><a href="lecture5.html#lasso"><i class="fa fa-check"></i><b>8.1</b> Lasso</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lecture5.html"><a href="lecture5.html#choosing-the-optimal-lambda"><i class="fa fa-check"></i><b>8.1.1</b> Choosing the optimal lambda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lecture5.html"><a href="lecture5.html#pre-testing"><i class="fa fa-check"></i><b>8.2</b> Pre-Testing</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lecture5.html"><a href="lecture5.html#omitted-variable-bias"><i class="fa fa-check"></i><b>8.2.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.2.2" data-path="lecture5.html"><a href="lecture5.html#pre-test-bias"><i class="fa fa-check"></i><b>8.2.2</b> Pre-test bias</a></li>
<li class="chapter" data-level="8.2.3" data-path="lecture5.html"><a href="lecture5.html#partioned-regression"><i class="fa fa-check"></i><b>8.2.3</b> Partioned Regression</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lecture5.html"><a href="lecture5.html#post-double-selection"><i class="fa fa-check"></i><b>8.3</b> Post Double Selection</a></li>
<li class="chapter" data-level="8.4" data-path="lecture5.html"><a href="lecture5.html#references-8"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matlabcode.html"><a href="matlabcode.html"><i class="fa fa-check"></i><b>9</b> Matlab Code</a><ul>
<li class="chapter" data-level="9.1" data-path="matlabcode.html"><a href="matlabcode.html#lecture-1"><i class="fa fa-check"></i><b>9.1</b> Lecture 1</a></li>
<li class="chapter" data-level="9.2" data-path="matlabcode.html"><a href="matlabcode.html#lecture-2"><i class="fa fa-check"></i><b>9.2</b> Lecture 2</a></li>
<li class="chapter" data-level="9.3" data-path="matlabcode.html"><a href="matlabcode.html#lecture-3"><i class="fa fa-check"></i><b>9.3</b> Lecture 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture5" class="section level1">
<h1><span class="header-section-number">Lecture 8</span> Variable Selection</h1>
<div id="lasso" class="section level2">
<h2><span class="header-section-number">8.1</span> Lasso</h2>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) is a particularly popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called <em>penalized estimators</em>. It allows number of series terms higher than <span class="math inline">\(n\)</span>.</p>
<p>Consider data <span class="math inline">\(D = \{ x_i, y_i \}_{i=1}^n\)</span> with <span class="math inline">\(\dim (x_i) = p\)</span>. Assume that <span class="math inline">\(p\)</span> is large relative to <span class="math inline">\(n\)</span>. Two possible reasons:</p>
<ul>
<li>we have an intrinsic problem of high dimensionality</li>
<li><span class="math inline">\(p\)</span> indicates the number of expansion terms of small number of underlying important variables (e.g. series estimation)</li>
</ul>
<p><strong>Assumption</strong>:
<span class="math inline">\(y_i = x_i&#39; \beta_0 + r_i + \varepsilon_i\)</span> where <span class="math inline">\(\beta_0\)</span> depends on <span class="math inline">\(p\)</span>, <span class="math inline">\(r_i\)</span> is a remainder term.</p>
<p>Note that in classic non-parametrics, we have <span class="math inline">\(x_i&#39;\beta_0\)</span> as <span class="math inline">\(p_1(x_i) \beta_{1,K} + \dots + p_K(x_i) \beta_{K,K}\)</span>. For simplicity, we assume <span class="math inline">\(r_i = 0\)</span>, as if we had extreme undersmoothing. Hence the model becomes:
<span class="math display">\[
    y_i = x_i&#39; \beta_0 + \varepsilon_i, \qquad p \geq n
\]</span>
We cannot run OLS because <span class="math inline">\(p \geq n\)</span>, thus the rank condition is violated.</p>
<p>We define the <strong>Lasso estimator</strong> as
<span class="math display">\[
    \hat{\beta}_L = \arg \min \quad \underbrace{\mathbb E_n \Big[ (y_i - x_i&#39; \beta)^2 \Big]}_{\text{SSR term}} + \underbrace{\frac{\lambda}{n} \sum_{j=1}^{P} | \beta_j |}_{\text{Penalty term}}
\]</span>
where <span class="math inline">\(\lambda\)</span> is called <strong>penalty parameter</strong>.</p>
<p>The <strong>penalty term</strong> discourages large values of <span class="math inline">\(| \beta_j |\)</span>.
The choice of <span class="math inline">\(\lambda\)</span> is analogous to the choice of <span class="math inline">\(K\)</span> in series estimation and <span class="math inline">\(h\)</span> in the kernel estimation.</p>
<blockquote>
<p>The shrinkage to zero of the coefficients directly follows from the <span class="math inline">\(|| \cdot ||_1\)</span> norm. On the contrary, another famous penalized estimator, <em>ridge regression</em>, uses the <span class="math inline">\(|| \cdot ||_2\)</span> norm and does not have this property.</p>
</blockquote>
<div class="figure">
<img src="figures/Fig_551.png" alt="" />
<p class="caption">Constraints</p>
</div>
<blockquote>
<p>Minimizing SSR + penalty is equivalent to minimize SSR <span class="math inline">\(s.t.\)</span> pen <span class="math inline">\(\leq c\)</span> (clear from the picture).</p>
</blockquote>
<p>Let <span class="math inline">\(S_0 = \{ j: \beta_{0,j} \ne 0 \}\)</span>, we define <span class="math inline">\(s_0 = \# S_0\)</span> as the <strong>sparsity</strong> of <span class="math inline">\(\beta_0\)</span>. If <span class="math inline">\(s_0/n \to 0\)</span>, we are dealing with a <strong>sparse regression</strong> (analogous of smooth regression).</p>
<blockquote>
<p>Remark on sparsity:</p>
<ul>
<li>In words, sparsity means that even if we have a lot of variables, only a small number of them (relative to <span class="math inline">\(n\)</span>) have an effect on the dependent variable.</li>
<li><em>Approximate sparsity imposes a restriction that only <span class="math inline">\(s_0\)</span> variables among all of <span class="math inline">\(x_{ij}\)</span>, where <span class="math inline">\(s_0\)</span> is much smaller than <span class="math inline">\(n\)</span>, have associated coefficients <span class="math inline">\(\beta_{0j}\)</span> that are different from zero, while permitting a nonzero approximation error. Thus, estimators for this kind of model attempt to learn the identities of the variables with large nonzero coefficients, while simultaneously estimating these coefficients.</em> (Belloni et al., 2004)</li>
<li>Sparsity is an assumption. <span class="math inline">\(\beta_0\)</span> is said to be <span class="math inline">\(s_0\)</span>-sparse with <span class="math inline">\(s_0 &lt; n\)</span> if
<span class="math display">\[
| \{ j: \beta_{0j} \neq 0 \} | \leq s_0
\]</span></li>
</ul>
</blockquote>
<p><strong>Theorem</strong>:
Suppose that for data <span class="math inline">\(D_n = (y_i, x_i)_{i=1}^N\)</span> with <span class="math inline">\(y_i = x_i&#39; \beta + \varepsilon_i\)</span>. Let <span class="math inline">\(\hat{\beta}_L\)</span> be the Lasso estimator. Let <span class="math inline">\(\mathcal{S} = 2 \max_j | \mathbb E[ x_{ij} \varepsilon_i] |\)</span>. Suppose <span class="math inline">\(|support(\beta_0) \leq s_0\)</span> (sparsity assumption). Let <span class="math inline">\(c_0 = (\mathcal{S} + \lambda/n )/(-\mathcal{S} + \lambda/n )\)</span>. Let
<span class="math display">\[
    \kappa_{c_0, s_0} = \min_{  d \in \mathbb R^p, A \subseteq \{ 1, ... , p \} : |A| \leq s_0 ,  || d_{A^c}|| \leq c_0 || d_A ||_1  }  \sqrt{  \frac{ s_0 d&#39; \mathbb E_n [x_i x_i&#39;] d }{|| d_A ||_1^2}  }
\]</span>
Then</p>
<p><span class="math display">\[
    \mathbb I_{ \left\{ \frac{\lambda}{n} &gt; \mathcal{S}  \right\}} \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \leq 2 \frac{\lambda}{n} \frac{\sqrt{s_0}}{\kappa_{c_0, s_0}} \\
\]</span></p>
<p>Intuition: for a sufficiently high lambda the root mean squared error of Lasso is approximately zero.</p>
<p><span class="math display">\[
    \text{ RMSE }:  \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \simeq 0  \quad \Leftrightarrow \quad \frac{\lambda}{n} &gt; \mathcal{S}
\]</span></p>
<blockquote>
<p>Note on the theorem:</p>
<ul>
<li>The minimization region is the set of ``essentially sparse" vectors <span class="math inline">\(d \in \mathbb R^p\)</span> where essentially sparse is defined by <span class="math inline">\(\mathcal{C}, \mathcal{S}\)</span>. In particular the condition <span class="math inline">\(k_{\mathcal{C}, \mathcal{S}}&gt;0\)</span> means that no essentially sparse vector <span class="math inline">\(d\)</span> has <span class="math inline">\(\mathbb E[x_i x_i&#39;]d = 0\)</span>, i.e. regressors were not added multiple times.</li>
<li>Need to dominate the score with the penalty term <span class="math inline">\(\lambda\)</span>.</li>
<li>Need no collinearity on a small (<span class="math inline">\(\leq s_0\)</span>) subset of regressors (<span class="math inline">\(\to k_{c_0, s_0}&gt;0\)</span>).</li>
</ul>
</blockquote>
<p><strong>When Lasso?</strong> For prediction problems in high dimensional environments. <strong>NB!</strong> Lasso is not good for inference, only for prediction.</p>
<p>In particular, in econometrics it’s used for selecting either</p>
<ul>
<li>instruments (predicting <span class="math inline">\(\hat{x}\)</span> in the first stage)</li>
<li>control variables (next section: double prediction problem, in the first stage and in the reduced form)</li>
</ul>
<div id="choosing-the-optimal-lambda" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Choosing the optimal lambda</h3>
<p>The choice of <span class="math inline">\(\lambda\)</span> determines the bias-variance tradeoff:</p>
<ul>
<li>if <span class="math inline">\(\lambda\)</span> is too big: <span class="math inline">\(\lambda \approx \infty \mathbb Rightarrow \hat{\beta} \approx 0\)</span>;</li>
<li>if <span class="math inline">\(\lambda\)</span> is too small: <span class="math inline">\(\lambda \approx 0 \mathbb Rightarrow\)</span> overfitting.</li>
</ul>
<p>Possible solutions: Bonferroni correction, bootstrapping or <span class="math inline">\(\frac{\lambda}{n} \asymp \sqrt{\frac{\log(p)}{n}}\)</span> (asymptotically equal to), <span class="math inline">\(\mathcal{S}\)</span> behaves like the maximum of gaussians.</p>
<p><strong>The Lasso Path</strong>: how the estimated <span class="math inline">\(\hat{\beta}\)</span> depends on the penalty parameter <span class="math inline">\(\lambda\)</span>?</p>
<div class="figure">
<img src="figures/Fig_642.png" alt="" />
<p class="caption">Lasso Path</p>
</div>
<p><strong>Post Lasso</strong>: fit OLS without the penalty with all the nonzero coeficients selected by Lasso in the first step.</p>
<blockquote>
<p>On Lasso:</p>
<ul>
<li>Do not do inference with post-Lasso because standard errors are not uniformely valid.</li>
<li>As <span class="math inline">\(n \to \infty\)</span> the CV and the <strong>score domination</strong> bounds converge to a unique bound.</li>
<li>What is the problem of cross-validation? In high dimensional settings you can overfit in so many ways that CV doesn’t work and still overfits.</li>
<li>Using <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\frac{\lambda}{n} &gt; \mathcal{S}\)</span> small coefficients get shrunk to zero with high probability. In this case with small we mean <span class="math inline">\(\propto \frac{1}{\sqrt{n}}\)</span> or <span class="math inline">\(2 \max_j | \mathbb E_n[\varepsilon_i x_{ij}] |\)</span>.</li>
<li>If <span class="math inline">\(| \beta_{0j}| \leq \frac{c}{\sqrt{n}}\)</span> for a sufficiently small constant <span class="math inline">\(c\)</span>, then <span class="math inline">\(\hat{\beta}_{LASSO} \overset{p}{\to} 0\)</span>.</li>
<li>In standard t-tests <span class="math inline">\(c = 1.96\)</span>.</li>
<li><span class="math inline">\(\sqrt{n}\)</span> factor is important since it is the demarcation line for reliable statistical detection.</li>
</ul>
</blockquote>
</div>
</div>
<div id="pre-testing" class="section level2">
<h2><span class="header-section-number">8.2</span> Pre-Testing</h2>
<p>Main reference for this section: Belloni, Chernozhukov and Hansen <em>Inference for Treatment Effects with High Dimensional Controls</em> in the <em>Review of Economic Studies</em> (2014).</p>
<div id="omitted-variable-bias" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Omitted Variable Bias</h3>
<p>Consider two separate statistical models. Assume the following <strong>long regression</strong> of interest:</p>
<p><span class="math display">\[
  y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
\]</span></p>
<p>Define the corresponding <strong>short regression</strong></p>
<p><span class="math display">\[
  y_i = x_i&#39; \alpha_0 + v_i \quad \text{ with } v_i = z_i&#39; \beta_0 + \varepsilon_i
\]</span></p>
<p><strong>Theorem</strong>:
Suppose that the DGP for the long regression corresponds to <span class="math inline">\(\alpha_0\)</span>, <span class="math inline">\(\beta_0\)</span>. Suppose further that <span class="math inline">\(\mathbb E[x_i] = 0\)</span>, <span class="math inline">\(\mathbb E[z_i] = 0\)</span>, <span class="math inline">\(\mathbb E[\varepsilon_i |x_i,z_i] = 0\)</span>. Then, unless <span class="math inline">\(\beta_0 = 0\)</span> or <span class="math inline">\(z_i\)</span> is orthogonal to <span class="math inline">\(x_i\)</span>, the (sole) stochastic regressor <span class="math inline">\(x_i\)</span> is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for <span class="math inline">\(\alpha_0\)</span> due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of <span class="math inline">\(\hat{\alpha}_{SHORT}\)</span> from the short regression is
<span class="math display">\[
    \hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
\]</span></p>
</div>
<div id="pre-test-bias" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Pre-test bias</h3>
<p>Consider data <span class="math inline">\(D= (y_i, x_i, z_i)_{i=1}^n\)</span>, where the true model is:
<span class="math display">\[
\begin{aligned}
&amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \\
&amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(x_i\)</span> is the variable of interest (we want to make inference on <span class="math inline">\(\alpha_0\)</span>) and <span class="math inline">\(z_i\)</span> is a high dimensional set of control variables. \</p>
<p>From now on, we will work under the following assumptions:</p>
<ul>
<li><span class="math inline">\(\dim(x_i)=1\)</span> for all <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> uniformely bounded in <span class="math inline">\(n\)</span></li>
<li>Strict exogeneity: <span class="math inline">\(\mathbb E[\varepsilon_i | x_i, z_i] = 0\)</span> and <span class="math inline">\(\mathbb E[u_i | z_i] = 0\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\gamma_0\)</span> have dimension (and hence value) that depend on <span class="math inline">\(n\)</span></li>
</ul>
<p>Pre Testing procedure:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span></li>
<li>For each <span class="math inline">\(j = 1, ..., p = \dim(z_i)\)</span> calculate a test statistic <span class="math inline">\(t_j\)</span></li>
<li>Let <span class="math inline">\(\hat{T} = \{ j: |t_j| &gt; C &gt; 0 \}\)</span> for some constant <span class="math inline">\(C\)</span> (set of statistically significant coefficients).</li>
<li>Re-run the new ``model" using <span class="math inline">\((x_i, z_{\hat{T},i})\)</span> (i.e. using the selected covariates).</li>
<li>Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.</li>
</ol>
<p>This procedure is unfortunately not desirable in most practical settings since it introduces <strong>omitted variable bias</strong>.</p>
<div class="figure">
<img src="figures/Fig_621.png" alt="" />
<p class="caption">Pre-test Bias</p>
</div>
<p>Concept of <strong>uniformity</strong>: the DGP varies with <span class="math inline">\(n\)</span>. Instead of having a fixed “true” parameter <span class="math inline">\(\beta_0\)</span>, you have a sequence <span class="math inline">\(\beta_0(n)\)</span>. Then, we could plot a sequence of ordered pairs <span class="math inline">\((n, DGP)\)</span> as below. In order to study the behavior of <span class="math inline">\(\hat{\beta}\)</span> at your particular <span class="math inline">\(n\)</span>, you study its behavior along the sequence.</p>
<div class="figure">
<img src="figures/Fig_621.png" alt="" />
<p class="caption">Pre-test Bias</p>
</div>
<blockquote>
<p>Remark: pre-test bias is problematic because the post-selection estimator is not asymptotically normal. Moreover, for particular data generating processes, it even fails to be consistent at the rate of <span class="math inline">\(\sqrt{n}\)</span> (belloni et al., 2014). An approximate depiction of the DGPs for which consistency fails is depicted below.</p>
</blockquote>
<div class="figure">
<img src="figures/Fig_623.png" alt="" />
<p class="caption">Pre-test Bias</p>
</div>
<p>The intuition for the three different regions (from below to above) is the following.</p>
<ol style="list-style-type: decimal">
<li>When <span class="math inline">\(\beta_0 = o(1/\sqrt{n})\)</span>, <span class="math inline">\(z_i\)</span> is excluded with probability <span class="math inline">\(p \to 1\)</span>. But, given that <span class="math inline">\(\beta_0\)</span> is small enough, failing to control for <span class="math inline">\(z_i\)</span> does not introduce large omitted variables bias [BCH 14].</li>
<li>If however the coefficient on the control is ``moderately close to zero" (<span class="math inline">\(\beta_0 = O(1/\sqrt{n})\)</span>), the t-test set-up above still cannot distinguish this coefficient from <span class="math inline">\(0\)</span>, and the control <span class="math inline">\(z_i\)</span> is dropped with probability <span class="math inline">\(p \to 1\)</span>. However now the omitted variable bias created by dropping <span class="math inline">\(z_i\)</span> scaled by <span class="math inline">\(\sqrt{n}\)</span>, diverges to infinity. That is, the standard post-selection estimator is not asymptotically normal and even fails to be consistent at the rate of <span class="math inline">\(\sqrt{n}\)</span> [BCH 14].</li>
<li>Lastly, when <span class="math inline">\(\beta_0\)</span> is large enough, the null pre-testing hypothesis <span class="math inline">\(H_0 : \beta_0 = 0\)</span> will be rejected sufficiently often so that the bias is negligible.</li>
</ol>
<p>The post-double-selection estimator, <span class="math inline">\(\hat{\alpha}_{PDS}\)</span> solves this problem by doing variable selection via standard t-tests or Lasso-type selectors with the two ``true model" equations (<strong>first stage</strong> and <strong>reduced form</strong>) that contain the information from the model and then estimating <span class="math inline">\(\alpha_0\)</span> by regressing <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and the union of the selected controls. By doing so, <span class="math inline">\(z_i\)</span> is omitted only if its coefficient in both equations is small which greatly limits the potential for omitted variables bias (Beloni et al., 2014).</p>
</div>
<div id="partioned-regression" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Partioned Regression</h3>
<p><strong>Frisch-Waugh Theorem</strong>:
Consider the data <span class="math inline">\(D = \{ x_i, y_i, z_i \}_{i=1}^\infty\)</span> with DGP: <span class="math inline">\(Y = X \alpha + Z \beta + \varepsilon\)</span>. The following estimators of <span class="math inline">\(\alpha\)</span> are numerically equivalent (if <span class="math inline">\([X, Z]\)</span> has full rank):</p>
<ul>
<li><span class="math inline">\(\hat{\alpha}\)</span> from regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X, Z\)</span></li>
<li><span class="math inline">\(\tilde{\alpha}\)</span> from regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(\tilde{X}\)</span></li>
<li><span class="math inline">\(\bar{\alpha}\)</span> from regressing <span class="math inline">\(\tilde{Y}\)</span> on <span class="math inline">\(\tilde{X}\)</span></li>
</ul>
<p>where the operation of passing to <span class="math inline">\(Y, X\)</span> to <span class="math inline">\(\tilde{Y}, \tilde{X}\)</span> is called <em>projection out <span class="math inline">\(Z\)</span></em>, e.g.<span class="math inline">\(\tilde{X}\)</span> are the residuals from regressing <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>.</p>
<p><strong>Proof</strong>:
We want to show that <span class="math inline">\(\hat{\alpha} = \tilde{\alpha}\)</span>.</p>
<p>Claim: <span class="math inline">\(\hat{\alpha } = \tilde{\alpha} \Leftrightarrow \tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0\)</span>.</p>
<p>Proof of the claim: if <span class="math inline">\(\hat{\alpha} = \tilde{\alpha}\)</span>, we can write <span class="math inline">\(Y\)</span> as
<span class="math display">\[
    Y =  X \hat{\alpha} + Z \hat{\beta} + \hat{\varepsilon}  = \tilde{X} \hat{\alpha} + \underbrace{(X - \tilde{X}) \hat{\alpha } + Z \hat{\beta} + \hat{\varepsilon}}_\text{residual of $Y$ on $\tilde{X} $} = \tilde{X} \tilde{\alpha} + \nu_i
\]</span></p>
<p>Therefore, by the orthogonality property of the OLS residual, it must be that <span class="math inline">\(\tilde{X}&#39;\nu_i= 0\)</span>.
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<p>Having established the claim, we want to show that the normal equation <span class="math inline">\(\tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0\)</span> is satisfied. We follow 3 steps:</p>
<ol style="list-style-type: decimal">
<li><p>First we have that <span class="math inline">\(\tilde{X}&#39; (X - \tilde{X})\hat{\alpha} = 0\)</span>. This follows from the fact that <span class="math inline">\(\tilde{X}&#39; = X&#39; M_Z\)</span> and hence:
<span class="math display">\[
  \begin{aligned}
  \tilde{X}&#39; (X - \tilde{X})  &amp;  = X&#39; M_Z (X - M_Z) = X&#39; M_Z X - X&#39; \overbrace{M_Z M_Z}^{M_Z} X \\ &amp; = X&#39;M_Z X - X&#39; M_Z X = 0
  \end{aligned}
\]</span></p></li>
<li><p><span class="math inline">\(\tilde{X}&#39; Z \hat{\beta} = 0\)</span> since <span class="math inline">\(\tilde{X}\)</span> is the residual from the regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>, by normal equation it holds that <span class="math inline">\(\tilde{X}&#39; Z = 0\)</span>.</p></li>
<li><p><span class="math inline">\(\tilde{X}&#39; \hat{\varepsilon} = 0\)</span>. This follows from (i) <span class="math inline">\(M_Z &#39; M_{X, Z} = M_{X,Z}\)</span> and (ii) <span class="math inline">\(X&#39; M_{X, Z} = 0\)</span>:
<span class="math display">\[
  \tilde{X}&#39; \hat{\varepsilon} = (M_Z X)&#39; (M_{X, Z} \varepsilon)  = X&#39;M_Z&#39; M_{X, Z} \varepsilon = \underbrace{X&#39; M_{X, Z}}_0 \varepsilon = 0.
\]</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p></li>
</ol>
<p>The coefficient <span class="math inline">\(\hat{\alpha}\)</span> is a <em>partial regression</em> coefficient identified from the variation in <span class="math inline">\(X\)</span> that is orthogonal to <span class="math inline">\(Z\)</span>. This is often known as <strong>residual variation</strong>.</p>
</div>
</div>
<div id="post-double-selection" class="section level2">
<h2><span class="header-section-number">8.3</span> Post Double Selection</h2>
<p>We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.</p>
<p>Consider a regression <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span>. <span class="math inline">\(x_i\)</span> is the 1-dimensional variable of interest, <span class="math inline">\(z_i\)</span> is a high-dimensional set of control variables. We have the following procedure:</p>
<ol style="list-style-type: decimal">
<li><strong>First Stage</strong> selection: lasso <span class="math inline">\(x_i\)</span> on <span class="math inline">\(z_i\)</span>. Let the selected variables be collected in the set <span class="math inline">\(S_{FS} \subseteq z_i\)</span></li>
<li><strong>Reduced Form</strong> selection: lasso <span class="math inline">\(y_i\)</span> on <span class="math inline">\(z_i\)</span>. Let the selected variables be collected in the set <span class="math inline">\(S_{RF} \subseteq z_i\)</span></li>
<li>Regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(S_{FS} \cup S_{RF}\)</span></li>
</ol>
<p><strong>Theorem</strong>:
Let <span class="math inline">\(\{P^n\}\)</span> be a sequence of data-generating processes for <span class="math inline">\(D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n\)</span> where <span class="math inline">\(p\)</span> depends on <span class="math inline">\(n\)</span>. For each <span class="math inline">\(n\)</span>, the data are iid with <span class="math inline">\(yi = x_i&#39;\alpha_0^{(n)} + z_i&#39; \beta_0^{(n)} + \varepsilon_i\)</span> and <span class="math inline">\(x_i = z_i&#39; \gamma_0^{(n)} + u_i\)</span> where <span class="math inline">\(\mathbb E[\varepsilon_i | x_i,z_i] = 0\)</span> and <span class="math inline">\(\mathbb E[u_i|z_i] = 0\)</span>. The sparsity of the vectors <span class="math inline">\(\beta_0^{(n)}\)</span>, <span class="math inline">\(\gamma_0^{(n)}\)</span> is controlled by <span class="math inline">\(|| \beta_0^{(n)} ||_0 \leq s\)</span> with <span class="math inline">\(s^2 (\log p)^2/n \to 0\)</span>. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables <span class="math inline">\(y_i\)</span> , <span class="math inline">\(x_i\)</span> , <span class="math inline">\(z_i\)</span> as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level <span class="math inline">\(\xi \in (0, 1)\)</span>
<span class="math display">\[
        \Pr(\alpha_0 \in CI) \to 1- \xi
\]</span></p>
<p>In order to have valid confidence intervals you want their bias to be negligibly. Since
<span class="math display">\[
  CI = \left[ \hat{\beta} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
\]</span></p>
<p>If the bias is <span class="math inline">\(o(n^{-1/2})\)</span> then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is <span class="math inline">\(O(n^{-1/2})\)</span> then it has the same magnitude of the confidence interval and it does not asymptotically vanish.</p>
<p>The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:</p>
<ol style="list-style-type: decimal">
<li>Its partial correlation with the outcome, and</li>
<li>Its partial correlation with the variable of interest.</li>
</ol>
<p>If both those partial correlations are <span class="math inline">\(O( \sqrt{\log p/n})\)</span>, then the omitted variables bias is <span class="math inline">\((s \times O( \sqrt{\log p/n})^2 = o(n^{-1/2})\)</span>, provided <span class="math inline">\(s^2 (\log p)^2/n \to 0\)</span>. Relative to the <span class="math inline">\(n^{-1/2}\)</span> convergence rate, the omitted variables bias is negligible.</p>
<p>In our omitted variable bias case, we want <span class="math inline">\(| \gamma_0 \delta_0 | = o \left(\frac{1}{\sqrt{n}}\right)\)</span>. Post-double selection guarantees that</p>
<ul>
<li><em>First stage</em> selection: any “missing” variable has <span class="math inline">\(|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}\)</span></li>
<li><em>Reduced form</em> selection: any “missing” variable has <span class="math inline">\(|\delta_{0j}| \leq \frac{c}{\sqrt{n}}\)</span></li>
</ul>
<p>As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
<span class="math display">\[
    OVB = |\gamma_{0j}| \cdot|\delta_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
\]</span></p>
<div class="figure">
<img src="figures/Fig_641.png" alt="" />
<p class="caption">Pre-test Bias</p>
</div>
<p>Under homoskedasticity, the above estimator achieves the semiparametric efficiency bound.</p>
<blockquote>
<p>Example: <span class="math inline">\(y_i = \alpha_0 x_i + \delta_0 z_{i1} + \gamma_0 z_{i2} + \varepsilon_i\)</span></p>
<ul>
<li><span class="math inline">\(| \delta_0 | = \frac{1}{2} \qquad \quad | \gamma_0| = \frac{1}{2} \quad\)</span> then the bias is <span class="math inline">\(= \frac{1}{4}\)</span></li>
<li><span class="math inline">\(| \delta_0 | = \frac{1}{2} \qquad \quad | \gamma_0| = \frac{1}{\sqrt{n}}\)</span> then the bias is <span class="math inline">\(= \frac{1}{2 \sqrt{n}}\)</span></li>
<li><span class="math inline">\(| \delta_0 | = \frac{1}{\sqrt{n}} \qquad | \gamma_0| = \frac{1}{2}\quad\)</span> then the bias is <span class="math inline">\(= \frac{1}{2 \sqrt{n}}\)</span></li>
<li><span class="math inline">\(| \delta_0 | = \frac{1}{\sqrt{n}} \qquad | \gamma_0| = \frac{1}{\sqrt{n}}\)</span> then the bias is <span class="math inline">\(= \frac{1}{n}\)</span></li>
</ul>
<p>Only in the last case the bias is <span class="math inline">\(o \left(\frac{1}{\sqrt{n}}\right)\)</span>.</p>
</blockquote>
<p>What is the criterium that should guide the selection of <span class="math inline">\(\lambda\)</span>?
<span class="math display">\[
  \frac{\lambda}{n} \geq 2 \mathbb E_n[x_{ij} \varepsilon_i] \qquad \forall j \quad \text{ if } Var(x_{ij} \varepsilon_i) = 1
\]</span></p>
<p>How to choose the optimal <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>Decide the coverage of the confidence intervals (<span class="math inline">\(1-\alpha\)</span>):
<span class="math display">\[
\Pr \left( \sqrt{n} \Big| \mathbb E_n [x_{ij} \varepsilon_i] \Big| &gt; t \right) = 1- \alpha
\]</span></li>
<li>Solve for <span class="math inline">\(t\)</span></li>
<li>Get <span class="math inline">\(\lambda\)</span> such that all scores are dominated by <span class="math inline">\(\frac{\lambda}{n}\)</span> with <span class="math inline">\(\alpha\%\)</span> probability.</li>
</ul>
<blockquote>
<p>It turns out that the optimal <span class="math inline">\(t \propto \sqrt{\log(p)}\)</span></p>
</blockquote>
</div>
<div id="references-8" class="section level2">
<h2><span class="header-section-number">8.4</span> References</h2>
<ul>
<li>Belloni, A., Chernozhukov, V., &amp; Hansen, C. (2014). <em>Inference on Treatment Effects after Selection among High-Dimensional Controls</em>. The Review of Economic Studies, 81(2), 608–650.</li>
<li>Hastie, Tibshirani, Friedman (2001). “<em>The Elements of Statistical Learning</em>”.</li>
<li>Hansen (2019). “<em>Econometrics</em>”. Chapter 24.</li>
<li>Kozbur (2019). PhD Econometrics - Lecture Notes.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matlabcode.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["notes.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
