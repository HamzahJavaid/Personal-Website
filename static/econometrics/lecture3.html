<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 6 Endogeneity | Econometrics Notes</title>
  <meta name="description" content="Lecture 6 Endogeneity | Econometrics Notes" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 6 Endogeneity | Econometrics Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture 6 Endogeneity | Econometrics Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 6 Endogeneity | Econometrics Notes" />
  
  <meta name="twitter:description" content="Lecture 6 Endogeneity | Econometrics Notes" />
  

<meta name="author" content="Matteo Courthoud" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture2.html">
<link rel="next" href="lecture4.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../" target="_self">MATTEO COURTHOUD</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="appendix1.html"><a href="appendix1.html"><i class="fa fa-check"></i><b>1</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="appendix1.html"><a href="appendix1.html#basics"><i class="fa fa-check"></i><b>1.1</b> Basics</a></li>
<li class="chapter" data-level="1.2" data-path="appendix1.html"><a href="appendix1.html#spectral-decomposition"><i class="fa fa-check"></i><b>1.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="1.3" data-path="appendix1.html"><a href="appendix1.html#quadratic-forms-and-definite-matrices"><i class="fa fa-check"></i><b>1.3</b> Quadratic Forms and Definite Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="appendix1.html"><a href="appendix1.html#matrix-calculus"><i class="fa fa-check"></i><b>1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.5" data-path="appendix1.html"><a href="appendix1.html#references-1"><i class="fa fa-check"></i><b>1.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="appendix2.html"><a href="appendix2.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="appendix2.html"><a href="appendix2.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a></li>
<li class="chapter" data-level="2.2" data-path="appendix2.html"><a href="appendix2.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="appendix2.html"><a href="appendix2.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a></li>
<li class="chapter" data-level="2.4" data-path="appendix2.html"><a href="appendix2.html#inequalities"><i class="fa fa-check"></i><b>2.4</b> Inequalities</a></li>
<li class="chapter" data-level="2.5" data-path="appendix2.html"><a href="appendix2.html#theorems"><i class="fa fa-check"></i><b>2.5</b> Theorems</a></li>
<li class="chapter" data-level="2.6" data-path="appendix2.html"><a href="appendix2.html#statistical-models"><i class="fa fa-check"></i><b>2.6</b> Statistical Models</a></li>
<li class="chapter" data-level="2.7" data-path="appendix2.html"><a href="appendix2.html#references-2"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix3.html"><a href="appendix3.html"><i class="fa fa-check"></i><b>3</b> Asymptotic Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="appendix3.html"><a href="appendix3.html#convergence"><i class="fa fa-check"></i><b>3.1</b> Convergence</a></li>
<li class="chapter" data-level="3.2" data-path="appendix3.html"><a href="appendix3.html#theorems-1"><i class="fa fa-check"></i><b>3.2</b> Theorems</a></li>
<li class="chapter" data-level="3.3" data-path="appendix3.html"><a href="appendix3.html#ergodic-theory"><i class="fa fa-check"></i><b>3.3</b> Ergodic Theory</a></li>
<li class="chapter" data-level="3.4" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-estimators"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Properties of Estimators</a></li>
<li class="chapter" data-level="3.5" data-path="appendix3.html"><a href="appendix3.html#asymptotic-properties-of-test-statistics"><i class="fa fa-check"></i><b>3.5</b> Asymptotic Properties of Test Statistics</a></li>
<li class="chapter" data-level="3.6" data-path="appendix3.html"><a href="appendix3.html#references-3"><i class="fa fa-check"></i><b>3.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>4</b> OLS Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="lecture1.html"><a href="lecture1.html#the-gauss-markov-model"><i class="fa fa-check"></i><b>4.1</b> The Gauss Markov Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lecture1.html"><a href="lecture1.html#matlab"><i class="fa fa-check"></i><b>4.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture1.html"><a href="lecture1.html#the-ols-estimator"><i class="fa fa-check"></i><b>4.2</b> The OLS estimator</a><ul>
<li class="chapter" data-level="4.2.1" data-path="lecture1.html"><a href="lecture1.html#matlab-1"><i class="fa fa-check"></i><b>4.2.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture1.html"><a href="lecture1.html#ols-residual-properties"><i class="fa fa-check"></i><b>4.3</b> OLS residual properties</a><ul>
<li class="chapter" data-level="4.3.1" data-path="lecture1.html"><a href="lecture1.html#matlab-2"><i class="fa fa-check"></i><b>4.3.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture1.html"><a href="lecture1.html#finite-sample-properties-of-the-ols-estimator"><i class="fa fa-check"></i><b>4.4</b> Finite Sample Properties of the OLS estimator</a><ul>
<li class="chapter" data-level="4.4.1" data-path="lecture1.html"><a href="lecture1.html#matlab-3"><i class="fa fa-check"></i><b>4.4.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lecture1.html"><a href="lecture1.html#references-4"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>5</b> OLS Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="lecture2.html"><a href="lecture2.html#asymptotic-theory-of-the-ols-estimator"><i class="fa fa-check"></i><b>5.1</b> Asymptotic Theory of the OLS Estimator</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lecture2.html"><a href="lecture2.html#gaussian-error-term"><i class="fa fa-check"></i><b>5.1.1</b> Gaussian Error Term</a></li>
<li class="chapter" data-level="5.1.2" data-path="lecture2.html"><a href="lecture2.html#homoskedastic-error-term"><i class="fa fa-check"></i><b>5.1.2</b> Homoskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.3" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-error-term"><i class="fa fa-check"></i><b>5.1.3</b> Heteroskedastic Error Term</a></li>
<li class="chapter" data-level="5.1.4" data-path="lecture2.html"><a href="lecture2.html#heteroskedastic-and-autocorrelated-error-term"><i class="fa fa-check"></i><b>5.1.4</b> Heteroskedastic and Autocorrelated Error Term</a></li>
<li class="chapter" data-level="5.1.5" data-path="lecture2.html"><a href="lecture2.html#fixed-b-asymptotics"><i class="fa fa-check"></i><b>5.1.5</b> Fixed b asymptotics</a></li>
<li class="chapter" data-level="5.1.6" data-path="lecture2.html"><a href="lecture2.html#fixed-g-asymptotics"><i class="fa fa-check"></i><b>5.1.6</b> Fixed G asymptotics</a></li>
<li class="chapter" data-level="5.1.7" data-path="lecture2.html"><a href="lecture2.html#matlab-4"><i class="fa fa-check"></i><b>5.1.7</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lecture2.html"><a href="lecture2.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lecture2.html"><a href="lecture2.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2.2" data-path="lecture2.html"><a href="lecture2.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="lecture2.html"><a href="lecture2.html#matlab-5"><i class="fa fa-check"></i><b>5.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="lecture2.html"><a href="lecture2.html#references-5"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>6</b> Endogeneity</a><ul>
<li class="chapter" data-level="6.1" data-path="lecture3.html"><a href="lecture3.html#instrumental-variables"><i class="fa fa-check"></i><b>6.1</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lecture3.html"><a href="lecture3.html#matlab-6"><i class="fa fa-check"></i><b>6.1.1</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lecture3.html"><a href="lecture3.html#gmm"><i class="fa fa-check"></i><b>6.2</b> GMM</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lecture3.html"><a href="lecture3.html#step-gmm"><i class="fa fa-check"></i><b>6.2.1</b> 1-step GMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="lecture3.html"><a href="lecture3.html#step-gmm-1"><i class="fa fa-check"></i><b>6.2.2</b> 2-step GMM</a></li>
<li class="chapter" data-level="6.2.3" data-path="lecture3.html"><a href="lecture3.html#matlab-7"><i class="fa fa-check"></i><b>6.2.3</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture3.html"><a href="lecture3.html#testing-overidentifying-restrictions"><i class="fa fa-check"></i><b>6.3</b> Testing Overidentifying Restrictions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lecture3.html"><a href="lecture3.html#naive-test"><i class="fa fa-check"></i><b>6.3.1</b> Naive Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="lecture3.html"><a href="lecture3.html#hansens-test"><i class="fa fa-check"></i><b>6.3.2</b> Hansen’s Test</a></li>
<li class="chapter" data-level="6.3.3" data-path="lecture3.html"><a href="lecture3.html#special-case-conditional-homoskedasticity"><i class="fa fa-check"></i><b>6.3.3</b> Special Case: Conditional Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lecture3.html"><a href="lecture3.html#small-sample-properties-of-2sls"><i class="fa fa-check"></i><b>6.4</b> Small-Sample Properties of 2SLS</a></li>
<li class="chapter" data-level="6.5" data-path="lecture3.html"><a href="lecture3.html#many-instrument-robust-estimation"><i class="fa fa-check"></i><b>6.5</b> Many Instrument Robust Estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="lecture3.html"><a href="lecture3.html#liml"><i class="fa fa-check"></i><b>6.5.1</b> LIML</a></li>
<li class="chapter" data-level="6.5.2" data-path="lecture3.html"><a href="lecture3.html#jive"><i class="fa fa-check"></i><b>6.5.2</b> JIVE</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lecture3.html"><a href="lecture3.html#hausman-test"><i class="fa fa-check"></i><b>6.6</b> Hausman Test</a></li>
<li class="chapter" data-level="6.7" data-path="lecture3.html"><a href="lecture3.html#references-6"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>7</b> Non-Parametric Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="lecture4.html"><a href="lecture4.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="lecture4.html"><a href="lecture4.html#discrete-x---cell-estimator"><i class="fa fa-check"></i><b>7.2</b> Discrete x - Cell Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="lecture4.html"><a href="lecture4.html#local-non-parametric-estimation---kernels"><i class="fa fa-check"></i><b>7.3</b> Local Non-Parametric Estimation - Kernels</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lecture4.html"><a href="lecture4.html#estimator-examples"><i class="fa fa-check"></i><b>7.3.1</b> Estimator examples:</a></li>
<li class="chapter" data-level="7.3.2" data-path="lecture4.html"><a href="lecture4.html#kernel-examples"><i class="fa fa-check"></i><b>7.3.2</b> Kernel examples:</a></li>
<li class="chapter" data-level="7.3.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-bandwidth"><i class="fa fa-check"></i><b>7.3.3</b> Choice of the optimal bandwidth</a></li>
<li class="chapter" data-level="7.3.4" data-path="lecture4.html"><a href="lecture4.html#inference-1"><i class="fa fa-check"></i><b>7.3.4</b> Inference</a></li>
<li class="chapter" data-level="7.3.5" data-path="lecture4.html"><a href="lecture4.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>7.3.5</b> Bias-variance trade-off</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lecture4.html"><a href="lecture4.html#global-non-parametric-estimation---series"><i class="fa fa-check"></i><b>7.4</b> Global Non-Parametric Estimation - Series</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lecture4.html"><a href="lecture4.html#examples"><i class="fa fa-check"></i><b>7.4.1</b> Examples</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture4.html"><a href="lecture4.html#estimation"><i class="fa fa-check"></i><b>7.4.2</b> Estimation</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture4.html"><a href="lecture4.html#choice-of-the-optimal-k"><i class="fa fa-check"></i><b>7.4.3</b> Choice of the optimal <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture4.html"><a href="lecture4.html#inference-2"><i class="fa fa-check"></i><b>7.4.4</b> Inference</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture4.html"><a href="lecture4.html#kernel-vs-series"><i class="fa fa-check"></i><b>7.4.5</b> Kernel vs Series</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lecture4.html"><a href="lecture4.html#references-7"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>8</b> Variable Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="lecture5.html"><a href="lecture5.html#lasso"><i class="fa fa-check"></i><b>8.1</b> Lasso</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lecture5.html"><a href="lecture5.html#choosing-the-optimal-lambda"><i class="fa fa-check"></i><b>8.1.1</b> Choosing the optimal lambda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lecture5.html"><a href="lecture5.html#pre-testing"><i class="fa fa-check"></i><b>8.2</b> Pre-Testing</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lecture5.html"><a href="lecture5.html#omitted-variable-bias"><i class="fa fa-check"></i><b>8.2.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.2.2" data-path="lecture5.html"><a href="lecture5.html#pre-test-bias"><i class="fa fa-check"></i><b>8.2.2</b> Pre-test bias</a></li>
<li class="chapter" data-level="8.2.3" data-path="lecture5.html"><a href="lecture5.html#partioned-regression"><i class="fa fa-check"></i><b>8.2.3</b> Partioned Regression</a></li>
<li class="chapter" data-level="8.2.4" data-path="lecture5.html"><a href="lecture5.html#matlab-8"><i class="fa fa-check"></i><b>8.2.4</b> <code>Matlab</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lecture5.html"><a href="lecture5.html#post-double-selection"><i class="fa fa-check"></i><b>8.3</b> Post Double Selection</a></li>
<li class="chapter" data-level="8.4" data-path="lecture5.html"><a href="lecture5.html#references-8"><i class="fa fa-check"></i><b>8.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matlabcode.html"><a href="matlabcode.html"><i class="fa fa-check"></i><b>9</b> Matlab Code</a><ul>
<li class="chapter" data-level="9.1" data-path="matlabcode.html"><a href="matlabcode.html#lecture-1"><i class="fa fa-check"></i><b>9.1</b> Lecture 1</a></li>
<li class="chapter" data-level="9.2" data-path="matlabcode.html"><a href="matlabcode.html#lecture-2"><i class="fa fa-check"></i><b>9.2</b> Lecture 2</a></li>
<li class="chapter" data-level="9.3" data-path="matlabcode.html"><a href="matlabcode.html#lecture-3"><i class="fa fa-check"></i><b>9.3</b> Lecture 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture3" class="section level1">
<h1><span class="header-section-number">Lecture 6</span> Endogeneity</h1>
<div id="instrumental-variables" class="section level2">
<h2><span class="header-section-number">6.1</span> Instrumental Variables</h2>
<p>We say that there is <strong>endogeneity</strong> in the linear regression model if <span class="math inline">\(\mathbb E[x_i \varepsilon_i] \neq 0\)</span>.</p>
<p>The random vector <span class="math inline">\(z_i\)</span> is an <strong>instrumental variable</strong> in the linear regression model if the following conditions are met.</p>
<ul>
<li><strong>Exclusion restriction</strong>: the instruments are uncorrelated with the regression error
<span class="math display">\[
  \mathbb E_n[z_i \varepsilon_i] = 0 
\]</span>
almost surely, i.e. with probability <span class="math inline">\(p \to 1\)</span>.</li>
<li><strong>Rank condition</strong>: no linearly redundant instruments
<span class="math display">\[ 
  \mathbb E_n[z_i z_i&#39;] \neq 0 
\]</span>
almost surely, i.e. with probability <span class="math inline">\(p \to 1\)</span>.</li>
<li><strong>Relevance condition</strong> (need <span class="math inline">\(L &gt; K\)</span>):
<span class="math display">\[
  rank \ (\mathbb E_n[z_i x_i&#39;]) = K
\]</span>
almost surely, i.e. with probability <span class="math inline">\(p \to 1\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(K = dim(x_i)\)</span> and <span class="math inline">\(L = dim(z_i)\)</span>. We say that the model is <strong>just-identified</strong> if <span class="math inline">\(L = K\)</span> (method: IV) and <strong>over-identified</strong> if <span class="math inline">\(L &gt; K\)</span> (method: 2SLS).</p>
<p>Assume <span class="math inline">\(z_i\)</span> satisfies the instrumental variable assumptions above and <span class="math inline">\(dim(z_i) = dim(x_i)\)</span>, then the <strong>instrumental variables (IV)</strong> estimator <span class="math inline">\(\hat{\beta} _ {IV}\)</span> is given by
<span class="math display">\[
\begin{aligned}
    \hat{\beta} _ {IV} &amp;= \mathbb E_n[z_i x_i&#39;]^{-1} \mathbb E_n[z_i y_i] = \\
    &amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \\
    &amp;= (Z&#39;X)^{-1} (Z&#39;y) 
\end{aligned}
\]</span></p>
<p>Assume <span class="math inline">\(z_i\)</span> satisfies the instrumental variable assumptions above and <span class="math inline">\(dim(z_i) &gt; dim(x_i)\)</span>, then the <strong>two-stage-least squares (2SLS)</strong> estimator <span class="math inline">\(\hat{\beta} _ {2SLS}\)</span> is given by
<span class="math display">\[
    \hat{\beta} _ {2SLS} =  \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;X \Big)^{-1} \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;y \Big)
\]</span>
Where <span class="math inline">\(\hat{x}_i\)</span> is the predicted <span class="math inline">\(x_i\)</span> from the <strong>first stage</strong> regression of <span class="math inline">\(x_i\)</span> on <span class="math inline">\(z_i\)</span>. This is equivalent of the IV estimator using <span class="math inline">\(\hat{x}_i\)</span> as an instrument for <span class="math inline">\(x_i\)</span>.</p>
<blockquote>
<p>On the algebra of 2SLS:</p>
<ul>
<li>The estimator is called  since it can be rewritten as an IV estimator that uses <span class="math inline">\(\hat{X}\)</span> as instrument:
<span class="math display">\[
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;= \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;X \Big)^{-1} \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;y \Big) = \\
&amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \\
&amp;= \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i] 
\end{aligned}
\]</span></li>
<li>Moreover it can be rewritten as
<span class="math display">\[
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \\
&amp;= (X&#39; P_Z X)^{-1} X&#39; P_Z y = \\
&amp;= (X&#39; P_Z P_Z X)^{-1} X&#39; P_Z y = \\
&amp;= (\hat{X}&#39; \hat{X})^{-1} \hat{X}&#39; y = \\
&amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i] 
\end{aligned}
\]</span></li>
<li>How to the test the relevance condition? Rule of thumb: <span class="math inline">\(F\)</span>-test in the first stage <span class="math inline">\(&gt;10\)</span> (joint test on <span class="math inline">\(z_i\)</span>).
<strong>Problem</strong>: as <span class="math inline">\(n \to \infty\)</span>, with finite <span class="math inline">\(L\)</span>, <span class="math inline">\(F \to \infty\)</span> (bad rule of thumb).</li>
</ul>
</blockquote>
<p><strong>Theorem</strong>:
If <span class="math inline">\(K=L\)</span>, <span class="math inline">\(\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}\)</span>.</p>
<p><strong>Proof</strong>:
If <span class="math inline">\(K=L\)</span>, <span class="math inline">\(X&#39;Z\)</span> and <span class="math inline">\(Z&#39;X\)</span> are squared matrices and, by the relevance condition, non-singular (invertible).
<span class="math display">\[
\begin{aligned}
    \hat{\beta} _ {\text{2SLS}} &amp;= \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;X \Big)^{-1} \Big( X&#39;Z (Z&#39;Z)^{-1} Z&#39;y \Big) = \\
    &amp;= (Z&#39;X)^{-1} (Z&#39;Z) (X&#39;Z)^{-1} X&#39;Z (Z&#39;Z)^{-1} Z&#39;y = \\
    &amp;= (Z&#39;X)^{-1} (Z&#39;Z) (Z&#39;Z)^{-1} Z&#39;y = \\
    &amp;= (Z&#39;X)^{-1} (Z&#39;y) = \\
    &amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
\]</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<blockquote>
<p><strong>Example</strong> from Hayiashi (2000) page 187: demand and supply simultaneous equations.
<span class="math display">\[
\begin{aligned}
&amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \\
&amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i 
\end{aligned}
\]</span></p>
<p>We have an endogeneity problem. To see why, we solve the system of equations for <span class="math inline">\((p_i, q_i)\)</span>:
<span class="math display">\[
\begin{aligned}
&amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \\
&amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 } 
\end{aligned}
\]</span></p>
<p>Then the price variable is not independent from the error term in neither equation:
<span class="math display">\[
\begin{aligned}
&amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \\
&amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 } 
\end{aligned}
\]</span></p>
<p>As a consequence, the two coefficient estimates are not consistent:
<span class="math display">\[
\begin{aligned}
&amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \\
&amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)} 
\end{aligned}
\]</span></p>
<p>In general, running OLS on <span class="math inline">\(q_i = \gamma p_i + \varepsilon_i\)</span> you estimate
<span class="math display">\[
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \\
&amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \\
&amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} 
\end{aligned}
\]</span></p>
<p>Which is neither <span class="math inline">\(\alpha_1\)</span> nor <span class="math inline">\(\beta_1\)</span> but a variance weighted average of the two.</p>
<p>Suppose we have a supply shifter <span class="math inline">\(z_i\)</span> such that <span class="math inline">\(\mathbb E[z_i v_i] \neq 0\)</span> and <span class="math inline">\(\mathbb E[z_i u_i] = 0\)</span>. We combine the second condition and <span class="math inline">\(\mathbb E[u_i] = 0\)</span> to get a system of 2 equations in 2 unknowns: <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\alpha_1\)</span>.
<span class="math display">\[
\begin{aligned}
&amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \\
&amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0  
\end{aligned}
\]</span></p>
<p>We could try to solve for the vector <span class="math inline">\(\alpha\)</span> that solves
<span class="math display">\[
\begin{aligned}
&amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \\
&amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0 
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(\mathbb E_n[z_ix_i]\)</span> is invertible, we get <span class="math inline">\(\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]\)</span> which is indeed the IV estimator of <span class="math inline">\(\alpha\)</span> using <span class="math inline">\(z_i\)</span> as an instrument for the endogenous variable <span class="math inline">\(p_i\)</span>.</p>
</blockquote>
<div id="matlab-6" class="section level3">
<h3><span class="header-section-number">6.1.1</span> <code>Matlab</code></h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb8-1"><a href="lecture3.html#cb8-1"></a><span class="co">% Set the dimension of Z</span></span>
<span id="cb8-2"><a href="lecture3.html#cb8-2"></a>l = <span class="fl">3</span>;</span>
<span id="cb8-3"><a href="lecture3.html#cb8-3"></a></span>
<span id="cb8-4"><a href="lecture3.html#cb8-4"></a><span class="co">% Draw instruments</span></span>
<span id="cb8-5"><a href="lecture3.html#cb8-5"></a>Z = randn(n,l);</span>
<span id="cb8-6"><a href="lecture3.html#cb8-6"></a></span>
<span id="cb8-7"><a href="lecture3.html#cb8-7"></a><span class="co">% Correlation matrix for error terms</span></span>
<span id="cb8-8"><a href="lecture3.html#cb8-8"></a>S = eye(<span class="fl">2</span>,<span class="fl">2</span>); S(<span class="fl">1</span>,<span class="fl">2</span>)=<span class="fl">.8</span>; S(<span class="fl">2</span>,<span class="fl">1</span>)=<span class="fl">.8</span>; </span>
<span id="cb8-9"><a href="lecture3.html#cb8-9"></a></span>
<span id="cb8-10"><a href="lecture3.html#cb8-10"></a><span class="co">% Endogenous X</span></span>
<span id="cb8-11"><a href="lecture3.html#cb8-11"></a>gamma = [<span class="fl">2</span>, <span class="fl">0</span>; <span class="fl">0</span>, -<span class="fl">1</span>; -<span class="fl">1</span>, <span class="fl">3</span>];</span>
<span id="cb8-12"><a href="lecture3.html#cb8-12"></a>e = randn(n,<span class="fl">2</span>)*chol(S);</span>
<span id="cb8-13"><a href="lecture3.html#cb8-13"></a>X = Z*gamma + e(:,<span class="fl">1</span>);</span>
<span id="cb8-14"><a href="lecture3.html#cb8-14"></a></span>
<span id="cb8-15"><a href="lecture3.html#cb8-15"></a><span class="co">% Calculate Y</span></span>
<span id="cb8-16"><a href="lecture3.html#cb8-16"></a>Y = X*b + e(:,<span class="fl">2</span>);</span>
<span id="cb8-17"><a href="lecture3.html#cb8-17"></a></span>
<span id="cb8-18"><a href="lecture3.html#cb8-18"></a><span class="co">% Estimate beta OLS</span></span>
<span id="cb8-19"><a href="lecture3.html#cb8-19"></a>beta_OLS = (X&#39;*X)\(X&#39;*Y) <span class="co">% = 2.1957, -0.9022</span></span>
<span id="cb8-20"><a href="lecture3.html#cb8-20"></a></span>
<span id="cb8-21"><a href="lecture3.html#cb8-21"></a><span class="co">% IV: l=k=2 instruments</span></span>
<span id="cb8-22"><a href="lecture3.html#cb8-22"></a>Z_IV = Z(:,<span class="fl">1</span>:k);</span>
<span id="cb8-23"><a href="lecture3.html#cb8-23"></a>beta_IV = (Z_IV&#39;*X)\(Z_IV&#39;*Y) <span class="co">% = 2.1207, -1.3617</span></span>
<span id="cb8-24"><a href="lecture3.html#cb8-24"></a></span>
<span id="cb8-25"><a href="lecture3.html#cb8-25"></a><span class="co">% Calculate standard errors</span></span>
<span id="cb8-26"><a href="lecture3.html#cb8-26"></a>ehat = Y - X*beta_IV;</span>
<span id="cb8-27"><a href="lecture3.html#cb8-27"></a>V_NHC_IV = var(ehat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);</span>
<span id="cb8-28"><a href="lecture3.html#cb8-28"></a>V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * diag(ehat.^<span class="fl">2</span>) * Z_IV*inv(Z_IV&#39;*X);</span>
<span id="cb8-29"><a href="lecture3.html#cb8-29"></a></span>
<span id="cb8-30"><a href="lecture3.html#cb8-30"></a><span class="co">% 2SLS: l=3 instruments</span></span>
<span id="cb8-31"><a href="lecture3.html#cb8-31"></a>Pz = Z*inv(Z&#39;*Z)*Z&#39;;</span>
<span id="cb8-32"><a href="lecture3.html#cb8-32"></a>beta_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*Y) <span class="co">% = 2.0723, -0.9628</span></span>
<span id="cb8-33"><a href="lecture3.html#cb8-33"></a></span>
<span id="cb8-34"><a href="lecture3.html#cb8-34"></a><span class="co">% Calculate standard errors</span></span>
<span id="cb8-35"><a href="lecture3.html#cb8-35"></a>ehat = Y - X*beta_2SLS;</span>
<span id="cb8-36"><a href="lecture3.html#cb8-36"></a>V_NCH_2SLS = var(ehat) * inv(X&#39;*Pz*X);</span>
<span id="cb8-37"><a href="lecture3.html#cb8-37"></a>V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * diag(ehat.^<span class="fl">2</span>) *Pz*X*inv(X&#39;*Pz*X);</span></code></pre></div>
</div>
</div>
<div id="gmm" class="section level2">
<h2><span class="header-section-number">6.2</span> GMM</h2>
<p>Setting: we have a system of <span class="math inline">\(L\)</span> moment conditions
<span class="math display">\[
\begin{aligned}
    &amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \\
    &amp; \vdots \\
    &amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(L = \dim (\delta_0)\)</span>, no problem. If <span class="math inline">\(L &gt; \dim (\delta_0)\)</span>, there may be no solution to the system of equations. There are two possibilities.</p>
<ol style="list-style-type: decimal">
<li><strong>First Solution</strong>: add moment conditions until the system is identified
<span class="math display">\[
   \mathbb E[ a&#39; g(\omega_i, \delta_0)] = 0
 \]</span>
Solve <span class="math inline">\(\mathbb E[Ag(\omega_i, \delta)] = 0\)</span> for <span class="math inline">\(\hat{\delta}\)</span>. How to choose <span class="math inline">\(A\)</span>? Such that it minimizes <span class="math inline">\(Var(\hat{\delta})\)</span>.<br />
</li>
<li><strong>Second Solution</strong>: generalized method of moments (GMM)
<span class="math display">\[
 \begin{aligned}
   \hat{\delta} _ {GMM} &amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \\
   &amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&#39; W \mathbb E_n [g(\omega_i, \delta)]
 \end{aligned}
 \]</span></li>
</ol>
<blockquote>
<p>The choice of <span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span> are closely related to each other.</p>
</blockquote>
<div id="step-gmm" class="section level3">
<h3><span class="header-section-number">6.2.1</span> 1-step GMM</h3>
<p>Since <span class="math inline">\(J(\delta,W)\)</span> is a quadratic form, a closed form solution exists:
<span class="math display">\[
    \hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i x_i&#39;] \Big)^{-1}\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i y_i]
\]</span></p>
<p><strong>Assumptions</strong> for consistency of the GMM estimator given data <span class="math inline">\(\mathcal D = \{y_i, x_i, z_i \} _ {i=1}^n\)</span>:</p>
<ul>
<li><strong>Linearity</strong>: <span class="math inline">\(y_i = x_i\gamma_0 + \varepsilon_i\)</span></li>
<li><strong>IID</strong>: <span class="math inline">\((y_i, x_i, z_i)\)</span> iid</li>
<li><strong>Orthogonality</strong>: <span class="math inline">\(\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0\)</span></li>
<li><strong>Rank identification</strong>: <span class="math inline">\(\Sigma_{xz} = \mathbb E[z_i x_i&#39;]\)</span> has full rank</li>
</ul>
<p><strong>Theorem</strong>:
Under linearity, independence, orthogonality and rank conditions, if <span class="math inline">\(\hat{W} \overset{p}{\to} W\)</span> positive definite, then
<span class="math display">\[
    \hat{\delta}(\hat{W}) \to \delta(W)
\]</span>
If in addition to the above assumption, <span class="math inline">\(\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)\)</span> for a fixed positive definite <span class="math inline">\(S\)</span>, then
<span class="math display">\[
    \sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
\]</span>
where <span class="math inline">\(V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}\)</span>.</p>
<p>Finally, if a consistent estimator <span class="math inline">\(\hat{S}\)</span> of <span class="math inline">\(S\)</span> is available, then using sample analogues <span class="math inline">\(\hat{\Sigma}_{xz}\)</span> it follows that
<span class="math display">\[
    \hat{V} \overset{p}{\to} V
\]</span></p>
<blockquote>
<p>If <span class="math inline">\(W = S^{-1}\)</span> then <span class="math inline">\(V\)</span> reduces to <span class="math inline">\(V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}\)</span>. Moreover, <span class="math inline">\((\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}\)</span> is the smallest possible form of <span class="math inline">\(V\)</span>, in a positive definite sense.</p>
</blockquote>
<p>Therefore, to have an efficient estimator, you want to construct <span class="math inline">\(\hat{W}\)</span> such that <span class="math inline">\(\hat{W} \overset{p}{\to} S^{-1}\)</span>.</p>
</div>
<div id="step-gmm-1" class="section level3">
<h3><span class="header-section-number">6.2.2</span> 2-step GMM</h3>
<p>Estimation steps:</p>
<ul>
<li>Choose an arbitrary weighting matrix <span class="math inline">\(\hat{W}_{init}\)</span> (usually the identity matrix <span class="math inline">\(I_K\)</span>)</li>
<li>Estimate <span class="math inline">\(\hat{\delta} _ {init}(\hat{W} _ {init})\)</span></li>
<li>Estimate <span class="math inline">\(\hat{S}\)</span> (asymptotic variance of the moment condition)</li>
<li>Estimate <span class="math inline">\(\hat{\delta}(\hat{S}^{-1})\)</span></li>
</ul>
<blockquote>
<p>On the procedure:</p>
<ul>
<li>This estimator achieves the semiparametric efficiency bound.</li>
<li>This strategy works only if <span class="math inline">\(\hat{S} \overset{p}{\to} S\)</span> exists.</li>
<li>For iid cases: we can use <span class="math inline">\(\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &#39; ]\)</span> where <span class="math inline">\(\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})\)</span>.</li>
</ul>
</blockquote>
</div>
<div id="matlab-7" class="section level3">
<h3><span class="header-section-number">6.2.3</span> <code>Matlab</code></h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb9-1"><a href="lecture3.html#cb9-1"></a><span class="co">% GMM 1-step: inefficient weighting matrix</span></span>
<span id="cb9-2"><a href="lecture3.html#cb9-2"></a>W_1 = eye(l);</span>
<span id="cb9-3"><a href="lecture3.html#cb9-3"></a></span>
<span id="cb9-4"><a href="lecture3.html#cb9-4"></a><span class="co">% Objective function</span></span>
<span id="cb9-5"><a href="lecture3.html#cb9-5"></a>gmm_1 = @(b) ( Y - X*b )&#39; * Z * W_1 *  Z&#39; * ( Y - X*b );</span>
<span id="cb9-6"><a href="lecture3.html#cb9-6"></a></span>
<span id="cb9-7"><a href="lecture3.html#cb9-7"></a><span class="co">% Estimate GMM</span></span>
<span id="cb9-8"><a href="lecture3.html#cb9-8"></a>beta_gmm_1 = fminsearch(gmm_1, beta_OLS) <span class="co">% = 2.0763, -0.9548</span></span>
<span id="cb9-9"><a href="lecture3.html#cb9-9"></a>ehat = Y - X*beta_gmm_1;</span>
<span id="cb9-10"><a href="lecture3.html#cb9-10"></a></span>
<span id="cb9-11"><a href="lecture3.html#cb9-11"></a><span class="co">% Standard errors GMM</span></span>
<span id="cb9-12"><a href="lecture3.html#cb9-12"></a>S_hat = Z&#39;*diag(ehat.^<span class="fl">2</span>)*Z;</span>
<span id="cb9-13"><a href="lecture3.html#cb9-13"></a>d_hat = -X&#39;*Z;</span>
<span id="cb9-14"><a href="lecture3.html#cb9-14"></a>V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;);</span>
<span id="cb9-15"><a href="lecture3.html#cb9-15"></a></span>
<span id="cb9-16"><a href="lecture3.html#cb9-16"></a><span class="co">% GMM 2-step: efficient weighting matrix</span></span>
<span id="cb9-17"><a href="lecture3.html#cb9-17"></a>W_2 = inv(S_hat);</span>
<span id="cb9-18"><a href="lecture3.html#cb9-18"></a>gmm_2 = @(b) ( Y - X*b )&#39; * Z * W_2 *  Z&#39; * ( Y - X*b );</span>
<span id="cb9-19"><a href="lecture3.html#cb9-19"></a>beta_gmm_2 = fminsearch(gmm_2, beta_OLS) <span class="co">% = 2.0595, -0.9666</span></span>
<span id="cb9-20"><a href="lecture3.html#cb9-20"></a></span>
<span id="cb9-21"><a href="lecture3.html#cb9-21"></a><span class="co">% Standard errors GMM</span></span>
<span id="cb9-22"><a href="lecture3.html#cb9-22"></a>ehat = Y - X*beta_gmm_2;</span>
<span id="cb9-23"><a href="lecture3.html#cb9-23"></a>S_hat = Z&#39;*diag(ehat.^<span class="fl">2</span>)*Z;</span>
<span id="cb9-24"><a href="lecture3.html#cb9-24"></a>d_hat = -X&#39;*Z;</span>
<span id="cb9-25"><a href="lecture3.html#cb9-25"></a>V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;);</span></code></pre></div>
</div>
</div>
<div id="testing-overidentifying-restrictions" class="section level2">
<h2><span class="header-section-number">6.3</span> Testing Overidentifying Restrictions</h2>
<p>If the equations are <strong>exactly identified</strong>, then it is possible to choose <span class="math inline">\(\delta\)</span> so that all the elements of the sample moments <span class="math inline">\(\mathbb E_n[g(\omega_i; \delta)]\)</span> are zero and thus that the distance
<span class="math display">\[
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&#39; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
\]</span>
is zero. (The <span class="math inline">\(\delta\)</span> that does it is the IV estimator.)</p>
<p>If the equations are <strong>overidentified</strong>, i.e. <span class="math inline">\(L\)</span> (number of instruments) <span class="math inline">\(&gt; K\)</span> (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be <em>close</em> to zero.</p>
<div id="naive-test" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Naive Test</h3>
<p>Suppose your model is overidentified (<span class="math inline">\(L &gt; K\)</span>) and you use the following naive testing procedure:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{\delta}\)</span> using a subset of dimension <span class="math inline">\(K\)</span> of instruments <span class="math inline">\(\{z_1 , .. , z_K\}\)</span> for <span class="math inline">\(\{x_1 , ... , x_K\}\)</span></li>
<li>Set <span class="math inline">\(\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}\)</span></li>
<li>Infer the size of the remaining <span class="math inline">\(L-K\)</span> moment conditions <span class="math inline">\(\mathbb E[z _{i, K+1} \varepsilon_i], ..., \mathbb E[z _{i, L} \varepsilon_i]\)</span> looking at their empirical counterparts <span class="math inline">\(\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], ..., \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]\)</span></li>
<li>Reject exogeneity if the empirical expectations are high. How high? Calculate p-values.</li>
</ol>
<blockquote>
<p><strong>Example</strong>
If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don’t reject it.</p>
<ul>
<li>Model: <span class="math inline">\(y_i = x_i + \varepsilon_i\)</span> and <span class="math inline">\(x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i\)</span></li>
<li>Have <span class="math display">\[
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0.5 \\ 0 &amp; 0 &amp; 0.5 &amp; 1 
\end{bmatrix}
\]</span></li>
<li>You want to test whether the second instrument is valid (is not since <span class="math inline">\(\mathbb E[z_2 \varepsilon] \neq 0\)</span>).
You use <span class="math inline">\(z_1\)</span> and estimate <span class="math inline">\(\hat{\beta} \to\)</span> the estimator is consistent.</li>
<li>You obtain <span class="math inline">\(\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0\)</span> even if <span class="math inline">\(z_2\)</span> is invalid</li>
<li>Problem: you are using an invalid instrument in the first place.</li>
</ul>
</blockquote>
</div>
<div id="hansens-test" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Hansen’s Test</h3>
<p><strong>Theorem</strong>:
We are interested in testing <span class="math inline">\(H_0: \mathbb E[z_i \varepsilon_i] = 0\)</span> against <span class="math inline">\(H_1: \mathbb E[z_i \varepsilon_i] \neq 0\)</span>. Suppose <span class="math inline">\(\hat{S} \overset{p}{\to} S\)</span>. Then
<span class="math display">\[
    J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
\]</span>
For <span class="math inline">\(c\)</span> satisfying <span class="math inline">\(\alpha = 1- G_{L - K} ( c )\)</span>, <span class="math inline">\(\Pr(J&gt;c | H_0) \to \alpha\)</span> so the test <em>reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(J &gt; c\)</span></em> has asymptotic size <span class="math inline">\(\alpha\)</span>.</p>
<blockquote>
<p>On Hansen’s test:</p>
<ul>
<li>The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions.</li>
<li>This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large <span class="math inline">\(J\)</span> statistic as evidence for the endogeneity of some of the <span class="math inline">\(L\)</span> instruments included in <span class="math inline">\(x\)</span>.</li>
<li>Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative).</li>
<li>Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level <span class="math inline">\(\alpha\)</span>.</li>
</ul>
</blockquote>
</div>
<div id="special-case-conditional-homoskedasticity" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Special Case: Conditional Homoskedasticity</h3>
<p>The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is <span class="math inline">\(\hat{S}^{-1} = \mathbb En [z_i z_i&#39; \varepsilon_i^2]^{-1}\)</span>. With conditional homoskedasticity, the efficient weighting matrix is <span class="math inline">\(\mathbb E_n[z_iz_i&#39;]^{-1} \sigma^{-2}\)</span>, or equivalently <span class="math inline">\(\mathbb E_n[z_iz_i&#39;]^{-1}\)</span>. Then, the GMM estimator becomes
<span class="math display">\[
    \hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i x_i&#39;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i y_i&#39;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
\]</span></p>
<p><strong>Proof</strong>:
Consider the matrix notation.
<span class="math display">\[
\begin{aligned}
    \hat{\delta} \left( \frac{Z&#39;Z}{n}\right) &amp;= \left( \frac{X&#39;Z}{n} \left( \frac{Z&#39;Z}{n}\right)^{-1} \frac{Z&#39;X}{n} \right)^{-1} \frac{X&#39;Z}{n} \left( \frac{Z&#39;Z}{n}\right)^{-1} \frac{Z&#39;Y}{n} = \\
    &amp;= \left( X&#39;Z(Z&#39;Z)^{-1} Z&#39;X \right)^{-1} X&#39;Z(Z&#39;Z)^{-1} Z&#39;Y = \\
    &amp;= \left(X&#39;P_ZX\right)^{-1} X&#39;P_ZY = \\
    &amp;= \left(X&#39;P_ZP_ZX\right)^{-1} X&#39;P_ZY = \\
    &amp;= \left(\hat{X}&#39;_Z \hat{X}_Z\right)^{-1} \hat{X}&#39;_ZY = \\
    &amp;= \hat{\delta} _ {2SLS}
\end{aligned}
\]</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
</div>
</div>
<div id="small-sample-properties-of-2sls" class="section level2">
<h2><span class="header-section-number">6.4</span> Small-Sample Properties of 2SLS</h2>
<p><strong>Theorem</strong>:
When the number of instruments is equal to the sample size (<span class="math inline">\(L = n\)</span>), then <span class="math inline">\(\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}\)</span></p>
<p><strong>Proof</strong>:
We have a perfect prediction problem. The first stage estimated coefficient <span class="math inline">\(\hat{\gamma}\)</span> is such that it solves the normal equations: <span class="math inline">\(\hat{\gamma} = z_i^{-1} x_i\)</span>. Then
<span class="math display">\[
\begin{aligned}
    \hat{\delta} _ {2SLS} &amp;= \mathbb E_n[\hat{x}_i x&#39;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \\
    &amp;= \mathbb E_n[z_i z_i^{-1} x_i x&#39;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \\
    &amp;= \mathbb E_n[x_i x&#39;_i]^{-1} \mathbb E_n[x_i y_i] = \\
    &amp;= \hat{\delta} _ {OLS}
\end{aligned}
\]</span>
<span class="math display">\[\tag*{$\blacksquare$}\]</span></p>
<blockquote>
<p>You have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid.</p>
</blockquote>
<blockquote>
<p><strong>Example</strong> from Angrist (1992)</p>
<ul>
<li>They regress wages on years of schooling.</li>
<li>Problem: endogeneity: both variables are correlated with skills which are unobserved.</li>
<li>Solution: instrument years of schooling with the quarter of birth. Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year.</li>
<li>Problem: quarters of birth are three dummies. In order to ``improve the first stage fit" they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the <span class="math inline">\(R^2\)</span> but also increases the bias of the 2SLS estimator.</li>
<li>Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012).</li>
</ul>
</blockquote>
<div class="figure">
<img src="figures/Fig_441.png" alt="" />
<p class="caption">RJIVE</p>
</div>
</div>
<div id="many-instrument-robust-estimation" class="section level2">
<h2><span class="header-section-number">6.5</span> Many Instrument Robust Estimation</h2>
<p>Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for <span class="math inline">\(L=n\)</span>, the two estimators coincide.</p>
<div class="figure">
<img src="figures/Fig_451.png" alt="" />
<p class="caption">IV to OLS</p>
</div>
<div id="liml" class="section level3">
<h3><span class="header-section-number">6.5.1</span> LIML</h3>
<p>An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of <span class="math inline">\((y_i, x_i)\)</span>. The estimator is known as <strong>limited information maximum likelihood</strong>, or <strong>LIML</strong>.
This estimator is called ``limited information" because it is based on the structural equation for <span class="math inline">\((y_i, x_i)\)</span> combined with the reduced form equation for <span class="math inline">\(x_i\)</span>. If maximum likelihood is derived based on a structural equation for <span class="math inline">\(x_i\)</span> as well, then this leads to what is known as <strong>full information maximum likelihood (FIML)</strong>. The advantage of the LIML approach relative to FIML is that the former does not require a structural model for <span class="math inline">\(x_i\)</span>, and thus allows the researcher to focus on the structural equation of interest - that for <span class="math inline">\(y_i\)</span>.</p>
<p>The <strong>k-class</strong> estimators have the form
<span class="math display">\[
    \hat{\delta}(\alpha) = (X&#39; P_Z X - \alpha X&#39; X)^{-1} (X&#39; P_Z Y - \alpha X&#39; Y) 
\]</span></p>
<p>The limited information maximum likelihood estimator <strong>LIML</strong> is the k-class estimator <span class="math inline">\(\hat{\delta}(\alpha)\)</span> where
<span class="math display">\[
    \alpha = \lambda_{min} \Big( ([X&#39; , Y]^{-1} [X&#39; , Y])^{-1} [X&#39; , Y]^{-1} P_Z [X&#39; , Y] \Big) 
\]</span></p>
<p>If <span class="math inline">\(\alpha = 0\)</span> then <span class="math inline">\(\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}\)</span> while for <span class="math inline">\(\alpha \to \infty\)</span>, <span class="math inline">\(\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}\)</span>.</p>
<blockquote>
<p>Comments on LIML:</p>
<ul>
<li>The particular choice of <span class="math inline">\(\alpha\)</span> gives a many instruments robust estimate</li>
<li>The LIML estimator has no finite sample moments. <span class="math inline">\(\mathbb E[\delta(\alpha_{LIML})]\)</span> does not exist in general</li>
<li>In simulation studies performs well</li>
<li>Has good asymptotic properties</li>
</ul>
</blockquote>
<p>Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings.</p>
</div>
<div id="jive" class="section level3">
<h3><span class="header-section-number">6.5.2</span> JIVE</h3>
<p>The <strong>Jacknife IV</strong> procedure is the following</p>
<ul>
<li>Regress <span class="math inline">\(\{ x_j \} _ {j \neq i}\)</span> on <span class="math inline">\(\{ z_j \} _ {j \neq i}\)</span> and estimate <span class="math inline">\(\pi_{-i}\)</span> (leave the <span class="math inline">\(i^{th}\)</span> observation out).</li>
<li>Form <span class="math inline">\(\hat{x}_i = \hat{\pi} _ {-i} z_i\)</span>.</li>
<li>Run IV using <span class="math inline">\(\hat{x}_i\)</span> as instruments.
<span class="math display">\[
  \hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i&#39;]
\]</span></li>
</ul>
<blockquote>
<p>Comments on JIVE:</p>
<ul>
<li>Prevents overfitting.</li>
<li>With many instruments you get bad out of sample prediction which implies low correlation between <span class="math inline">\(\hat{x}_i\)</span> and <span class="math inline">\(x_i\)</span>: <span class="math inline">\(\mathbb E_n[\hat{x}_i x_i&#39;] \simeq 0\)</span>.</li>
<li>Use lasso/ridge regression in the first stage in case of too many instruments.</li>
</ul>
</blockquote>
</div>
</div>
<div id="hausman-test" class="section level2">
<h2><span class="header-section-number">6.6</span> Hausman Test</h2>
<p>Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor <span class="math inline">\(z_i\)</span> instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector <span class="math inline">\(x_i\)</span> whose validity is not in question. Further, assume for simplicity that <span class="math inline">\(L = K\)</span> so that the efficient GMM estimator is the IV estimator.</p>
<p>The <strong>Hausman test statistic</strong>
<span class="math display">\[
    H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&#39; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
\]</span>
is asymptotically distributed as a <span class="math inline">\(\chi^2_{L-s}\)</span> under the null where <span class="math inline">\(s = \# z_i \cup x_i\)</span>: the number of regressors that are retained as instruments in <span class="math inline">\(x_i\)</span>.</p>
<blockquote>
<p>In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under <span class="math inline">\(H_0\)</span> but inconsistent under <span class="math inline">\(H_1\)</span> (in this case, OLS), and another which is consistent under <span class="math inline">\(H_1\)</span> (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis <span class="math inline">\(H_0\)</span> of unconditional strict exogeneity. In that case, under <span class="math inline">\(H_0\)</span> Random Effects estimators are efficient but under <span class="math inline">\(H_1\)</span> they are inconsistent. Fixed Effects estimators instead are consistent under <span class="math inline">\(H_1\)</span>.</p>
<p>The Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test <em>accepts</em> the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is <em>small</em> and/or when the regressor <span class="math inline">\(z_i\)</span> is <em>almost valid</em>. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.</p>
<p>The Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data).</p>
</blockquote>
</div>
<div id="references-6" class="section level2">
<h2><span class="header-section-number">6.7</span> References</h2>
<ul>
<li>Belloni, A., Chen, H., Chernozhukov, V., &amp; Hansen, C. B. (2012). <em>Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain</em>. Econometrica, 80(6), 2369–2429.</li>
<li>Hansen (2019). “<em>Econometrics</em>”. Chapters 12 and 13.</li>
<li>Hayiashi (2000). “<em>Econometrics</em>”.</li>
<li>Kozbur (2019). PhD Econometrics - Lecture Notes.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["notes.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
